{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10182328,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nimport shutil\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\nfrom concurrent.futures import ProcessPoolExecutor\nimport warnings\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib import patches\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nfrom torchvision.ops.boxes import box_iou\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.transforms import functional as TF\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\n#import orjson\nimport ast\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per il deep learning avanzato\nfrom torch.amp import GradScaler, autocast\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:42:43.682322Z","iopub.execute_input":"2024-12-31T15:42:43.682696Z","iopub.status.idle":"2024-12-31T15:42:53.635076Z","shell.execute_reply.started":"2024-12-31T15:42:43.682644Z","shell.execute_reply":"2024-12-31T15:42:53.633840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"COCO_JSON_NM = 'COCO_annotations_new.json' \nOUT_COCO_JSON_NM = 'mod_COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / COCO_JSON_NM\nnew_coco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\n\ntrain_path = '/kaggle/working/train.json'\ntest_path = '/kaggle/working/test.json'\nval_path = '/kaggle/working/val.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:42:53.636120Z","iopub.execute_input":"2024-12-31T15:42:53.636662Z","iopub.status.idle":"2024-12-31T15:42:53.642602Z","shell.execute_reply.started":"2024-12-31T15:42:53.636631Z","shell.execute_reply":"2024-12-31T15:42:53.641502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:42:53.643699Z","iopub.execute_input":"2024-12-31T15:42:53.644091Z","iopub.status.idle":"2024-12-31T15:42:53.666247Z","shell.execute_reply.started":"2024-12-31T15:42:53.644042Z","shell.execute_reply":"2024-12-31T15:42:53.665083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:42:53.667287Z","iopub.execute_input":"2024-12-31T15:42:53.667620Z","iopub.status.idle":"2024-12-31T15:42:53.687121Z","shell.execute_reply.started":"2024-12-31T15:42:53.667593Z","shell.execute_reply":"2024-12-31T15:42:53.685886Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Processing vecchio json","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato.\n    Seleziona casualmente il 50% delle immagini e aggiorna le annotazioni di conseguenza.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    with open(input_path, 'r') as f:\n        data = json.load(f)\n\n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n\n    for category in tqdm(raw_categories, desc=\"Processing Categories\"):\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n\n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n\n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n\n    # Preprocessa le annotazioni in un dizionario per immagini\n    image_annotations_dict = {}\n    for annotation in tqdm(data.get('annotations', []), desc=\"Building Image Annotations Dictionary\"):\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_dict:\n            image_annotations_dict[image_id] = []\n        image_annotations_dict[image_id].append(annotation)\n\n    # Seleziona casualmente il 50% delle immagini\n    all_images = data.get('images', [])\n    selected_images = random.sample(all_images, len(all_images) // 2)\n    selected_image_ids = {image['id'] for image in selected_images}\n\n    # Filtra le annotazioni per includere solo quelle relative alle immagini selezionate\n    selected_annotations = [\n        annotation for annotation in data.get('annotations', [])\n        if annotation['image_id'] in selected_image_ids\n    ]\n\n    # Lista di nuove annotazioni da aggiungere per immagini senza bbox\n    new_annotations = []\n\n    # Elenco di annotazioni da rimuovere\n    annotations_to_remove = []\n\n    for annotation in tqdm(selected_annotations, desc=\"Processing Annotations\"):\n        if annotation['category_id'] == 0:  # Se è Aircraft\n            annotation['category_id'] = 11\n\n        # Converte il formato del bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n\n        x, y, width, height = annotation['bbox']\n        xmin = x\n        xmax = x + width\n        ymin = y\n        ymax = y + height\n\n        # Verifica che xmin < xmax e ymin < ymax\n        if xmin >= xmax or ymin >= ymax or width == 10 or height == 10:\n            annotations_to_remove.append(annotation['id'])\n        else:\n            annotation['bbox'] = [xmin, xmax, ymin, ymax]\n\n    # Rimuovi le annotazioni non valide\n    selected_annotations = [\n        ann for ann in selected_annotations if ann['id'] not in annotations_to_remove\n    ]\n\n    # Verifica se ci sono immagini senza annotazioni (usando il dizionario delle annotazioni)\n    for image in tqdm(selected_images, desc=\"Processing Images\"):\n        if image['id'] not in image_annotations_dict:  # Se l'immagine non ha annotazioni\n            # Aggiungi la categoria \"background\"\n            new_annotation = {\n                'id': len(selected_annotations) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, image['width'], 0.0, image['height']],  # Background con bbox che copre tutta l'immagine\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n\n    # Aggiungi le nuove annotazioni al JSON selezionato\n    selected_annotations.extend(new_annotations)\n\n    # Aggiorna il JSON con le immagini e annotazioni selezionate\n    processed_data = {\n        'images': selected_images,\n        'annotations': selected_annotations,\n        'categories': categories\n    }\n\n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(processed_data, f, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:42:53.690131Z","iopub.execute_input":"2024-12-31T15:42:53.690411Z","iopub.status.idle":"2024-12-31T15:42:53.712865Z","shell.execute_reply.started":"2024-12-31T15:42:53.690386Z","shell.execute_reply":"2024-12-31T15:42:53.711483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_categories_from_custom_coco_json(json_path):\n    \"\"\"\n    Estrae i nomi delle categorie da un file JSON COCO personalizzato, ordinati per ID.\n\n    Args:\n        json_path (str): Path del file JSON COCO personalizzato.\n\n    Returns:\n        list: Lista di nomi delle categorie ordinate per ID.\n    \"\"\"\n    # Leggi il file JSON\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    # Ottieni le categorie dal JSON\n    raw_categories = data.get('categories', [])\n    \n    # Crea una lista di categorie con id e nome\n    categories = []\n    for category in raw_categories:\n        # Si assume che category sia un dizionario con \"id\" e \"name\"\n        if isinstance(category, dict) and 'id' in category and 'name' in category:\n            categories.append(category)\n        else:\n            print(f\"Errore nel formato della categoria: {category}\")\n    \n    # Ordina le categorie in base all'ID\n    categories = sorted(categories, key=lambda cat: cat[\"id\"])\n    \n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.insert(0, {\"id\": 0, \"name\": \"background\"})\n    \n    # Estrai solo i nomi delle categorie\n    category_names = [cat[\"name\"] for cat in categories]\n    \n    return category_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:48:42.871671Z","iopub.execute_input":"2024-12-31T15:48:42.872105Z","iopub.status.idle":"2024-12-31T15:48:42.879408Z","shell.execute_reply.started":"2024-12-31T15:48:42.872070Z","shell.execute_reply":"2024-12-31T15:48:42.878242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_images_with_bboxes(json_file, specific_images, images_folder, mode = 2):\n    \"\"\"\n    Visualizza le immagini specificate con tutti i bounding box sopra di esse.\n\n    :param json_file: percorso del file JSON contenente le immagini, annotazioni e categorie\n    :param specific_images: lista di nomi delle immagini da visualizzare\n    :param images_folder: percorso della cartella che contiene le immagini\n    \"\"\"\n    # Carica il JSON\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    # Estrai le immagini e le annotazioni\n    images = data[\"images\"]\n    annotations = data[\"annotations\"]\n\n    # Crea un dizionario per mappare l'id delle immagini al nome del file\n    image_dict = {image[\"id\"]: image[\"file_name\"] for image in images}\n\n    # Filtra le annotazioni per le immagini specifiche\n    specific_annotations = [ann for ann in annotations if image_dict[ann[\"image_id\"]] in specific_images]\n\n    # Creiamo un dizionario per raccogliere tutte le annotazioni per ciascuna immagine\n    image_bboxes = {}\n    for annotation in specific_annotations:\n        image_name = image_dict[annotation[\"image_id\"]]\n        if image_name not in image_bboxes:\n            image_bboxes[image_name] = []\n        if mode == 1:\n            bbox = ast.literal_eval(annotation[\"bbox\"])\n        else:\n            bbox = annotation[\"bbox\"]\n        category_id = annotation[\"category_id\"]\n        image_bboxes[image_name].append((bbox, category_id))\n\n    # Visualizza tutte le immagini con tutti i bounding box\n    for image_name, bboxes in image_bboxes.items():\n        # Carica l'immagine\n        image_path = f'{images_folder}/{image_name}'  # Usa il percorso corretto per le immagini\n        image = Image.open(image_path)\n\n        # Crea la figura per la visualizzazione\n        plt.figure(figsize=(8, 8))\n        plt.imshow(image)\n\n        # Aggiungi tutti i bounding box e il category_id\n        if mode == 1:\n            for bbox, category_id in bboxes:\n                x, y, w, h = bbox\n                plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none'))\n                plt.text(x, y - 5, f'Category: {category_id}', color='red', fontsize=10, backgroundcolor='white')\n        else:\n            for bbox, category_id in bboxes:\n                x, y, x2, y2 = bbox\n                w = x2 - x\n                h = y2 - y\n                plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none'))\n                plt.text(x, y - 5, f'Category: {category_id}', color='red', fontsize=10, backgroundcolor='white')\n\n        # Imposta il titolo e disattiva gli assi\n        plt.title(f\"Image: {image_name}\")\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:42:53.734494Z","iopub.execute_input":"2024-12-31T15:42:53.734761Z","iopub.status.idle":"2024-12-31T15:42:53.753268Z","shell.execute_reply.started":"2024-12-31T15:42:53.734733Z","shell.execute_reply":"2024-12-31T15:42:53.752251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_custom_coco_json(coco_json_pth, new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:42:53.754159Z","iopub.execute_input":"2024-12-31T15:42:53.754437Z","iopub.status.idle":"2024-12-31T15:44:55.150255Z","shell.execute_reply.started":"2024-12-31T15:42:53.754400Z","shell.execute_reply":"2024-12-31T15:44:55.149339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Estrai i nomi delle categorie\ncategories = extract_categories_from_custom_coco_json(new_coco_json_pth)\n# Mostra i nomi delle categorie\nprint(categories)\n\n# Creazione del label map e reverse label map\nlabel_map = {idx: category for idx, category in enumerate(categories)}\nrev_label_map = {category: idx for idx, category in enumerate(categories)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:48:47.374292Z","iopub.execute_input":"2024-12-31T15:48:47.374669Z","iopub.status.idle":"2024-12-31T15:48:49.384339Z","shell.execute_reply.started":"2024-12-31T15:48:47.374631Z","shell.execute_reply":"2024-12-31T15:48:49.383114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_custom_coco_json(coco_json_pth, new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:49:11.336404Z","iopub.execute_input":"2024-12-31T15:49:11.336735Z","iopub.status.idle":"2024-12-31T15:51:18.179120Z","shell.execute_reply.started":"2024-12-31T15:49:11.336709Z","shell.execute_reply":"2024-12-31T15:51:18.178094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_images_with_bboxes(new_coco_json_pth, 'img_1982_0_640.jpg', img_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:52:38.052901Z","iopub.execute_input":"2024-12-31T15:52:38.053337Z","iopub.status.idle":"2024-12-31T15:52:40.626912Z","shell.execute_reply.started":"2024-12-31T15:52:38.053305Z","shell.execute_reply":"2024-12-31T15:52:40.625613Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preparazione nuovo json","metadata":{}},{"cell_type":"code","source":"def count_bboxes_per_category(json_path):\n    \"\"\"\n    Funzione che conta il numero di bounding box per ciascuna categoria in un file JSON formato COCO.\n    \n    :param json_path: Percorso al file JSON.\n    :return: Dizionario con i nomi delle categorie come chiavi e il conteggio dei bounding box come valori.\n    \"\"\"\n    # Leggi il JSON dal file\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    # Ottieni mapping delle categorie (id -> nome)\n    category_mapping = {cat['id']: cat['name'] for cat in data.get('categories', [])}\n    \n    # Conta i bounding box per ciascun category_id\n    bbox_counts = defaultdict(int)\n    for annotation in data.get('annotations', []):\n        category_id = annotation['category_id']\n        bbox_counts[category_id] += 1\n    \n    # Converti il conteggio usando i nomi delle categorie\n    bbox_counts_named = {category_mapping[cat_id]: count for cat_id, count in bbox_counts.items()}\n\n    return bbox_counts_named","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T16:02:47.590117Z","iopub.execute_input":"2024-12-29T16:02:47.590449Z","iopub.status.idle":"2024-12-29T16:02:47.595603Z","shell.execute_reply.started":"2024-12-29T16:02:47.590424Z","shell.execute_reply":"2024-12-29T16:02:47.594875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bbox_counts = count_bboxes_per_category(new_coco_json_pth)\n\n# Stampa i risultati\nfor category, count in bbox_counts.items():\n    print(f\"Categoria: {category}, Numero di bbox: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T16:02:47.597887Z","iopub.execute_input":"2024-12-29T16:02:47.598113Z","iopub.status.idle":"2024-12-29T16:02:49.008221Z","shell.execute_reply.started":"2024-12-29T16:02:47.598096Z","shell.execute_reply":"2024-12-29T16:02:49.007480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_invalid_boxes(annotations):\n    \"\"\"Filtra le annotazioni con bounding box non validi, mantenendo la classe 'background'.\"\"\"\n    valid_annotations = []\n    background_class_id = 0  # Supponiamo che '0' rappresenti la classe 'background'\n\n    for annotation in annotations:\n        bbox = annotation['bbox']\n        category_id = annotation.get('category_id')\n\n        if isinstance(bbox, str):\n            try:\n                bbox = json.loads(bbox)\n            except json.JSONDecodeError:\n                raise ValueError(f\"Bounding box non valido: {bbox} (conversione da stringa fallita).\")\n\n        if len(bbox) == 4:\n            x_min, y_min, width, height = bbox\n            if width > 0 and height > 0 or category_id == background_class_id:\n                annotation['bbox'] = [x_min, x_min + width, y_min, y_min + height]\n                valid_annotations.append(annotation)\n\n    return valid_annotations\n\n\ndef split(json_file, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n    # Carica il JSON\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    \n    # Filtra le annotazioni con bounding box non validi\n    valid_annotations = filter_invalid_boxes(data['annotations'])\n    \n    # Ottieni la lista delle immagini\n    images = data['images']\n    \n    # Mescola casualmente gli ID delle immagini\n    random.shuffle(images)\n    \n    # Calcola i limiti per train, validation e test\n    total_images = len(images)\n    total_annotations = len(valid_annotations)\n    train_end = int(total_images * train_ratio)\n    val_end = int(total_images * (train_ratio + val_ratio))\n    \n    # Suddividi le immagini nei rispettivi set\n    train_images = images[:train_end]\n    val_images = images[train_end:val_end]\n    test_images = images[val_end:]\n    \n    # Raggruppa gli ID delle immagini per i rispettivi set\n    train_image_ids = {image['id'] for image in train_images}\n    val_image_ids = {image['id'] for image in val_images}\n    test_image_ids = {image['id'] for image in test_images}\n    \n    # Filtra le annotazioni per i rispettivi set di immagini\n    train_annotations = [ann for ann in valid_annotations if ann['image_id'] in train_image_ids]\n    val_annotations = [ann for ann in valid_annotations if ann['image_id'] in val_image_ids]\n    test_annotations = [ann for ann in valid_annotations if ann['image_id'] in test_image_ids]\n    \n    # Crea i nuovi JSON per train, validation e test\n    train_data = {'images': train_images, 'annotations': train_annotations, 'categories': data['categories']}\n    val_data = {'images': val_images, 'annotations': val_annotations, 'categories': data['categories']}\n    test_data = {'images': test_images, 'annotations': test_annotations, 'categories': data['categories']}\n    \n    # Salva i file JSON\n    with open('train.json', 'w') as f:\n        json.dump(train_data, f, indent=4)\n    \n    with open('val.json', 'w') as f:\n        json.dump(val_data, f, indent=4)\n    \n    with open('test.json', 'w') as f:\n        json.dump(test_data, f, indent=4)\n    \n    # Controlla la proporzione delle immagini e delle annotazioni\n    check_split_proportions(total_images, total_annotations, \n                            len(train_images), len(val_images), len(test_images), \n                            len(train_annotations), len(val_annotations), len(test_annotations), \n                            train_ratio, val_ratio, test_ratio, \n                            train_annotations, val_annotations, test_annotations, data['categories'])\n\n\ndef check_split_proportions(total_images, total_annotations, train_count, val_count, test_count, \n                            train_bbox_count, val_bbox_count, test_bbox_count, \n                            train_ratio, val_ratio, test_ratio, \n                            train_annotations, val_annotations, test_annotations, categories):\n    # Percentuali per immagini\n    train_image_percentage = (train_count / total_images) * 100\n    val_image_percentage = (val_count / total_images) * 100\n    test_image_percentage = (test_count / total_images) * 100\n    \n    # Percentuali per bbox\n    train_bbox_percentage = (train_bbox_count / total_annotations) * 100\n    val_bbox_percentage = (val_bbox_count / total_annotations) * 100\n    test_bbox_percentage = (test_bbox_count / total_annotations) * 100\n    \n    print(f\"Totale immagini: {total_images}\")\n    print(f\"Totale annotazioni (bbox): {total_annotations}\")\n    print(f\"Train: {train_count} immagini ({train_image_percentage:.2f}%) ({train_bbox_count} bbox) ({train_bbox_percentage:.2f}%)\")\n    print(f\"Val: {val_count} immagini ({val_image_percentage:.2f}%) ({val_bbox_count} bbox) ({val_bbox_percentage:.2f}%)\")\n    print(f\"Test: {test_count} immagini ({test_image_percentage:.2f}%) ({test_bbox_count} bbox) ({test_bbox_percentage:.2f}%)\")\n    \n    # Calcola il numero di annotazioni per categoria nei vari set\n    category_count_train = defaultdict(int)\n    category_count_val = defaultdict(int)\n    category_count_test = defaultdict(int)\n    \n    for annotation in train_annotations:\n        category_count_train[annotation['category_id']] += 1\n    for annotation in val_annotations:\n        category_count_val[annotation['category_id']] += 1\n    for annotation in test_annotations:\n        category_count_test[annotation['category_id']] += 1\n    \n    # Stampa le proporzioni per categoria\n    print(\"\\nProporzioni per categoria:\")\n    for category in categories:\n        try:\n            category_id = int(category['id'])  # Converte 'id' in un intero\n            category_name = category['name']\n    \n            # Conta il numero di annotazioni per categoria in ogni set\n            train_cat_count = category_count_train.get(category_id, 0)\n            val_cat_count = category_count_val.get(category_id, 0)\n            test_cat_count = category_count_test.get(category_id, 0)\n    \n            # Calcola la percentuale di annotazioni per categoria\n            total_cat_annotations = train_cat_count + val_cat_count + test_cat_count\n            if total_cat_annotations > 0:\n                train_cat_percentage = (train_cat_count / total_cat_annotations) * 100\n                val_cat_percentage = (val_cat_count / total_cat_annotations) * 100\n                test_cat_percentage = (test_cat_count / total_cat_annotations) * 100\n            else:\n                train_cat_percentage = val_cat_percentage = test_cat_percentage = 0.0\n    \n            print(f\"{category_name}:\")\n            print(f\"  Train: {train_cat_count} annotazioni ({train_cat_percentage:.2f}%)\")\n            print(f\"  Val: {val_cat_count} annotazioni ({val_cat_percentage:.2f}%)\")\n            print(f\"  Test: {test_cat_count} annotazioni ({test_cat_percentage:.2f}%)\")\n        except KeyError as e:\n            print(f\"Chiave mancante nella categoria: {e}\")\n        except ValueError as e:\n            print(f\"Errore nel parsing della categoria: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T16:02:49.009756Z","iopub.execute_input":"2024-12-29T16:02:49.010005Z","iopub.status.idle":"2024-12-29T16:02:49.023562Z","shell.execute_reply.started":"2024-12-29T16:02:49.009985Z","shell.execute_reply":"2024-12-29T16:02:49.022884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chiamata della funzione\nsplit(new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T16:02:49.024334Z","iopub.execute_input":"2024-12-29T16:02:49.024513Z","iopub.status.idle":"2024-12-29T16:02:55.173901Z","shell.execute_reply.started":"2024-12-29T16:02:49.024496Z","shell.execute_reply":"2024-12-29T16:02:55.172909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"import operator\nclass CustomDataset(Dataset):\n    def __init__(self, json_file, img_dir, aug=False):\n        \"\"\"\n        Inizializza il dataset personalizzato.\n        Args:\n        - json_file: Il file JSON preprocessato contenente immagini, annotazioni e categorie.\n        - img_dir: La directory contenente le immagini.\n        - aug: Booleano per attivare o meno l'augmentazione.\n        \"\"\"\n        # Carica il file JSON preprocessato\n        with open(json_file, 'r') as f:\n            coco_data = json.load(f)\n\n        # Estrai informazioni su immagini, annotazioni e categorie\n        self.image_info = {image['id']: image['file_name'] for image in coco_data['images']}\n        self.image_annotations = {}\n        self.image_bboxes = {}\n\n        # Estrai le classi (categorie) dal file JSON\n        self.classes = {}\n        for category in coco_data['categories']:\n            category_id = category['id']  # ID numerico\n            category_name = category['name']  # Nome della categoria\n            self.classes[int(category_id)] = category_name\n\n\n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            bbox = annotation['bbox']\n\n            # Associa annotazioni e bounding box alle immagini\n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n            \n            self.image_annotations[image_id].append(annotation['category_id'])\n            self.image_bboxes[image_id].append(bbox)\n\n        # Configura il percorso delle immagini e seleziona solo immagini valide\n        self.img_dir = img_dir\n        self.image_paths = []\n        self.image_ids = []\n        for image_id, file_name in self.image_info.items():\n            if image_id in self.image_annotations:\n                img_path = os.path.join(img_dir, file_name)\n                if os.path.exists(img_path):\n                    self.image_paths.append(img_path)\n                    self.image_ids.append(image_id)\n\n        # Definisci trasformazioni\n        self.base_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug = aug\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Recupera un'immagine e le sue annotazioni.\n        \"\"\"\n        img_path = self.image_paths[index]\n        img_id = self.image_ids[index]\n\n        # Carica l'immagine\n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n\n        # Applica le trasformazioni\n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n\n        # Recupera le annotazioni e i bounding box\n        categories = self.image_annotations[img_id]\n        bboxes = self.image_bboxes[img_id]\n\n        # Calcola il ridimensionamento\n        scale_x = 224 / original_width\n        scale_y = 224 / original_height\n\n        # Ridimensiona i bounding box\n        scaled_bboxes = [\n            torch.tensor([\n                bbox[0] * scale_x,  # x_min\n                bbox[1] * scale_y,  # y_min\n                bbox[2] * scale_x,  # x_max\n                bbox[3] * scale_y   # y_max\n            ], dtype=torch.float32)\n            for bbox in bboxes\n        ]\n\n        # Costruisci il target\n        target = {\n            \"boxes\": torch.stack(scaled_bboxes),\n            \"labels\": torch.tensor(categories, dtype=torch.int64)\n        }\n\n        return image_tensor, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T16:02:55.175030Z","iopub.execute_input":"2024-12-29T16:02:55.175289Z","iopub.status.idle":"2024-12-29T16:02:55.185404Z","shell.execute_reply.started":"2024-12-29T16:02:55.175267Z","shell.execute_reply":"2024-12-29T16:02:55.184544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione di collation per il DataLoader, utile per il batching di immagini e annotazioni.\n    La funzione restituirà un batch di immagini e un batch di target, formattato correttamente per Faster R-CNN.\n    \n    Args:\n    - batch: lista di tuple (image, target)\n    \n    Returns:\n    - images: batch di immagini\n    - targets: lista di dizionari contenenti le annotazioni per ogni immagine\n    \"\"\"\n    # Separa immagini e target\n    images, targets = zip(*batch)\n\n    # Converte la lista di immagini in un batch di immagini\n    images = list(images)\n\n    # Restituisci il batch\n    return images, list(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T16:02:55.186143Z","iopub.execute_input":"2024-12-29T16:02:55.186344Z","iopub.status.idle":"2024-12-29T16:02:55.200576Z","shell.execute_reply.started":"2024-12-29T16:02:55.186327Z","shell.execute_reply":"2024-12-29T16:02:55.199868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_path, img_dir=img_fldr, aug=True)\nvalid_dataset = CustomDataset(val_path, img_dir=img_fldr, aug=False)  \ntest_dataset = CustomDataset(test_path, img_dir=img_fldr, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\nval_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:45:03.551354Z","iopub.execute_input":"2024-12-29T18:45:03.551683Z","iopub.status.idle":"2024-12-29T18:45:35.552730Z","shell.execute_reply.started":"2024-12-29T18:45:03.551653Z","shell.execute_reply":"2024-12-29T18:45:35.552062Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check Dataloader","metadata":{}},{"cell_type":"code","source":"def validate_dataloader(dataloader):\n    \"\"\"\n    Valida un DataLoader verificando che ogni immagine abbia un target associato\n    e che nessun target sia `None` o vuoto.\n    \n    Args:\n    - dataloader: Il DataLoader da verificare.\n    \n    Returns:\n    - error_messages: Lista di messaggi di errore. Vuota se tutti i dati sono validi.\n    \"\"\"\n    error_messages = []\n    for batch_idx, (images, targets) in enumerate(dataloader):\n        for idx, target in enumerate(targets):\n            if target is None:\n                error_messages.append(f\"Batch {batch_idx}, Immagine {idx}: Target è None.\")\n            elif target[\"boxes\"].numel() == 0 or target[\"labels\"].numel() == 0:\n                error_messages.append(\n                    f\"Batch {batch_idx}, Immagine {idx}: Target è vuoto o mancano 'boxes'/'labels'.\"\n                )\n    return error_messages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:45:35.553927Z","iopub.execute_input":"2024-12-29T18:45:35.554213Z","iopub.status.idle":"2024-12-29T18:45:35.558992Z","shell.execute_reply.started":"2024-12-29T18:45:35.554186Z","shell.execute_reply":"2024-12-29T18:45:35.558135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validazione del DataLoader di training\ntrain_errors = validate_dataloader(train_loader)\n\nif train_errors:\n    print(\"Errori nel DataLoader di training:\")\n    for error in train_errors:\n        print(error)\nelse:\n    print(\"Tutti i target nel DataLoader di training sono validi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:45:35.560627Z","iopub.execute_input":"2024-12-29T18:45:35.560850Z","iopub.status.idle":"2024-12-29T18:46:22.848168Z","shell.execute_reply.started":"2024-12-29T18:45:35.560828Z","shell.execute_reply":"2024-12-29T18:46:22.847130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validazione del DataLoader di training\nval_errors = validate_dataloader(val_loader)\n\nif val_errors:\n    print(\"Errori nel DataLoader di validation:\")\n    for error in val_errors:\n        print(error)\nelse:\n    print(\"Tutti i target nel DataLoader di validation sono validi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:46:22.849608Z","iopub.execute_input":"2024-12-29T18:46:22.849884Z","iopub.status.idle":"2024-12-29T18:46:29.184784Z","shell.execute_reply.started":"2024-12-29T18:46:22.849861Z","shell.execute_reply":"2024-12-29T18:46:29.183863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validazione del DataLoader di training\ntest_errors = validate_dataloader(test_loader)\n\nif test_errors:\n    print(\"Errori nel DataLoader di test:\")\n    for error in test_errors:\n        print(error)\nelse:\n    print(\"Tutti i target nel DataLoader di test sono validi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:46:29.185882Z","iopub.execute_input":"2024-12-29T18:46:29.186123Z","iopub.status.idle":"2024-12-29T18:46:35.551700Z","shell.execute_reply.started":"2024-12-29T18:46:29.186100Z","shell.execute_reply":"2024-12-29T18:46:35.550868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_images_and_targets(dataloader):\n    \"\"\"\n    Conta il numero totale di immagini e target in un DataLoader.\n    \"\"\"\n    num_images = 0\n    num_targets = 0\n\n    for images, targets in dataloader:\n        # Conta le immagini nel batch\n        num_images += len(images)\n        \n        # Conta i target per ogni immagine (numero di oggetti)\n        for target in targets:\n            num_targets += len(target[\"boxes\"])  # Ogni immagine ha un numero di bounding boxes\n    \n    return num_images, num_targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:46:35.552651Z","iopub.execute_input":"2024-12-29T18:46:35.552976Z","iopub.status.idle":"2024-12-29T18:46:35.557470Z","shell.execute_reply.started":"2024-12-29T18:46:35.552950Z","shell.execute_reply":"2024-12-29T18:46:35.556572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_images_train, num_targets_train = count_images_and_targets(train_loader)\n\nprint(f\"Numero totale di immagini per il train: {num_images_train}\")\nprint(f\"Numero totale di target per il train: {num_targets_train}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:46:35.558226Z","iopub.execute_input":"2024-12-29T18:46:35.558509Z","iopub.status.idle":"2024-12-29T18:47:23.374427Z","shell.execute_reply.started":"2024-12-29T18:46:35.558489Z","shell.execute_reply":"2024-12-29T18:47:23.373295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_images_val, num_targets_val = count_images_and_targets(val_loader)\n\nprint(f\"Numero totale di immagini per il validation: {num_images_val}\")\nprint(f\"Numero totale di target per il validation: {num_targets_val}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:47:23.377485Z","iopub.execute_input":"2024-12-29T18:47:23.377770Z","iopub.status.idle":"2024-12-29T18:47:29.993242Z","shell.execute_reply.started":"2024-12-29T18:47:23.377743Z","shell.execute_reply":"2024-12-29T18:47:29.992200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_images_test, num_targets_test = count_images_and_targets(test_loader)\n\nprint(f\"Numero totale di immagini per il test: {num_images_test}\")\nprint(f\"Numero totale di target per il test: {num_targets_test}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:47:29.995176Z","iopub.execute_input":"2024-12-29T18:47:29.995425Z","iopub.status.idle":"2024-12-29T18:47:36.349507Z","shell.execute_reply.started":"2024-12-29T18:47:29.995403Z","shell.execute_reply":"2024-12-29T18:47:36.348602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Numero totale di immagini: {num_images_train + num_images_val +num_images_test}\")\nprint(f\"Numero totale di target: {num_targets_train + num_targets_val +num_targets_test}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:47:36.350518Z","iopub.execute_input":"2024-12-29T18:47:36.350879Z","iopub.status.idle":"2024-12-29T18:47:36.355676Z","shell.execute_reply.started":"2024-12-29T18:47:36.350851Z","shell.execute_reply":"2024-12-29T18:47:36.354955Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Definizione delle funzioni di addestramento","metadata":{}},{"cell_type":"markdown","source":"def train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs=10, num_classes=12, accumulation_steps=4):\n    \"\"\"\n    Addestra e valida il modello Faster R-CNN.\n    \"\"\"\n    # Scaler per mixed precision training\n    scaler = GradScaler()\n\n    # Sposta il modello sul dispositivo\n    model.to(device)\n\n    train_losses = []\n    val_losses = []\n    train_confidence = []\n    val_confidence = []\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoca {epoch + 1}/{num_epochs}\")\n\n        # --------------------\n        # Training\n        # --------------------\n        model.train()\n        total_train_loss = 0.0\n        train_loop = tqdm(train_loader, desc=\"Training\", leave=False)\n\n        for i, (images, targets) in enumerate(train_loop):\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            with autocast('cuda'):\n                # Calcola le perdite\n                loss_dict = model(images, targets)\n\n                # Controlla che il risultato sia un dizionario\n                if not isinstance(loss_dict, dict):\n                    raise ValueError(f\"Il modello ha restituito un oggetto non valido: {type(loss_dict)}\")\n                \n                # Somma delle perdite\n                losses = sum(loss for loss in loss_dict.values())\n\n            # Backward pass con gradient scaling\n            scaler.scale(losses).backward()\n\n            # Gradient accumulation\n            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n            total_train_loss += losses.item()\n            train_loop.set_postfix(loss=losses.item())\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        print(f\"Perdita media di training: {avg_train_loss:.4f}\")\n\n        # Calcolo Precision e Recall per il training\n        train_conf = compute_average_confidence(model, train_loader, device, num_classes)\n        train_confidence.append(train_conf)\n        #train_recall.append(train_rec)\n        print(f\"Average confidence di training: {train_conf:.4f}\")\n\n        # --------------------\n        # Validazione\n        # --------------------\n        model.train()  # Necessario per calcolare la loss\n        total_val_loss = 0.0\n        val_loop = tqdm(val_loader, desc=\"Validazione\", leave=False)\n\n        for i, (images, targets) in enumerate(val_loop):\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # Calcolo delle perdite nella validazione\n            with torch.no_grad(), autocast('cuda'):\n                # Calcola la loss durante la validazione\n                loss_dict = model(images, targets)\n                \n                # Controlla che il risultato sia un dizionario\n                if not isinstance(loss_dict, dict):\n                    raise ValueError(f\"Il modello ha restituito un oggetto non valido: {type(loss_dict)}\")\n                \n                # Somma delle perdite\n                losses = sum(loss for loss in loss_dict.values())\n                total_val_loss += losses.item()\n                val_loop.set_postfix(loss=losses.item())\n\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        print(f\"Perdita media di validazione: {avg_val_loss:.4f}\")\n\n        # Calcolo delle metriche Precision e Recall\n        val_conf = compute_average_confidence(model, val_loader, device, num_classes)\n        val_confidence.append(val_conf)\n        #val_recall.append(val_rec)\n        print(f\"Average confidence di validazione: {val_conf:.4f}\")\n\n        # --------------------\n        # Salvataggio del modello\n        # --------------------\n        torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")\n        print(f\"Modello salvato: model_epoch_{epoch + 1}.pth\")\n\n    return train_losses, val_losses, train_confidence, val_confidence","metadata":{"execution":{"iopub.status.busy":"2024-12-29T18:47:36.356509Z","iopub.execute_input":"2024-12-29T18:47:36.356736Z","iopub.status.idle":"2024-12-29T18:47:36.369620Z","shell.execute_reply.started":"2024-12-29T18:47:36.356717Z","shell.execute_reply":"2024-12-29T18:47:36.368778Z"}}},{"cell_type":"code","source":"def train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs=10, num_classes=12, accumulation_steps=4):\n    \"\"\"\n    Addestra e valida il modello Faster R-CNN.\n    \"\"\"\n    # Scaler per mixed precision training\n    scaler = GradScaler()\n\n    # Sposta il modello sul dispositivo\n    model.to(device)\n\n    train_losses = []\n    val_losses = []\n    train_confidences = []\n    val_confidences = []\n    train_precisions = []\n    train_recalls = []\n    val_precisions = []\n    val_recalls = []\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoca {epoch + 1}/{num_epochs}\")\n\n        # --------------------\n        # Training\n        # --------------------\n        model.train()\n        total_train_loss = 0.0\n        train_loop = tqdm(train_loader, desc=\"Training\", leave=False)\n\n        for i, (images, targets) in enumerate(train_loop):\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            with autocast('cuda'):\n                # Calcola le perdite\n                loss_dict = model(images, targets)\n\n                # Controlla che il risultato sia un dizionario\n                if not isinstance(loss_dict, dict):\n                    raise ValueError(f\"Il modello ha restituito un oggetto non valido: {type(loss_dict)}\")\n                \n                # Somma delle perdite\n                losses = sum(loss for loss in loss_dict.values())\n\n            # Backward pass con gradient scaling\n            scaler.scale(losses).backward()\n\n            # Gradient accumulation\n            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n            total_train_loss += losses.item()\n            train_loop.set_postfix(loss=losses.item())\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        print(f\"Perdita media di training: {avg_train_loss:.4f}\")\n\n        # Calcolo metriche per il training\n        train_precision, train_recall, train_conf = evaluate_metrics(model, train_loader, device, num_classes)\n        train_precisions.append(train_precision)\n        train_recalls.append(train_recall)\n        train_confidences.append(train_conf)\n        print(f\"Precision di training: {train_precision:.4f}, Recall di training: {train_recall:.4f}, Confidenza media di training: {train_conf:.4f}\")\n\n        # --------------------\n        # Validazione\n        # --------------------\n        model.train()  # Necessario per calcolare la loss\n        total_val_loss = 0.0\n        val_loop = tqdm(val_loader, desc=\"Validazione\", leave=False)\n\n        for i, (images, targets) in enumerate(val_loop):\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # Calcolo delle perdite nella validazione\n            with torch.no_grad(), autocast('cuda'):\n                loss_dict = model(images, targets)\n                if not isinstance(loss_dict, dict):\n                    raise ValueError(f\"Il modello ha restituito un oggetto non valido: {type(loss_dict)}\")\n                losses = sum(loss for loss in loss_dict.values())\n                total_val_loss += losses.item()\n                val_loop.set_postfix(loss=losses.item())\n\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        print(f\"Perdita media di validazione: {avg_val_loss:.4f}\")\n\n        # Calcolo metriche per la validazione\n        val_precision, val_recall, val_conf = evaluate_metrics(model, val_loader, device, num_classes)\n        val_precisions.append(val_precision)\n        val_recalls.append(val_recall)\n        val_confidences.append(val_conf)\n        print(f\"Precision di validazione: {val_precision:.4f}, Recall di validazione: {val_recall:.4f}, Confidenza media di validazione: {val_conf:.4f}\")\n\n        # --------------------\n        # Salvataggio del modello\n        # --------------------\n        torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")\n        print(f\"Modello salvato: model_epoch_{epoch + 1}.pth\")\n\n    return train_losses, val_losses, train_precisions, train_recalls, train_confidences, val_precisions, val_recalls, val_confidences\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def evaluate_precision_recall(model, data_loader, device, num_classes, iou_threshold=0.5, score_threshold=0.5):\n    \"\"\"\n    Calcola Precision e Recall basati su IoU per Faster R-CNN.\n    \"\"\"\n    model.eval()\n    true_positives = [0] * num_classes\n    false_positives = [0] * num_classes\n    false_negatives = [0] * num_classes\n\n    with torch.no_grad():\n        for images, targets in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # Predizioni\n            outputs = model(images)\n\n            for target, output in zip(targets, outputs):\n                target_boxes = target['boxes'].cpu()\n                target_labels = target['labels'].cpu()\n                pred_boxes = output['boxes'].cpu()\n                pred_labels = output['labels'].cpu()\n                pred_scores = output['scores'].cpu()\n\n                # Filtra predizioni con punteggio basso\n                high_score_indices = pred_scores > score_threshold\n                pred_boxes = pred_boxes[high_score_indices]\n                pred_labels = pred_labels[high_score_indices]\n\n                # Confronta predizioni e target\n                if len(pred_boxes) > 0 and len(target_boxes) > 0:\n                    ious = box_iou(pred_boxes, target_boxes)  # IoU\n                    for pred_idx, pred_label in enumerate(pred_labels):\n                        iou_values = ious[pred_idx]\n                        max_iou_idx = torch.argmax(iou_values)\n                        max_iou = iou_values[max_iou_idx]\n\n                        if max_iou >= iou_threshold and pred_label == target_labels[max_iou_idx]:\n                            true_positives[pred_label] += 1\n                            ious[:, max_iou_idx] = 0  # Disabilita target già usati\n                        else:\n                            false_positives[pred_label] += 1\n\n                # False negatives\n                for target_label in target_labels:\n                    if target_label not in pred_labels:\n                        false_negatives[target_label] += 1\n\n    # Precision e Recall\n    precision = sum(true_positives) / (sum(true_positives) + sum(false_positives) + 1e-6)\n    recall = sum(true_positives) / (sum(true_positives) + sum(false_negatives) + 1e-6)\n\n    return precision, recall","metadata":{"execution":{"iopub.status.busy":"2024-12-29T12:02:02.589528Z","iopub.execute_input":"2024-12-29T12:02:02.589761Z","iopub.status.idle":"2024-12-29T12:02:02.604711Z","shell.execute_reply.started":"2024-12-29T12:02:02.589740Z","shell.execute_reply":"2024-12-29T12:02:02.603739Z"}}},{"cell_type":"markdown","source":"def compute_average_confidence(model, data_loader, device, num_classes, iou_threshold=0.5, score_threshold=0.5):\n    \"\"\"\n    Calcola la confidenza media per le predizioni corrette basate su IoU e score.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n\n    total_confidence = 0.0\n    num_valid_predictions = 0\n\n    with torch.no_grad():\n        for images, targets in tqdm(data_loader, desc=\"Evaluating Confidences\", leave=False):\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # Get model predictions\n            outputs = model(images)\n\n            for target, output in zip(targets, outputs):\n                target_boxes = target['boxes'].cpu()\n                pred_boxes = output['boxes'].cpu()\n                pred_scores = output['scores'].cpu()\n\n                # Filter out predictions with low scores\n                high_score_indices = pred_scores > score_threshold\n                pred_boxes = pred_boxes[high_score_indices]\n                pred_scores = pred_scores[high_score_indices]\n\n                # Only compute confidence if there are valid predictions\n                if len(pred_boxes) > 0 and len(target_boxes) > 0:\n                    # Compute IoU\n                    ious = box_iou(pred_boxes, target_boxes)\n\n                    for pred_idx in range(len(pred_boxes)):\n                        iou_values = ious[pred_idx]\n                        max_iou_idx = torch.argmax(iou_values)\n                        max_iou = iou_values[max_iou_idx]\n\n                        # If IoU is above threshold and label matches, count as a correct prediction\n                        if max_iou >= iou_threshold:\n                            total_confidence += pred_scores[pred_idx].item()\n                            num_valid_predictions += 1\n\n    # Return the average confidence (mean of correct predictions' scores)\n    average_confidence = total_confidence / num_valid_predictions if num_valid_predictions > 0 else 0.0\n    print(num_valid_predictions)\n    return average_confidence\n","metadata":{"execution":{"iopub.status.busy":"2024-12-29T18:47:36.370408Z","iopub.execute_input":"2024-12-29T18:47:36.370658Z","iopub.status.idle":"2024-12-29T18:47:36.384432Z","shell.execute_reply.started":"2024-12-29T18:47:36.370629Z","shell.execute_reply":"2024-12-29T18:47:36.383702Z"}}},{"cell_type":"code","source":"def evaluate_metrics(model, data_loader, device, num_classes, iou_threshold=0.5, score_threshold=0.5):\n    \"\"\"\n    Calcola Precision, Recall e la confidenza media basati su IoU per Faster R-CNN.\n    \"\"\"\n    model.eval()\n    true_positives = [0] * num_classes\n    false_positives = [0] * num_classes\n    false_negatives = [0] * num_classes\n    total_confidence = 0.0\n    num_valid_predictions = 0\n\n    with torch.no_grad():\n        for images, targets in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # Predizioni\n            outputs = model(images)\n\n            for target, output in zip(targets, outputs):\n                target_boxes = target['boxes'].cpu()\n                target_labels = target['labels'].cpu()\n                pred_boxes = output['boxes'].cpu()\n                pred_labels = output['labels'].cpu()\n                pred_scores = output['scores'].cpu()\n\n                # Filtra predizioni con punteggio basso\n                high_score_indices = pred_scores > score_threshold\n                pred_boxes = pred_boxes[high_score_indices]\n                pred_labels = pred_labels[high_score_indices]\n                pred_scores = pred_scores[high_score_indices]\n\n                if len(pred_boxes) > 0 and len(target_boxes) > 0:\n                    ious = box_iou(pred_boxes, target_boxes)  # IoU\n                    for pred_idx, pred_label in enumerate(pred_labels):\n                        iou_values = ious[pred_idx]\n                        max_iou_idx = torch.argmax(iou_values)\n                        max_iou = iou_values[max_iou_idx]\n\n                        if max_iou >= iou_threshold and pred_label == target_labels[max_iou_idx]:\n                            true_positives[pred_label] += 1\n                            total_confidence += pred_scores[pred_idx].item()\n                            num_valid_predictions += 1\n                            ious[:, max_iou_idx] = 0  # Disabilita target già usati\n                        else:\n                            false_positives[pred_label] += 1\n\n                # False negatives\n                for target_label in target_labels:\n                    if target_label not in pred_labels:\n                        false_negatives[target_label] += 1\n\n    # Precision e Recall globali\n    precision = sum(true_positives) / (sum(true_positives) + sum(false_positives) + 1e-6)\n    recall = sum(true_positives) / (sum(true_positives) + sum(false_negatives) + 1e-6)\n\n    # Confidenza media\n    average_confidence = total_confidence / num_valid_predictions if num_valid_predictions > 0 else 0.0\n\n    return precision, recall, average_confidence\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def plot_metrics(train_losses, val_losses, train_precision, val_precision, train_recall, val_recall, num_epochs):\n    \"\"\"\n    Funzione per plottare le metriche di training e validazione (Loss e mAP).\n    \"\"\"\n    epochs_range = range(1, num_epochs + 1)\n\n    plt.figure(figsize=(12, 8))\n\n    # Plot della Loss\n    plt.subplot(3, 1, 1)\n    plt.plot(epochs_range, train_losses, label='Training Loss', color='blue')\n    plt.plot(epochs_range, val_losses, label='Validation Loss', color='red')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss per Epoca')\n\n    # Plot della precision\n    plt.subplot(3, 1, 2)\n    plt.plot(epochs_range, train_precision, label='Training precision', color='green')\n    plt.plot(epochs_range, val_precision, label='Validation precision', color='orange')\n    plt.xlabel('Epochs')\n    plt.ylabel('Precision')\n    plt.legend()\n    plt.title('Precision per Epoca')\n\n    # Plot della recall\n    plt.subplot(3, 1, 3)\n    plt.plot(epochs_range, train_recall, label='Training recall', color='blue')\n    plt.plot(epochs_range, val_recall, label='Validation recall', color='red')\n    plt.xlabel('Epochs')\n    plt.ylabel('Recall')\n    plt.legend()\n    plt.title('Recall per Epoca')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-12-29T13:52:44.774041Z","iopub.execute_input":"2024-12-29T13:52:44.774337Z","iopub.status.idle":"2024-12-29T13:52:44.793746Z","shell.execute_reply.started":"2024-12-29T13:52:44.774313Z","shell.execute_reply":"2024-12-29T13:52:44.792986Z"}}},{"cell_type":"code","source":"def plot_metrics(train_losses, val_losses, train_avg_confidence, val_avg_confidence, num_epochs): \n    \"\"\" Funzione per plottare le metriche di training e validazione (Loss e mAP). \"\"\" \n    epochs_range = range(1, num_epochs + 1)\n\n    plt.figure(figsize=(12, 8))\n    \n    # Plot della Loss\n    plt.subplot(2, 1, 1)\n    plt.plot(epochs_range, train_losses, label='Training Loss', color='blue')\n    plt.plot(epochs_range, val_losses, label='Validation Loss', color='red')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss per Epoca')\n    \n    # Plot della precision\n    plt.subplot(2, 1, 2)\n    plt.plot(epochs_range, train_avg_confidence, label='Training precision', color='green')\n    plt.plot(epochs_range, val_avg_confidence, label='Validation precision', color='orange')\n    plt.xlabel('Epochs')\n    plt.ylabel('Precision')\n    plt.legend()\n    plt.title('Average Confidence per Epoca')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:47:36.385117Z","iopub.execute_input":"2024-12-29T18:47:36.385326Z","iopub.status.idle":"2024-12-29T18:47:36.398553Z","shell.execute_reply.started":"2024-12-29T18:47:36.385298Z","shell.execute_reply":"2024-12-29T18:47:36.397944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n\n# Carica il modello Faster R-CNN con ResNet50 e FPN\nweights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\nmodel = fasterrcnn_resnet50_fpn(weights=weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:47:36.399270Z","iopub.execute_input":"2024-12-29T18:47:36.399545Z","iopub.status.idle":"2024-12-29T18:47:37.087228Z","shell.execute_reply.started":"2024-12-29T18:47:36.399526Z","shell.execute_reply":"2024-12-29T18:47:37.086313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 12\n\n# Modifica il numero di classi in output\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\n# Congela i layer della backbone (ResNet50)\nfor param in model.backbone.parameters():\n    param.requires_grad = True\n\n# Imposta il dispositivo (GPU o CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Configurazione training\nnum_epochs = 2\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\n\n# Sposta il modello su GPU o CPU\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:47:37.088151Z","iopub.execute_input":"2024-12-29T18:47:37.088459Z","iopub.status.idle":"2024-12-29T18:47:37.146872Z","shell.execute_reply.started":"2024-12-29T18:47:37.088429Z","shell.execute_reply":"2024-12-29T18:47:37.146168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"train_losses, val_losses, train_precision, val_precision, train_recall, val_recall = train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-12-29T13:39:36.655118Z","iopub.execute_input":"2024-12-29T13:39:36.655474Z","iopub.status.idle":"2024-12-29T13:39:39.013267Z","shell.execute_reply.started":"2024-12-29T13:39:36.655442Z","shell.execute_reply":"2024-12-29T13:39:39.011955Z"}}},{"cell_type":"code","source":"train_losses, val_losses, train_avg_confidence, val_avg_confidence = train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T19:58:42.625532Z","iopub.execute_input":"2024-12-29T19:58:42.625731Z","iopub.status.idle":"2024-12-29T19:58:44.837099Z","shell.execute_reply.started":"2024-12-29T19:58:42.625712Z","shell.execute_reply":"2024-12-29T19:58:44.835737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(train_losses, val_losses, train_avg_confidence, val_avg_confidence, num_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T19:58:42.257134Z","iopub.status.idle":"2024-12-29T19:58:42.257380Z","shell.execute_reply":"2024-12-29T19:58:42.257279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"def extract_categories_from_coco_json(json_path):\n    \"\"\"\n    Estrae i nomi delle categorie da un file JSON in formato COCO.\n\n    Args:\n        json_path (str): Path del file JSON COCO.\n\n    Returns:\n        list: Lista di nomi delle categorie ordinate.\n    \"\"\"\n    # Leggi il file JSON\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    # Ottieni i nomi delle categorie\n    categories = [cat[\"name\"] for cat in data.get('categories', [])]\n    \n    # Ordina i nomi delle categorie\n    categories = sorted(categories)\n    \n    # Aggiungi una categoria \"background\" come primo elemento se non esiste\n    if \"background\" not in categories:\n        categories.insert(0, \"background\")\n    \n    return categories\n\n# Path al file JSON COCO\njson_path = \"/kaggle/working/mod_COCO_annotations_new.json\"\n\n# Estrai i nomi delle categorie\ncategories = extract_categories_from_coco_json(json_path)\n\n# Mostra i nomi delle categorie\nprint(categories)","metadata":{"execution":{"iopub.status.busy":"2024-12-29T19:58:56.510260Z","iopub.execute_input":"2024-12-29T19:58:56.510539Z","iopub.status.idle":"2024-12-29T19:58:58.247857Z","shell.execute_reply.started":"2024-12-29T19:58:56.510516Z","shell.execute_reply":"2024-12-29T19:58:58.247104Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model, test_loader, device, class_names, num_classes=12, num_visualizations=5):\n    \"\"\"\n    Funzione per il testing del modello Faster R-CNN.\n    \"\"\"\n    model.to(device)\n    model.eval()\n    predictions = []\n    all_preds = []  # Per calcolo mAP\n    all_targets = []  # Per calcolo mAP\n\n    print(\"\\nInizio testing...\")\n    test_loop = tqdm(test_loader, desc=\"Testing\", leave=False)\n    visualized = 0  # Contatore per visualizzazioni\n\n    with torch.no_grad():\n        for images, targets in test_loop:\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Targets su dispositivo\n\n            # Predizioni dal modello\n            preds = model(images)\n\n            for pred, target in zip(preds, targets):\n                predictions.append({\n                    'boxes': pred['boxes'].cpu().numpy(),\n                    'labels': pred['labels'].cpu().numpy(),\n                    'scores': pred['scores'].cpu().numpy()\n                })\n\n                # Raccogli predizioni e target per calcolo mAP\n                all_preds.append(pred)\n                all_targets.append(target)\n\n                # Visualizza immagini con bounding box\n                if visualized < num_visualizations:\n                    visualize_predictions(\n                        images[0].cpu().numpy().transpose(1, 2, 0),  # Converti (C, H, W) -> (H, W, C)\n                        pred['boxes'].cpu().numpy(),\n                        pred['labels'].cpu().numpy(),\n                        pred['scores'].cpu().numpy(),\n                        class_names\n                    )\n                    visualized += 1\n\n            # Aggiorna barra di avanzamento\n            test_loop.set_postfix(processed=len(predictions))\n\n    # Calcola e visualizza statistiche globali\n    total_preds = sum(len(pred['labels']) for pred in all_preds)\n    total_targets = sum(len(target['labels']) for target in all_targets)\n    average_preds_per_image = total_preds / len(all_preds) if all_preds else 0\n    average_targets_per_image = total_targets / len(all_targets) if all_targets else 0\n\n    print(\"\\nStatistiche globali del test set:\")\n    print(f\"Numero totale di immagini processate: {len(all_preds)}\")\n    print(f\"Numero totale di oggetti predetti: {total_preds}\")\n    print(f\"Numero totale di oggetti target: {total_targets}\")\n    print(f\"Numero medio di predizioni per immagine: {average_preds_per_image:.2f}\")\n    print(f\"Numero medio di oggetti target per immagine: {average_targets_per_image:.2f}\")\n\n    # Visualizza la confusion matrix\n    plot_confusion_matrix(all_preds, all_targets, class_names)\n\n    print(\"Testing completato.\")\n    return predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-12-29T20:27:20.295471Z","iopub.execute_input":"2024-12-29T20:27:20.295846Z","iopub.status.idle":"2024-12-29T20:27:20.306850Z","shell.execute_reply.started":"2024-12-29T20:27:20.295781Z","shell.execute_reply":"2024-12-29T20:27:20.306007Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def test_model(model, test_loader, device, class_names=categories, num_classes=12, score_threshold=0.5, iou_threshold=0.5):\n    \"\"\"\n    Testa il modello su un set di dati di test calcolando Precision, Recall e altre metriche, inclusa la confidenza media.\n    \"\"\"\n    model.to(device)\n    model.eval()  # Metti il modello in modalità di valutazione\n\n    true_positives = [0] * num_classes\n    false_positives = [0] * num_classes\n    false_negatives = [0] * num_classes\n\n    total_test_loss = 0.0\n    loss_count = 0\n    total_test_confidence = 0.0  # Per accumulare la confidenza media\n\n    test_loop = tqdm(test_loader, desc=\"Testing\", leave=False)\n\n    with torch.no_grad():\n        for images, targets in test_loop:\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # Calcolo delle predizioni\n            outputs = model(images)\n\n            # Calcolo della loss per monitoraggio\n            loss_dict = model(images, targets)\n            if isinstance(loss_dict, dict):\n                losses = sum(loss for loss in loss_dict.values())\n                total_test_loss += losses.item()\n                loss_count += 1\n\n            # Aggiorna le metriche per precision, recall\n            for target, output in zip(targets, outputs):\n                true_positives, false_positives, false_negatives = update_metrics(\n                    target, output, true_positives, false_positives, false_negatives,\n                    iou_threshold=iou_threshold, score_threshold=score_threshold\n                )\n\n                # Calcola la confidenza media per le predizioni corrette\n                total_test_confidence += compute_average_confidence(target, output, iou_threshold)\n\n    # Calcola la perdita media\n    avg_test_loss = total_test_loss / loss_count if loss_count > 0 else 0.0\n    avg_test_confidence = total_test_confidence / len(test_loader)  # Media della confidenza su tutti i batch\n\n    print(f\"Perdita media di test: {avg_test_loss:.4f}\")\n    print(f\"Confidenza media di test: {avg_test_confidence:.4f}\")\n    print(f\"Numero medio di predizioni per immagine: {average_preds_per_image:.2f}\")\n    print(f\"Numero medio di oggetti target per immagine: {average_targets_per_image:.2f}\")\n\n    # Visualizza la confusion matrix\n    plot_confusion_matrix(all_preds, all_targets, class_names)\n\n    return {\n        \"average_loss\": avg_test_loss,\n        \"true_positives\": true_positives,\n        \"false_positives\": false_positives,\n        \"false_negatives\": false_negatives,\n        \"average_confidence\": avg_test_confidence\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-12-29T20:18:11.588197Z","iopub.execute_input":"2024-12-29T20:18:11.588509Z","iopub.status.idle":"2024-12-29T20:18:11.599599Z","shell.execute_reply.started":"2024-12-29T20:18:11.588484Z","shell.execute_reply":"2024-12-29T20:18:11.598826Z"}}},{"cell_type":"code","source":"def visualize_predictions(image, boxes, labels, scores, class_names, threshold=0.5):\n    \"\"\"\n    Visualizza le predizioni (bounding boxes e etichette) su una singola immagine.\n    \"\"\"\n    fig, ax = plt.subplots(1, figsize=(12, 9))\n    ax.imshow(image)\n\n    # Filtra predizioni con punteggio sopra la soglia\n    for box, label, score in zip(boxes, labels, scores):\n        if score >= threshold:\n            # Rettangolo per la bounding box\n            rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n                                      linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n\n            # Etichetta con classe e punteggio\n            ax.text(box[0], box[1] - 10, f'{class_names[label]}: {score:.2f}', color='r',\n                    fontsize=12, fontweight='bold', backgroundcolor='white')\n\n    plt.show()\n\ndef plot_confusion_matrix(all_preds, all_targets, class_names, score_threshold=0.5):\n    \"\"\"\n    Calcola e visualizza la confusion matrix.\n    \"\"\"\n    pred_labels = []\n    true_labels = []\n\n    for pred, target in zip(all_preds, all_targets):\n        # Filtra le predizioni in base al punteggio\n        valid_indices = pred['scores'] >= score_threshold\n        filtered_pred_labels = pred['labels'][valid_indices].cpu().numpy()\n\n        # Aggiungi tutte le etichette previste e reali\n        pred_labels.extend(filtered_pred_labels)\n        true_labels.extend(target['labels'].cpu().numpy())\n\n    # Assicurati che il numero di previsioni e target sia coerente\n    if len(pred_labels) != len(true_labels):\n        min_len = min(len(pred_labels), len(true_labels))\n        pred_labels = pred_labels[:min_len]\n        true_labels = true_labels[:min_len]\n\n    # Calcola la matrice di confusione\n    cm = confusion_matrix(true_labels, pred_labels, labels=range(len(class_names)))\n    #cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizza per riga\n\n    # Visualizza la matrice di confusione\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n    fig, ax = plt.subplots(figsize=(10, 10))\n    disp.plot(ax=ax, cmap=\"Blues\", values_format='d')\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n\n    #sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T20:31:14.887257Z","iopub.execute_input":"2024-12-29T20:31:14.887593Z","iopub.status.idle":"2024-12-29T20:31:14.896430Z","shell.execute_reply.started":"2024-12-29T20:31:14.887563Z","shell.execute_reply":"2024-12-29T20:31:14.895707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = test_model(model, test_loader, device, class_names=categories, num_classes=num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T20:31:14.897533Z","iopub.execute_input":"2024-12-29T20:31:14.897835Z","iopub.status.idle":"2024-12-29T20:34:48.094332Z","shell.execute_reply.started":"2024-12-29T20:31:14.897804Z","shell.execute_reply":"2024-12-29T20:34:48.093364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(net, test_loader, criterion, device, path_min_loss, json_path, original_img_path, reference_json_path):\n    \"\"\"\n    Funzione per testare il modello e visualizzare i risultati su immagini originali.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    :param json_path: Percorso al file JSON contenente i dettagli delle immagini del test set.\n    :param original_img_path: Percorso al folder delle immagini originali.\n    :param reference_json_path: Percorso al file JSON contenente le informazioni delle immagini originali.\n    \"\"\"\n    # Carica il miglior modello salvato (con il parametro weights_only=True per evitare il warning)\n    net.load_state_dict(torch.load(path_min_loss, map_location=device))\n    net.eval()\n\n    test_loss = 0.0\n    correct_test = 0\n    total_test = 0\n\n    all_labels = []\n    all_predictions = []\n\n    # Carica i file JSON\n    with open(json_path, 'r') as f:\n        test_data = json.load(f)\n\n    with open(reference_json_path, 'r') as f:\n        reference_data = json.load(f)\n\n    # Crea un dizionario per una ricerca rapida delle immagini originali\n    id_to_filename = {img['id']: img['file_name'] for img in reference_data['images']}\n\n    # Crea un dizionario per raggruppare i bounding box per image_id\n    image_id_to_bboxes = {}\n    for image_info in test_data:\n        image_id = image_info['image_id']\n        region_bbox = image_info['region_bbox']\n        if image_id not in image_id_to_bboxes:\n            image_id_to_bboxes[image_id] = []\n        image_id_to_bboxes[image_id].append(region_bbox)\n\n    with torch.no_grad():\n        for idx, (images, labels) in enumerate(test_loader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            test_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_test += labels.size(0)\n            correct_test += predicted.eq(labels).sum().item()\n\n            # Salva tutte le etichette e predizioni\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n            # Mostra alcuni esempi\n            if idx < 5:  # Mostra i primi 5 batch\n                for i in range(min(len(images), 3)):  # Mostra fino a 3 immagini per batch\n                    image_info = test_data[idx * len(images) + i]  # Recupera info immagine dal JSON\n                    image_id = image_info['image_id']\n\n                    # Trova il file_name usando l'image_id\n                    file_name = id_to_filename.get(image_id)\n                    if not file_name:\n                        print(f\"Immagine con ID {image_id} non trovata nel reference JSON.\")\n                        continue\n\n                    # Percorso dell'immagine originale\n                    img_path = os.path.join(original_img_path, file_name)\n\n                    # Carica l'immagine originale\n                    img = cv2.imread(img_path)\n                    if img is None:\n                        print(f\"Immagine non trovata: {img_path}\")\n                        continue\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n                    # Disegna i bounding box per le etichette reali (blu)\n                    bboxes_real = image_id_to_bboxes.get(image_id, [])\n                    for bbox in bboxes_real:\n                        xmin, ymin, xmax, ymax = bbox\n                        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)  # Blu per reale\n\n                    # Mostra l'immagine con i bounding box (senza le etichette scritte nell'immagine)\n                    plt.figure(figsize=(6, 6))\n                    plt.imshow(img)\n                    plt.axis('off')\n                    plt.show()\n\n                    # Ora stampa le etichette predette e reali sotto l'immagine\n                    print(f\"Predizioni vs Realtà per l'immagine {file_name}:\")\n                    for j, bbox in enumerate(bboxes_real):\n                        pred_label = predicted[i].item()\n                        # Stampa i valori predetti e reali\n                        print(f\"  Bounding Box {j + 1}: Predetto: {pred_label}, Reale: {labels[i].item()}\")\n\n    avg_test_loss = test_loss / len(test_loader.dataset)\n    test_accuracy = 100. * correct_test / total_test\n\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n\n    # Calcola e visualizza la matrice di confusione\n    cm = confusion_matrix(all_labels, all_predictions)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizza per riga\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T20:34:48.096626Z","iopub.execute_input":"2024-12-29T20:34:48.096909Z","iopub.status.idle":"2024-12-29T20:34:48.109929Z","shell.execute_reply.started":"2024-12-29T20:34:48.096886Z","shell.execute_reply":"2024-12-29T20:34:48.109169Z"}},"outputs":[],"execution_count":null}]}