{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\nimport torchvision.transforms as transforms\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\nfrom torchvision import models\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torch import optim\nimport torch\nimport json\nimport random\nfrom collections import defaultdict\nimport ast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:43:52.607878Z","iopub.execute_input":"2024-12-09T16:43:52.608370Z","iopub.status.idle":"2024-12-09T16:43:52.618445Z","shell.execute_reply.started":"2024-12-09T16:43:52.608335Z","shell.execute_reply":"2024-12-09T16:43:52.617239Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nOUT_COCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\ncfg_fldr_pth = Path(f'/kaggle/input/our-xview-dataset/{OUT_CFG_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / OUT_COCO_JSON_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\n#DATASET\ntrain_path = '/kaggle/working/train.json'\ntest_path = '/kaggle/working/test.json'\nval_path = '/kaggle/working/val.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:43:55.351970Z","iopub.execute_input":"2024-12-09T16:43:55.352329Z","iopub.status.idle":"2024-12-09T16:43:55.358179Z","shell.execute_reply.started":"2024-12-09T16:43:55.352299Z","shell.execute_reply":"2024-12-09T16:43:55.357272Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:43:58.722240Z","iopub.execute_input":"2024-12-09T16:43:58.722604Z","iopub.status.idle":"2024-12-09T16:43:58.799473Z","shell.execute_reply.started":"2024-12-09T16:43:58.722573Z","shell.execute_reply":"2024-12-09T16:43:58.798613Z"}},"outputs":[{"name":"stdout","text":"Cartella /kaggle/working pulita.\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:43:59.820890Z","iopub.execute_input":"2024-12-09T16:43:59.821685Z","iopub.status.idle":"2024-12-09T16:43:59.825471Z","shell.execute_reply.started":"2024-12-09T16:43:59.821653Z","shell.execute_reply":"2024-12-09T16:43:59.824619Z"}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"def split_stratified(json_file, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n    # Carica il JSON\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    \n    # Raggruppare le annotazioni per category_id\n    category_images = defaultdict(list)\n    for annotation in data['annotations']:\n        category_id = annotation['category_id']\n        image_id = annotation['image_id']\n        category_images[category_id].append(image_id)\n    \n    # Genera gli split per ogni category_id\n    train_images, val_images, test_images = set(), set(), set()\n    for category_id, image_ids in category_images.items():\n        # Mescola gli image_id\n        random.shuffle(image_ids)\n        \n        # Calcola i limiti per train, validation, e test\n        total = len(image_ids)\n        train_end = int(total * train_ratio)\n        val_end = int(total * (train_ratio + val_ratio))\n        \n        # Aggiungi agli split\n        train_images.update(image_ids[:train_end])\n        val_images.update(image_ids[train_end:val_end])\n        test_images.update(image_ids[val_end:])\n    \n    # Filtra le immagini e annotazioni per ciascuno split\n    def filter_data(split_images):\n        filtered_images = [image for image in data['images'] if image['id'] in split_images]\n        filtered_annotations = [annotation for annotation in data['annotations'] if annotation['image_id'] in split_images]\n        return {'images': filtered_images, 'annotations': filtered_annotations, 'categories': data['categories']}\n    \n    # Crea i nuovi JSON per train, validation, e test\n    train_data = filter_data(train_images)\n    val_data = filter_data(val_images)\n    test_data = filter_data(test_images)\n    \n    # Salva i file JSON\n    with open('train.json', 'w') as f:\n        json.dump(train_data, f, indent=4)\n    \n    with open('val.json', 'w') as f:\n        json.dump(val_data, f, indent=4)\n    \n    with open('test.json', 'w') as f:\n        json.dump(test_data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:44:01.109975Z","iopub.execute_input":"2024-12-09T16:44:01.110674Z","iopub.status.idle":"2024-12-09T16:44:01.119520Z","shell.execute_reply.started":"2024-12-09T16:44:01.110640Z","shell.execute_reply":"2024-12-09T16:44:01.118414Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Chiamata della funzione\nsplit_stratified(coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:44:02.869438Z","iopub.execute_input":"2024-12-09T16:44:02.869838Z","iopub.status.idle":"2024-12-09T16:44:21.600515Z","shell.execute_reply.started":"2024-12-09T16:44:02.869808Z","shell.execute_reply":"2024-12-09T16:44:21.599738Z"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchvision import transforms\nimport ast\n\nclass CustomDataset(Dataset):\n    def __init__(self, coco_json_file, img_dir, aug=False):\n        \"\"\"\n        Inizializza il dataset personalizzato.\n        Args:\n        - coco_json_file: Il file JSON contenente le annotazioni.\n        - img_dir: La cartella delle immagini.\n        - aug: Booleano per attivare o meno l'augmentazione.\n        \"\"\"\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n        \n        # Carica il file JSON delle annotazioni\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n        \n        # Crea una struttura per le annotazioni\n        self.image_annotations = {}\n        self.image_bboxes = {}\n        \n        # Estrai le classi (categorie) dal file JSON\n        self.classes = {}\n        for category in coco_data['categories']:\n            for key, value in category.items():\n                self.classes[int(key)] = value  # Associa l'ID categoria al nome\n        \n        # Aggiungi la mappa di annotazioni\n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox = annotation['bbox']  # Formato COCO [x_min, y_min, width, height]\n            \n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n            \n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n        \n        # Mappa per associare ID immagine a file_name\n        self.image_info = {\n            int(generate_id(image['file_name'])): image['file_name']\n            for image in coco_data['images']\n        }\n        \n        # Salva i percorsi delle immagini nel formato richiesto\n        self.img_dir = img_dir\n        self.image_paths = [\n            os.path.join(img_dir, image['file_name'])\n            for image in coco_data['images']\n        ]\n        \n        # Trasformazioni di base e di augmentation\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),   \n        ])\n        \n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ]) \n        \n        self.aug = aug\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        # Estrai il nome dell'immagine e l'ID corrispondente\n        img_path = self.image_paths[index]\n        img_name = os.path.basename(img_path)\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n        \n        if img_id not in self.image_info:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n        \n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n        \n        # Carica l'immagine\n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n        \n        # Applica le trasformazioni\n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n        \n        # Estrai le annotazioni e i bounding boxes\n        categories = self.image_annotations.get(img_id, [])\n        bboxes = self.image_bboxes.get(img_id, [])\n        \n        if not bboxes:  # Immagini senza annotazioni\n            target = {\n                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n                \"labels\": torch.zeros((0,), dtype=torch.int64)\n            }\n        else:\n            # Converte da formato COCO [x_min, y_min, width, height] a [x_min, y_min, x_max, y_max]\n            scale_x = 320 / original_width\n            scale_y = 320 / original_height\n            \n            # Scaling dei bounding boxes\n            scaled_bboxes = []\n            for bbox_str in bboxes:\n                bbox = ast.literal_eval(bbox_str)  # Converte la stringa in una lista\n                x_min, y_min, width, height = bbox\n                x_max = x_min + width\n                y_max = y_min + height\n                \n                # Verifica che x_min < x_max e y_min < y_max\n                if x_min >= x_max or y_min >= y_max:\n                    # Se il bounding box non è valido, salta questa immagine\n                    return None, None\n                \n                scaled_bboxes.append(torch.tensor([  \n                    float(x_min) * scale_x,               # x_min\n                    float(y_min) * scale_y,               # y_min\n                    float(x_max) * scale_x,               # x_max\n                    float(y_max) * scale_y                # y_max\n                ], dtype=torch.float32))\n            \n            target = {\n                \"boxes\": torch.stack(scaled_bboxes),\n                \"labels\": torch.tensor(categories, dtype=torch.int64)\n            }\n        \n        return image_tensor, target","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:25:14.123780Z","iopub.execute_input":"2024-12-09T18:25:14.124153Z","iopub.status.idle":"2024-12-09T18:25:14.139571Z","shell.execute_reply.started":"2024-12-09T18:25:14.124084Z","shell.execute_reply":"2024-12-09T18:25:14.138543Z"}},"outputs":[],"execution_count":192},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione di collation per il DataLoader, utile per il batching di immagini e annotazioni.\n    La funzione restituirà un batch di immagini e un batch di target, formattato correttamente per Faster R-CNN.\n    \n    Args:\n    - batch: lista di tuple (image, target)\n    \n    Returns:\n    - images: batch di immagini\n    - targets: lista di dizionari contenenti le annotazioni per ogni immagine\n    \"\"\"\n    # Separa immagini e target\n    images, targets = zip(*batch)\n\n    # Converte la lista di immagini in un batch di immagini\n    images = list(images)\n\n    # Restituisci il batch\n    return images, list(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:25:15.991017Z","iopub.execute_input":"2024-12-09T18:25:15.991439Z","iopub.status.idle":"2024-12-09T18:25:15.996382Z","shell.execute_reply.started":"2024-12-09T18:25:15.991393Z","shell.execute_reply":"2024-12-09T18:25:15.995425Z"}},"outputs":[],"execution_count":193},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_path, img_fldr,  aug=True)\nvalid_dataset = CustomDataset(val_path, img_fldr, aug=False)  \ntest_dataset = CustomDataset(test_path, img_fldr, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:25:17.366644Z","iopub.execute_input":"2024-12-09T18:25:17.367325Z","iopub.status.idle":"2024-12-09T18:25:22.244225Z","shell.execute_reply.started":"2024-12-09T18:25:17.367291Z","shell.execute_reply":"2024-12-09T18:25:22.243265Z"}},"outputs":[],"execution_count":194},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"code","source":"# Numero totale di campioni per ogni DataLoader\ntrain_size = len(train_loader.dataset)\nval_size = len(val_loader.dataset)\ntest_size = len(test_loader.dataset)\n\n# Numero di batch per ogni DataLoader\ntrain_batches = len(train_loader)\nval_batches = len(val_loader)\ntest_batches = len(test_loader)\n\n# Visualizza i risultati\nprint(f\"Numero totale di elementi nel train_loader: {train_size}\")\nprint(f\"Numero totale di batch nel train_loader: {train_batches}\")\nprint(f\"Numero totale di elementi nel val_loader: {val_size}\")\nprint(f\"Numero totale di batch nel val_loader: {val_batches}\")\nprint(f\"Numero totale di elementi nel test_loader: {test_size}\")\nprint(f\"Numero totale di batch nel test_loader: {test_batches}\")\n\n# Somma totale degli elementi nei DataLoader\ntotal_elements = train_size + val_size + test_size\nprint(f\"Numero totale di elementi in tutti i DataLoader: {total_elements}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:25:22.246182Z","iopub.execute_input":"2024-12-09T18:25:22.246921Z","iopub.status.idle":"2024-12-09T18:25:22.253994Z","shell.execute_reply.started":"2024-12-09T18:25:22.246877Z","shell.execute_reply":"2024-12-09T18:25:22.253073Z"}},"outputs":[{"name":"stdout","text":"Numero totale di elementi nel train_loader: 31248\nNumero totale di batch nel train_loader: 15624\nNumero totale di elementi nel val_loader: 19664\nNumero totale di batch nel val_loader: 9832\nNumero totale di elementi nel test_loader: 19803\nNumero totale di batch nel test_loader: 9902\nNumero totale di elementi in tutti i DataLoader: 70715\n","output_type":"stream"}],"execution_count":195},{"cell_type":"markdown","source":"# Modello Faster R-CNN (Resnet50)","metadata":{}},{"cell_type":"code","source":"def compute_class_weights(dataset):\n    # Conta la frequenza di ogni classe nel dataset\n    class_counts = np.zeros(len(dataset.classes))  # Non consideriamo lo sfondo, quindi senza +1\n    \n    # Usa tqdm per monitorare il progresso mentre si itera sul dataset\n    for _, targets in tqdm(dataset, desc=\"Calcolo frequenze delle classi\", leave=False):\n        # Controlla se targets è None\n        if targets is None:\n            continue\n        \n        for target in targets['labels']:  # Assumendo che 'labels' contenga le etichette delle classi\n            if target != 0:  # Ignora lo sfondo nelle etichette\n                class_counts[target] += 1\n\n    # Calcola i pesi per le classi\n    total_count = sum(class_counts)  # Totale delle etichette (senza sfondo)\n    \n    # Inizializza il peso per lo sfondo\n    background_weight = 0.2  # Assegna un peso fisso per lo sfondo (sfondo è il 20% del dataset)\n    class_counts_with_background = np.copy(class_counts)\n    class_counts_with_background = np.append(class_counts_with_background, 0)  # Aggiungi lo sfondo\n    class_counts_with_background[0] = background_weight * total_count  # Aggiorna il conteggio dello sfondo\n\n    # Calcola i pesi inversamente proporzionali alla frequenza\n    # Evita divisione per zero, mettendo i pesi delle classi con count zero a un valore molto alto\n    class_weights = np.divide(total_count, class_counts_with_background, where=class_counts_with_background != 0)\n\n    return class_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:26:31.647688Z","iopub.execute_input":"2024-12-09T18:26:31.648055Z","iopub.status.idle":"2024-12-09T18:26:31.654520Z","shell.execute_reply.started":"2024-12-09T18:26:31.648023Z","shell.execute_reply":"2024-12-09T18:26:31.653703Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"def train_and_validate(model, train_loader, val_loader, optimizer, device, class_weights, num_epochs=10, save_model=True):\n    \"\"\"\n    Funzione per il training e la validazione del modello Faster R-CNN con pesi delle classi.\n    \"\"\"\n    model.to(device)\n    losses_per_epoch = []\n\n    # Assicurati che i pesi siano un tensor PyTorch\n    class_weights = torch.tensor(class_weights).to(device)\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoca {epoch + 1}/{num_epochs}\")\n        model.train()\n        total_loss = 0\n\n        # Training loop con tqdm\n        train_loop = tqdm(train_loader, desc=\"Training\", leave=False)\n        for images, targets in train_loop:\n            # Controlla se le immagini sono valide (non None)\n            if images is None or targets is None:\n                continue\n            \n            images = [img.to(device) if img is not None else None for img in images]\n            targets = [{k: v.to(device) if v is not None else None for k, v in t.items()} for t in targets]\n\n            # Calcola le perdite\n            loss_dict = model(images, targets)\n            \n            # Pondera le perdite per le classi\n            # In questo caso, moltiplichiamo solo i contributi alla loss delle classi\n            losses = loss_dict['loss_classifier'] + loss_dict['loss_box_reg'] + loss_dict['loss_objectness'] + loss_dict['loss_rpn_box_reg']\n\n            # Se ci sono altre perdite legate alle classi, ponderale separatamente\n            for key in loss_dict:\n                if 'loss' in key and key != 'loss_classifier' and key != 'loss_box_reg' and key != 'loss_objectness' and key != 'loss_rpn_box_reg':\n                    losses += loss_dict[key] * class_weights\n\n            # Verifica se losses è un oggetto tensor\n            optimizer.zero_grad()\n            losses.backward() \n            optimizer.step()\n\n            total_loss += losses.item()\n            train_loop.set_postfix(loss=losses.item())\n\n        losses_per_epoch.append(total_loss)\n        print(f\"Perdita Totale per l'epoca {epoch + 1}: {total_loss:.4f}\")\n\n        # Validazione\n        model.eval()\n        val_loop = tqdm(val_loader, desc=\"Validazione\", leave=False)\n        with torch.no_grad():\n            for images, targets in val_loop:\n                # Controlla se le immagini sono valide (non None)\n                if images is None or targets is None:\n                    continue\n                \n                images = [img.to(device) if img is not None else None for img in images]\n                targets = [{k: v.to(device) if v is not None else None for k, v in t.items()} for t in targets]\n                predictions = model(images)\n                val_loop.set_postfix(processed=len(predictions))  # Placeholder per metriche future\n\n        # Salva il modello\n        if save_model:\n            torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")\n            print(f\"Modello salvato: model_epoch_{epoch + 1}.pth\")\n\n    return losses_per_epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:30:07.573601Z","iopub.execute_input":"2024-12-09T18:30:07.574305Z","iopub.status.idle":"2024-12-09T18:30:07.585125Z","shell.execute_reply.started":"2024-12-09T18:30:07.574270Z","shell.execute_reply":"2024-12-09T18:30:07.584142Z"}},"outputs":[],"execution_count":208},{"cell_type":"code","source":"def test_model(model, test_loader, device):\n    \"\"\"\n    Funzione per il testing del modello Faster R-CNN.\n    \n    Args:\n    - model: il modello Faster R-CNN.\n    - test_loader: DataLoader per il test set.\n    - device: dispositivo su cui eseguire (es. 'cuda' o 'cpu').\n    \n    Returns:\n    - predictions: lista delle predizioni per ogni batch (include 'boxes', 'labels', 'scores').\n    \"\"\"\n    model.to(device)\n    model.eval()\n    predictions = []\n    \n    print(\"\\nInizio testing...\")\n    test_loop = tqdm(test_loader, desc=\"Testing\", leave=False)\n    \n    with torch.no_grad():\n        for images, _ in test_loop:  # Durante il test, i target possono essere ignorati\n            images = [img.to(device) for img in images]\n            preds = model(images)\n            \n            # Predizioni di ciascun batch (contenente 'boxes', 'labels', 'scores')\n            # Le predizioni sono in un formato di lista di dizionari\n            for pred in preds:\n                predictions.append({\n                    'boxes': pred['boxes'].cpu().numpy(),\n                    'labels': pred['labels'].cpu().numpy(),\n                    'scores': pred['scores'].cpu().numpy()\n                })\n            \n            # Aggiungi aggiornamenti su quante predizioni sono state processate\n            test_loop.set_postfix(processed=len(predictions))\n\n    print(\"Testing completato.\")\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:30:09.992404Z","iopub.execute_input":"2024-12-09T18:30:09.993236Z","iopub.status.idle":"2024-12-09T18:30:09.998962Z","shell.execute_reply.started":"2024-12-09T18:30:09.993201Z","shell.execute_reply":"2024-12-09T18:30:09.998045Z"}},"outputs":[],"execution_count":209},{"cell_type":"code","source":"# Carica il modello Faster R-CNN con ResNet50 e FPN\nmodel = fasterrcnn_resnet50_fpn(pretrained=False)\n\nnum_classes = 12  # 11 classi + 1 per il background\n\n# Modifica il numero di classi in output\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\n# Imposta il dispositivo (GPU o CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Configurazione training\nnum_epochs = 2\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:25:29.205666Z","iopub.execute_input":"2024-12-09T18:25:29.206362Z","iopub.status.idle":"2024-12-09T18:25:29.878611Z","shell.execute_reply.started":"2024-12-09T18:25:29.206328Z","shell.execute_reply":"2024-12-09T18:25:29.877617Z"}},"outputs":[],"execution_count":199},{"cell_type":"code","source":"# Calcola i pesi delle classi\nclass_weights = compute_class_weights(train_loader.dataset)\n\nprint(class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:26:37.682727Z","iopub.execute_input":"2024-12-09T18:26:37.683436Z","iopub.status.idle":"2024-12-09T18:28:23.575581Z","shell.execute_reply.started":"2024-12-09T18:26:37.683402Z","shell.execute_reply":"2024-12-09T18:28:23.574565Z"}},"outputs":[{"name":"stderr","text":"                                                                                      ","output_type":"stream"},{"name":"stdout","text":"[5.00000000e+00 2.96286864e+00 1.93723199e+01 1.58589416e+02\n 1.07289276e+02 1.24362542e+02 1.73934526e+00 4.47539333e+03\n 3.31510617e+02 1.18963140e+02 1.59455819e+03 0.00000000e+00]\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":202},{"cell_type":"code","source":"# Esegui il training\nlosses_per_epoch = train_and_validate(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    optimizer=optimizer,\n    device=device,\n    class_weights=class_weights,\n    num_epochs=num_epochs,\n    save_model=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:30:11.856884Z","iopub.execute_input":"2024-12-09T18:30:11.857280Z"}},"outputs":[{"name":"stdout","text":"\nEpoca 1/2\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|▏         | 211/15624 [01:22<1:45:08,  2.44it/s, loss=2.01] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Testing\ntest_predictions = test_model(\n    model=model,\n    test_loader=test_loader_frcc,\n    device=device\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}