{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\nimport torchvision.transforms as transforms\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\nfrom torchvision import models\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torch import optim\nimport torch\nimport json\nimport random\nfrom collections import defaultdict\nimport ast\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom collections import defaultdict\n\nimport json\nimport ast\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:22.710913Z","iopub.execute_input":"2024-12-09T22:16:22.711277Z","iopub.status.idle":"2024-12-09T22:16:22.720116Z","shell.execute_reply.started":"2024-12-09T22:16:22.711244Z","shell.execute_reply":"2024-12-09T22:16:22.719225Z"}},"outputs":[],"execution_count":101},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nCOCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_COCO_JSON_NM = 'mod_COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\ncfg_fldr_pth = Path(f'/kaggle/input/our-xview-dataset/{OUT_CFG_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / COCO_JSON_NM\nnew_coco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\n#DATASET\ntrain_path = '/kaggle/working/train.json'\ntest_path = '/kaggle/working/test.json'\nval_path = '/kaggle/working/val.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:22.737439Z","iopub.execute_input":"2024-12-09T22:16:22.738005Z","iopub.status.idle":"2024-12-09T22:16:22.743605Z","shell.execute_reply.started":"2024-12-09T22:16:22.737977Z","shell.execute_reply":"2024-12-09T22:16:22.742744Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:22.792162Z","iopub.execute_input":"2024-12-09T22:16:22.792991Z","iopub.status.idle":"2024-12-09T22:16:22.900160Z","shell.execute_reply.started":"2024-12-09T22:16:22.792959Z","shell.execute_reply":"2024-12-09T22:16:22.899350Z"}},"outputs":[{"name":"stdout","text":"Cartella /kaggle/working pulita.\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:22.901819Z","iopub.execute_input":"2024-12-09T22:16:22.902195Z","iopub.status.idle":"2024-12-09T22:16:22.906346Z","shell.execute_reply.started":"2024-12-09T22:16:22.902151Z","shell.execute_reply":"2024-12-09T22:16:22.905402Z"}},"outputs":[],"execution_count":104},{"cell_type":"markdown","source":"# Background","metadata":{}},{"cell_type":"code","source":"def process_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione che prende il JSON COCO da un file di input, aggiunge un'annotazione di background per le immagini senza bbox,\n    aggiunge la categoria \"background\" in formato mapping id:name, e salva il JSON modificato nel file di output.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    with open(input_path, 'r') as f:\n        data = json.load(f)\n\n    # Ottieni le categorie\n    categories = data.get('categories', [])\n\n    # Rimappa la categoria \"Aircraft\" da id 0 a id 11\n    for category in categories:\n        if \"0\" in category and category[\"0\"] == \"Aircraft\":\n            category[\"0\"] = \"background\"\n    \n    categories.append({\"11\": \"Aircraft\"})\n\n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n\n    # Crea un set per verificare quali immagini hanno annotazioni\n    annotated_images = {ann['image_id'] for ann in data.get('annotations', [])}\n\n    # Lista di nuove annotazioni da aggiungere per le immagini senza bbox\n    new_annotations = []\n\n    for image in data.get('images', []):\n        image_id = image['id']\n        if image_id not in annotated_images:\n            # Crea una nuova annotazione per immagini senza annotazioni\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image_id,\n                'category_id': 0,  # Categoria background con ID 0\n                'area': float(image['width'] * image['height']),  # Area della bbox\n                'bbox': str([0.0, 0.0, float(image['width']), float(image['height'])])  # Bounding box come array\n            }\n            new_annotations.append(new_annotation)\n\n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n\n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:22.907467Z","iopub.execute_input":"2024-12-09T22:16:22.907764Z","iopub.status.idle":"2024-12-09T22:16:22.919034Z","shell.execute_reply.started":"2024-12-09T22:16:22.907738Z","shell.execute_reply":"2024-12-09T22:16:22.918206Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"process_coco_json(coco_json_pth, new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:22.920999Z","iopub.execute_input":"2024-12-09T22:16:22.921611Z","iopub.status.idle":"2024-12-09T22:16:30.565748Z","shell.execute_reply.started":"2024-12-09T22:16:22.921543Z","shell.execute_reply":"2024-12-09T22:16:30.564764Z"}},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"def split_stratified(json_file, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n    # Carica il JSON\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    \n    # Raggruppare le annotazioni per category_id\n    category_images = defaultdict(list)\n    \n    # Aggiungi solo le annotazioni valide (dove il bbox è valido)\n    valid_annotations = []\n    for annotation in data['annotations']:\n        # Converte la stringa del bbox in una lista\n        bbox = ast.literal_eval(annotation['bbox'])  # Converti la stringa in lista\n        x_min, y_min, width, height = bbox\n        x_max = x_min + width\n        y_max = y_min + height\n        \n        if x_min <= x_max and y_min <= y_max:  # Bounding box valido\n            valid_annotations.append(annotation)\n            category_id = annotation['category_id']\n            image_id = annotation['image_id']\n            category_images[category_id].append(image_id)\n    \n    # Genera gli split per ogni category_id\n    train_images, val_images, test_images = set(), set(), set()\n    for category_id, image_ids in category_images.items():\n        # Mescola gli image_id\n        random.shuffle(image_ids)\n        \n        # Calcola i limiti per train, validation, e test\n        total = len(image_ids)\n        train_end = int(total * train_ratio)\n        val_end = int(total * (train_ratio + val_ratio))\n        \n        # Aggiungi agli split\n        train_images.update(image_ids[:train_end])\n        val_images.update(image_ids[train_end:val_end])\n        test_images.update(image_ids[val_end:])\n    \n    # Filtra le immagini e annotazioni per ciascuno split\n    def filter_data(split_images):\n        # Filtra solo le immagini e annotazioni con image_id presente in split_images\n        filtered_images = [image for image in data['images'] if image['id'] in split_images]\n        filtered_annotations = [annotation for annotation in valid_annotations if annotation['image_id'] in split_images]\n        return {'images': filtered_images, 'annotations': filtered_annotations, 'categories': data['categories']}\n    \n    # Crea i nuovi JSON per train, validation, e test\n    train_data = filter_data(train_images)\n    val_data = filter_data(val_images)\n    test_data = filter_data(test_images)\n    \n    # Salva i file JSON\n    with open('train.json', 'w') as f:\n        json.dump(train_data, f, indent=4)\n    \n    with open('val.json', 'w') as f:\n        json.dump(val_data, f, indent=4)\n    \n    with open('test.json', 'w') as f:\n        json.dump(test_data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:30.567040Z","iopub.execute_input":"2024-12-09T22:16:30.567337Z","iopub.status.idle":"2024-12-09T22:16:30.577385Z","shell.execute_reply.started":"2024-12-09T22:16:30.567307Z","shell.execute_reply":"2024-12-09T22:16:30.576497Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"# Chiamata della funzione\nsplit_stratified(new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:30.578413Z","iopub.execute_input":"2024-12-09T22:16:30.578688Z","iopub.status.idle":"2024-12-09T22:16:57.566991Z","shell.execute_reply.started":"2024-12-09T22:16:30.578662Z","shell.execute_reply":"2024-12-09T22:16:57.566264Z"}},"outputs":[],"execution_count":108},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, coco_json_file, img_dir, aug=False):\n        \"\"\"\n        Inizializza il dataset personalizzato.\n        Args:\n        - coco_json_file: Il file JSON contenente le annotazioni.\n        - img_dir: La cartella delle immagini.\n        - aug: Booleano per attivare o meno l'augmentazione.\n        - save_filtered_json: Se True, salva un file JSON filtrato.\n        - filtered_json_path: Percorso del file JSON filtrato, se `save_filtered_json` è True.\n        \"\"\"\n        def generate_id(file_name):\n            return file_name.replace('_', '').replace('.jpg', '').replace('img', '')\n        \n        # Carica il file JSON delle annotazioni\n        with open(coco_json_file, 'r') as f:\n            coco_data = json.load(f)\n        \n        # Crea una struttura per le annotazioni\n        self.image_annotations = {}\n        self.image_bboxes = {}\n        \n        # Estrai le classi (categorie) dal file JSON\n        self.classes = {}\n        for category in coco_data['categories']:\n            # Associa l'ID della categoria al nome, usando la chiave numerica come ID\n            for category_id, category_name in category.items():\n                self.classes[category_id] = category_name  # Associa l'ID categoria al nome\n        \n        # Filtra le annotazioni con bounding box validi\n        valid_annotations = []\n        for annotation in coco_data['annotations']:\n            # Converti la stringa del bbox in una lista (se è una stringa)\n            bbox = annotation['bbox']\n            if isinstance(bbox, str):\n                bbox = json.loads(bbox)  # Converte la stringa in lista\n            x_min, y_min, width, height = bbox\n            x_max = x_min + width\n            y_max = y_min + height\n            \n            if x_min < x_max and y_min < y_max:  # Bounding box valido\n                valid_annotations.append(annotation)\n        \n        # Aggiungi la mappa di annotazioni valide\n        for annotation in valid_annotations:\n            image_id = annotation['image_id']\n            category_id = annotation['category_id']\n            bbox = annotation['bbox']  # Formato COCO [x_min, y_min, width, height]\n            \n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n            \n            self.image_annotations[image_id].append(category_id)\n            self.image_bboxes[image_id].append(bbox)\n        \n        # Mappa per associare ID immagine a file_name\n        self.image_info = {\n            image['id']: image['file_name']\n            for image in coco_data['images']\n        }\n        \n        # Filtra le immagini che non sono presenti nel JSON\n        self.img_dir = img_dir\n        self.image_paths = [\n            os.path.join(img_dir, image['file_name'])\n            for image in coco_data['images']\n            if image['id'] in self.image_info  # Prendi solo le immagini presenti nel JSON\n        ]\n        \n        # Trasformazioni di base e di augmentation\n        self.base_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),   \n        ])\n        \n        self.aug_transform = transforms.Compose([\n            transforms.Resize((320, 320)),\n            transforms.ToTensor(),\n        ]) \n        \n        self.aug = aug\n    \n    def save_filtered_json(self, coco_data, valid_annotations, filtered_json_path):\n        \"\"\"\n        Salva un file JSON con annotazioni filtrate.\n        Args:\n        - coco_data: Dati COCO originali.\n        - valid_annotations: Annotazioni valide filtrate.\n        - filtered_json_path: Percorso del file JSON filtrato.\n        \"\"\"\n        filtered_data = {\n            \"images\": coco_data[\"images\"],\n            \"annotations\": valid_annotations,\n            \"categories\": coco_data[\"categories\"]\n        }\n        with open(filtered_json_path, 'w') as f:\n            json.dump(filtered_data, f, indent=4)\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        # Estrai il nome dell'immagine e l'ID corrispondente\n        img_path = self.image_paths[index]\n        img_name = os.path.basename(img_path)\n        img_id = int(img_name.replace('_', '').replace('.jpg', '').replace('img', ''))\n        \n        if img_id not in self.image_info:\n            raise ValueError(f\"Immagine {img_name} non trovata nel file COCO\")\n        \n        if not os.path.exists(img_path):\n            raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n        \n        # Carica l'immagine\n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n        \n        # Applica le trasformazioni\n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n        \n        # Estrai le annotazioni e i bounding boxes\n        categories = self.image_annotations.get(img_id, [])\n        bboxes = self.image_bboxes.get(img_id, [])\n        \n        if not bboxes:  # Immagini senza annotazioni\n            target = {\n                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n                \"labels\": torch.zeros((0,), dtype=torch.int64)\n            }\n        else:\n            # Converte da formato COCO [x_min, y_min, width, height] a [x_min, y_min, x_max, y_max]\n            scale_x = 320 / original_width\n            scale_y = 320 / original_height\n            \n            # Scaling dei bounding boxes\n            scaled_bboxes = []\n            for bbox in bboxes:\n                if isinstance(bbox, str):\n                    bbox = json.loads(bbox)  # Converte la stringa in lista se necessario\n                x_min, y_min, width, height = bbox\n                x_max = x_min + width\n                y_max = y_min + height\n                \n                scaled_bboxes.append(torch.tensor([  \n                    float(x_min) * scale_x,               # x_min\n                    float(y_min) * scale_y,               # y_min\n                    float(x_max) * scale_x,               # x_max\n                    float(y_max) * scale_y                # y_max\n                ], dtype=torch.float32))\n            \n            target = {\n                \"boxes\": torch.stack(scaled_bboxes),\n                \"labels\": torch.tensor(categories, dtype=torch.int64)\n            }\n        \n        return image_tensor, target","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:57.569669Z","iopub.execute_input":"2024-12-09T22:16:57.569920Z","iopub.status.idle":"2024-12-09T22:16:57.586954Z","shell.execute_reply.started":"2024-12-09T22:16:57.569894Z","shell.execute_reply":"2024-12-09T22:16:57.586010Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione di collation per il DataLoader, utile per il batching di immagini e annotazioni.\n    La funzione restituirà un batch di immagini e un batch di target, formattato correttamente per Faster R-CNN.\n    \n    Args:\n    - batch: lista di tuple (image, target)\n    \n    Returns:\n    - images: batch di immagini\n    - targets: lista di dizionari contenenti le annotazioni per ogni immagine\n    \"\"\"\n    # Separa immagini e target\n    images, targets = zip(*batch)\n\n    # Converte la lista di immagini in un batch di immagini\n    images = list(images)\n\n    # Restituisci il batch\n    return images, list(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:57.587893Z","iopub.execute_input":"2024-12-09T22:16:57.588130Z","iopub.status.idle":"2024-12-09T22:16:57.601693Z","shell.execute_reply.started":"2024-12-09T22:16:57.588105Z","shell.execute_reply":"2024-12-09T22:16:57.600840Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_path, img_fldr,  aug=True)\nvalid_dataset = CustomDataset(val_path, img_fldr, aug=False)  \ntest_dataset = CustomDataset(test_path, img_fldr, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:16:57.602705Z","iopub.execute_input":"2024-12-09T22:16:57.603030Z","iopub.status.idle":"2024-12-09T22:17:06.897539Z","shell.execute_reply.started":"2024-12-09T22:16:57.602991Z","shell.execute_reply":"2024-12-09T22:17:06.896803Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"# Funzione per verificare i target con None\ndef check_none_target(data_loader):\n    none_count = 0\n    \n    for images, targets in data_loader:\n        for target in targets:\n            # Controlla se il target è vuoto\n            if target == None:  # Se il numero di box è 0, significa che non ci sono annotazioni\n                none_count += 1\n    \n    return none_count\n\n# Controlla i target nei DataLoader\ntrain_none_count = check_none_target(train_loader)\nval_none_count = check_none_target(val_loader)\ntest_none_count = check_none_target(test_loader)\n\nprint(f\"Numero di target con None nel train dataset: {train_none_count}\")\nprint(f\"Numero di target con None nel validation dataset: {val_none_count}\")\nprint(f\"Numero di target con None nel test dataset: {test_none_count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:24:51.706819Z","iopub.execute_input":"2024-12-09T22:24:51.707689Z","iopub.status.idle":"2024-12-09T22:28:53.337023Z","shell.execute_reply.started":"2024-12-09T22:24:51.707633Z","shell.execute_reply":"2024-12-09T22:28:53.336167Z"}},"outputs":[{"name":"stdout","text":"Numero di target con None nel train dataset: 0\nNumero di target con None nel validation dataset: 0\nNumero di target con None nel test dataset: 0\n","output_type":"stream"}],"execution_count":121},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"code","source":"# Numero totale di campioni per ogni DataLoader\ntrain_size = len(train_loader.dataset)\nval_size = len(val_loader.dataset)\ntest_size = len(test_loader.dataset)\n\n# Numero di batch per ogni DataLoader\ntrain_batches = len(train_loader)\nval_batches = len(val_loader)\ntest_batches = len(test_loader)\n\n# Visualizza i risultati\nprint(f\"Numero totale di elementi nel train_loader: {train_size}\")\nprint(f\"Numero totale di batch nel train_loader: {train_batches}\")\nprint(f\"Numero totale di elementi nel val_loader: {val_size}\")\nprint(f\"Numero totale di batch nel val_loader: {val_batches}\")\nprint(f\"Numero totale di elementi nel test_loader: {test_size}\")\nprint(f\"Numero totale di batch nel test_loader: {test_batches}\")\n\n# Somma totale degli elementi nei DataLoader\ntotal_elements = train_size + val_size + test_size\nprint(f\"Numero totale di elementi in tutti i DataLoader: {total_elements}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:17:06.898573Z","iopub.execute_input":"2024-12-09T22:17:06.898833Z","iopub.status.idle":"2024-12-09T22:17:06.904862Z","shell.execute_reply.started":"2024-12-09T22:17:06.898806Z","shell.execute_reply":"2024-12-09T22:17:06.903931Z"}},"outputs":[{"name":"stdout","text":"Numero totale di elementi nel train_loader: 42172\nNumero totale di batch nel train_loader: 21086\nNumero totale di elementi nel val_loader: 21054\nNumero totale di batch nel val_loader: 10527\nNumero totale di elementi nel test_loader: 21177\nNumero totale di batch nel test_loader: 10589\nNumero totale di elementi in tutti i DataLoader: 84403\n","output_type":"stream"}],"execution_count":112},{"cell_type":"markdown","source":"# Modello Faster R-CNN (Resnet50)","metadata":{}},{"cell_type":"code","source":"def compute_class_weights(dataset):\n    # Conta la frequenza di ogni classe nel dataset\n    class_counts = np.zeros(len(dataset.classes))  # Non consideriamo lo sfondo, quindi senza +1\n    \n    # Usa tqdm per monitorare il progresso mentre si itera sul dataset\n    for _, targets in tqdm(dataset, desc=\"Calcolo frequenze delle classi\", leave=False):\n        # Controlla se targets è None\n        if targets is None:\n            continue\n        \n        # Assicurati che 'labels' sia un array e itera su di esso\n        if 'labels' in targets:\n            for target in targets['labels']:  \n                class_counts[target] += 1\n\n    # Calcola i pesi per le classi (senza lo sfondo)\n    total_count = sum(class_counts)  \n\n    # Calcola i pesi inversamente proporzionali alla frequenza\n    class_weights = np.divide(total_count, class_counts)\n\n    return class_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:17:06.905892Z","iopub.execute_input":"2024-12-09T22:17:06.906113Z","iopub.status.idle":"2024-12-09T22:17:06.919650Z","shell.execute_reply.started":"2024-12-09T22:17:06.906088Z","shell.execute_reply":"2024-12-09T22:17:06.919008Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"def train_and_validate(model, train_loader, val_loader, optimizer, device, class_weights, num_epochs=10, save_model=True, num_classes=12):\n    \"\"\"\n    Funzione per il training e la validazione del modello Faster R-CNN con pesi delle classi e metriche di mAP e accuracy.\n    \"\"\"\n    model.to(device)\n    losses_per_epoch = []\n    train_accuracies = []\n    val_accuracies = []\n    all_train_preds = defaultdict(list)\n    all_train_labels = defaultdict(list)\n    all_train_scores = defaultdict(list)\n    all_val_preds = defaultdict(list)\n    all_val_labels = defaultdict(list)\n    all_val_scores = defaultdict(list)\n\n    # Assicurati che i pesi siano un tensor PyTorch\n    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoca {epoch + 1}/{num_epochs}\")\n        model.train()\n        total_loss = 0\n\n        # Training loop con tqdm\n        train_loop = tqdm(train_loader, desc=\"Training\", leave=False)\n        for images, targets in train_loop:\n            # Se targets è None (immagini senza annotazioni), crea un target con la classe \"sfondo\"\n            if targets is None:\n                targets = [{'boxes': torch.zeros((1, 4), device=device), 'labels': torch.tensor([0], device=device)} for _ in range(len(images))]\n            else:\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            images = [img.to(device) for img in images]\n\n            # Calcola le perdite\n            loss_dict = model(images, targets)\n            \n            # Somma le perdite principali (classificazione, regressione delle box, etc.)\n            losses = sum(loss for loss in loss_dict.values())\n\n            # Verifica se le perdite sono per classe e ponderale\n            for key in loss_dict:\n                if 'loss' in key and loss_dict[key].dim() == 1:  # Se è per classe\n                    losses += (loss_dict[key] * class_weights).sum()\n                else:\n                    losses += loss_dict[key]  # Per perdite scalari\n\n            # Ottieni le predizioni per il calcolo delle metriche\n            pred_labels = [t['labels'] for t in targets]\n            pred_scores = [output['scores'] for output in model(images)]\n\n            # Raccolta delle etichette predette e vere per la metrica\n            for i, target in enumerate(targets):\n                all_train_preds[i].extend(pred_labels[i].cpu().numpy())\n                all_train_labels[i].extend([t.item() for t in target['labels']])\n                all_train_scores[i].extend([score.item() for score in pred_scores[i]])\n\n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n\n            total_loss += losses.item()\n            train_loop.set_postfix(loss=losses.item())\n\n        # Calcola l'accuratezza del training\n        train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n        train_accuracies.append(train_accuracy)\n        print(f\"Perdita Totale per l'epoca {epoch + 1}: {total_loss:.4f} | Accuracy (Training): {train_accuracy:.4f}\")\n\n        losses_per_epoch.append(total_loss)\n\n        # Validazione\n        model.eval()\n        val_loop = tqdm(val_loader, desc=\"Validazione\", leave=False)\n\n        with torch.no_grad():\n            for images, targets in val_loop:\n                # Se targets è None (immagini senza annotazioni), crea un target con la classe \"sfondo\"\n                if targets is None:\n                    targets = [{'boxes': torch.zeros((1, 4), device=device), 'labels': torch.tensor([0], device=device)} for _ in range(len(images))]\n                else:\n                    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n                images = [img.to(device) for img in images]\n                predictions = model(images)\n\n                # Ottieni le predizioni per il calcolo delle metriche\n                pred_labels = [t['labels'] for t in targets]\n                pred_scores = [output['scores'] for output in predictions]\n\n                # Raccolta delle etichette predette e vere per la metrica\n                for i, target in enumerate(targets):\n                    all_val_preds[i].extend(pred_labels[i].cpu().numpy())\n                    all_val_labels[i].extend([t.item() for t in target['labels']])\n                    all_val_scores[i].extend([score.item() for score in pred_scores[i]])\n\n                val_loop.set_postfix(processed=len(predictions))  # Placeholder per metriche future\n\n        # Calcola mAP\n        ap = calculate_average_precision(all_val_scores, all_val_preds, all_val_labels, num_classes)\n        mAP = sum(ap.values()) / num_classes\n\n        # Calcola l'accuratezza della validazione\n        val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n        val_accuracies.append(val_accuracy)\n\n        print(f\"Accuracy (Validation): {val_accuracy:.4f}\")\n        print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n\n        # Salva il modello\n        if save_model:\n            torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")\n            print(f\"Modello salvato: model_epoch_{epoch + 1}.pth\")\n\n    # Plotting delle metriche\n    epochs_range = range(num_epochs)\n\n    plt.figure(figsize=(12, 8))\n\n    # Loss plot\n    plt.subplot(3, 1, 1)\n    plt.plot(epochs_range, losses_per_epoch, label='Training Loss', color='blue')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training Loss per Epoca')\n\n    # Accuracy plot\n    plt.subplot(3, 1, 2)\n    plt.plot(epochs_range, train_accuracies, label='Accuracy (Training)', color='green')\n    plt.plot(epochs_range, val_accuracies, label='Accuracy (Validation)', color='orange')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title('Accuracy per Epoca')\n\n    # mAP plot\n    plt.subplot(3, 1, 3)\n    plt.plot(epochs_range, [mAP] * num_epochs, label='mAP', color='red')\n    plt.xlabel('Epochs')\n    plt.ylabel('mAP')\n    plt.legend()\n    plt.title('mAP per Epoca')\n\n    plt.tight_layout()\n    plt.show()\n\n    return losses_per_epoch, train_accuracies, val_accuracies, mAP","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:17:06.920909Z","iopub.execute_input":"2024-12-09T22:17:06.921569Z","iopub.status.idle":"2024-12-09T22:17:06.943114Z","shell.execute_reply.started":"2024-12-09T22:17:06.921493Z","shell.execute_reply":"2024-12-09T22:17:06.942153Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"def test_model(model, test_loader, device):\n    \"\"\"\n    Funzione per il testing del modello Faster R-CNN.\n    \n    Args:\n    - model: il modello Faster R-CNN.\n    - test_loader: DataLoader per il test set.\n    - device: dispositivo su cui eseguire (es. 'cuda' o 'cpu').\n    \n    Returns:\n    - predictions: lista delle predizioni per ogni batch (include 'boxes', 'labels', 'scores').\n    \"\"\"\n    model.to(device)\n    model.eval()\n    predictions = []\n    \n    print(\"\\nInizio testing...\")\n    test_loop = tqdm(test_loader, desc=\"Testing\", leave=False)\n    \n    with torch.no_grad():\n        for images, _ in test_loop:  # Durante il test, i target possono essere ignorati\n            images = [img.to(device) for img in images]\n            preds = model(images)\n            \n            # Predizioni di ciascun batch (contenente 'boxes', 'labels', 'scores')\n            # Le predizioni sono in un formato di lista di dizionari\n            for pred in preds:\n                predictions.append({\n                    'boxes': pred['boxes'].cpu().numpy(),\n                    'labels': pred['labels'].cpu().numpy(),\n                    'scores': pred['scores'].cpu().numpy()\n                })\n            \n            # Aggiungi aggiornamenti su quante predizioni sono state processate\n            test_loop.set_postfix(processed=len(predictions))\n\n    print(\"Testing completato.\")\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:17:06.944107Z","iopub.execute_input":"2024-12-09T22:17:06.944377Z","iopub.status.idle":"2024-12-09T22:17:06.958685Z","shell.execute_reply.started":"2024-12-09T22:17:06.944351Z","shell.execute_reply":"2024-12-09T22:17:06.957930Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"# Carica il modello Faster R-CNN con ResNet50 e FPN\nmodel = fasterrcnn_resnet50_fpn(weights=None)\n\nnum_classes = 13\n\n# Modifica il numero di classi in output\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\n# Imposta il dispositivo (GPU o CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Configurazione training\nnum_epochs = 2\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:17:06.959966Z","iopub.execute_input":"2024-12-09T22:17:06.960369Z","iopub.status.idle":"2024-12-09T22:17:07.691187Z","shell.execute_reply.started":"2024-12-09T22:17:06.960329Z","shell.execute_reply":"2024-12-09T22:17:07.690445Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"# Calcola i pesi delle classi\nclass_weights = compute_class_weights(train_loader.dataset)\n\nprint(class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:17:07.693372Z","iopub.execute_input":"2024-12-09T22:17:07.693661Z","iopub.status.idle":"2024-12-09T22:19:07.596426Z","shell.execute_reply.started":"2024-12-09T22:17:07.693633Z","shell.execute_reply":"2024-12-09T22:19:07.595428Z"}},"outputs":[{"name":"stderr","text":"                                                                                      ","output_type":"stream"},{"name":"stdout","text":"[5.42913361e+01 3.01860691e+00 1.97388871e+01 1.61693762e+02\n 1.09486080e+02 1.26908012e+02 1.77176162e+00 4.62356757e+03\n 3.39092170e+02 1.21155807e+02 1.62925714e+03            inf]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/1940286729.py:20: RuntimeWarning: divide by zero encountered in divide\n  class_weights = np.divide(total_count, class_counts)\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"# Esegui il training\nlosses_per_epoch = train_and_validate(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    optimizer=optimizer,\n    device=device,\n    class_weights=class_weights,\n    num_epochs=num_epochs,\n    save_model=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:19:07.597697Z","iopub.execute_input":"2024-12-09T22:19:07.598002Z","iopub.status.idle":"2024-12-09T22:19:08.010788Z","shell.execute_reply.started":"2024-12-09T22:19:07.597971Z","shell.execute_reply":"2024-12-09T22:19:08.009544Z"}},"outputs":[{"name":"stdout","text":"\nEpoca 1/2\n","output_type":"stream"},{"name":"stderr","text":"                                                   \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[118], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Esegui il training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m losses_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[114], line 50\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, optimizer, device, class_weights, num_epochs, save_model, num_classes)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Ottieni le predizioni per il calcolo delle metriche\u001b[39;00m\n\u001b[1;32m     49\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m [t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m---> 50\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m [output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Raccolta delle etichette predette e vere per la metrica\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(targets):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:62\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets should not be none when in training mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/__init__.py:1777\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m-> 1777\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n","\u001b[0;31mAssertionError\u001b[0m: targets should not be none when in training mode"],"ename":"AssertionError","evalue":"targets should not be none when in training mode","output_type":"error"}],"execution_count":118},{"cell_type":"code","source":"# Chiamata alla funzione di test\npredictions = test_model(model=model, test_loader=test_loader, device=device)\n\n# Puoi fare qualcosa con le predizioni, come visualizzarle o calcolare metriche\nprint(f\"Numero di predizioni ottenute: {len(predictions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:19:08.011765Z","iopub.status.idle":"2024-12-09T22:19:08.012084Z","shell.execute_reply.started":"2024-12-09T22:19:08.011929Z","shell.execute_reply":"2024-12-09T22:19:08.011946Z"}},"outputs":[],"execution_count":null}]}