{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10182328,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nimport shutil\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\nfrom concurrent.futures import ProcessPoolExecutor\nimport warnings\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.transforms import functional as TF\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport ast\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per il deep learning avanzato\nfrom torch.amp import GradScaler, autocast\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:59:41.111131Z","iopub.execute_input":"2024-12-19T10:59:41.112065Z","iopub.status.idle":"2024-12-19T10:59:41.118930Z","shell.execute_reply.started":"2024-12-19T10:59:41.112027Z","shell.execute_reply":"2024-12-19T10:59:41.118248Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"COCO_JSON_NM = 'COCO_annotations_new.json' \nOUT_COCO_JSON_NM = 'mod_COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / COCO_JSON_NM\nnew_coco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\n\ntrain_path = '/kaggle/working/train.json'\ntest_path = '/kaggle/working/test.json'\nval_path = '/kaggle/working/val.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:00.188649Z","iopub.execute_input":"2024-12-19T10:47:00.189074Z","iopub.status.idle":"2024-12-19T10:47:00.194227Z","shell.execute_reply.started":"2024-12-19T10:47:00.189047Z","shell.execute_reply":"2024-12-19T10:47:00.193439Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:00.195479Z","iopub.execute_input":"2024-12-19T10:47:00.195828Z","iopub.status.idle":"2024-12-19T10:47:00.208050Z","shell.execute_reply.started":"2024-12-19T10:47:00.195792Z","shell.execute_reply":"2024-12-19T10:47:00.207319Z"}},"outputs":[{"name":"stdout","text":"Cartella /kaggle/working pulita.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:00.209739Z","iopub.execute_input":"2024-12-19T10:47:00.210000Z","iopub.status.idle":"2024-12-19T10:47:00.223275Z","shell.execute_reply.started":"2024-12-19T10:47:00.209976Z","shell.execute_reply":"2024-12-19T10:47:00.222498Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Background Labels","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    with open(input_path, 'r') as f:\n        data = json.load(f)\n\n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n\n    for category in tqdm(raw_categories, desc=\"Processing Categories\"):\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n\n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n\n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n\n    # Preprocessa le annotazioni in un dizionario per immagini\n    image_annotations_dict = {}\n    for annotation in tqdm(data.get('annotations', []), desc=\"Building Image Annotations Dictionary\"):\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_dict:\n            image_annotations_dict[image_id] = []\n        image_annotations_dict[image_id].append(annotation)\n\n    # Lista di nuove annotazioni da aggiungere per immagini senza bbox\n    new_annotations = []\n\n    # Elenco di annotazioni da rimuovere\n    annotations_to_remove = []\n\n    for annotation in tqdm(data.get('annotations', []), desc=\"Processing Annotations\"):\n        if annotation['category_id'] == 0:  # Se è Aircraft\n            annotation['category_id'] = 11\n        \n        # Converte il formato del bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n        \n        x, y, width, height = annotation['bbox']\n        xmin = x\n        xmax = x + width\n        ymin = y\n        ymax = y + height\n        \n        # Verifica che xmin < xmax e ymin < ymax\n        if xmin >= xmax or ymin >= ymax:\n            annotations_to_remove.append(annotation['id'])\n        else:\n            annotation['bbox'] = [xmin, xmax, ymin, ymax]\n\n    # Rimuovi le annotazioni non valide\n    data['annotations'] = [ann for ann in data['annotations'] if ann['id'] not in annotations_to_remove]\n\n    # Verifica se ci sono immagini senza annotazioni (usando il dizionario delle annotazioni)\n    for image in tqdm(data.get('images', []), desc=\"Processing Images\"):\n        if image['id'] not in image_annotations_dict:  # Se l'immagine non ha annotazioni\n            # Aggiungi la categoria \"background\"\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, image['width'], 0.0, image['height']],  # Background con bbox che copre tutta l'immagine\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n\n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n\n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n\n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:00.224388Z","iopub.execute_input":"2024-12-19T10:47:00.224643Z","iopub.status.idle":"2024-12-19T10:47:00.236852Z","shell.execute_reply.started":"2024-12-19T10:47:00.224616Z","shell.execute_reply":"2024-12-19T10:47:00.236040Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"process_custom_coco_json(coco_json_pth, new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:00.237755Z","iopub.execute_input":"2024-12-19T10:47:00.238025Z","iopub.status.idle":"2024-12-19T10:47:15.251210Z","shell.execute_reply.started":"2024-12-19T10:47:00.237998Z","shell.execute_reply":"2024-12-19T10:47:15.250276Z"}},"outputs":[{"name":"stderr","text":"Processing Categories: 100%|██████████| 11/11 [00:00<00:00, 146934.22it/s]\nBuilding Image Annotations Dictionary: 100%|██████████| 669983/669983 [00:00<00:00, 2377807.88it/s]\nProcessing Annotations: 100%|██████████| 669983/669983 [00:03<00:00, 191492.18it/s]\nProcessing Images: 100%|██████████| 45891/45891 [00:00<00:00, 701194.90it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def count_bboxes_per_category(json_path):\n    \"\"\"\n    Funzione che conta il numero di bounding box per ciascuna categoria in un file JSON formato COCO.\n    \n    :param json_path: Percorso al file JSON.\n    :return: Dizionario con i nomi delle categorie come chiavi e il conteggio dei bounding box come valori.\n    \"\"\"\n    # Leggi il JSON dal file\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    # Ottieni mapping delle categorie (id -> nome)\n    category_mapping = {cat['id']: cat['name'] for cat in data.get('categories', [])}\n    \n    # Conta i bounding box per ciascun category_id\n    bbox_counts = defaultdict(int)\n    for annotation in data.get('annotations', []):\n        category_id = annotation['category_id']\n        bbox_counts[category_id] += 1\n    \n    # Converti il conteggio usando i nomi delle categorie\n    bbox_counts_named = {category_mapping[cat_id]: count for cat_id, count in bbox_counts.items()}\n\n    return bbox_counts_named","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:15.252371Z","iopub.execute_input":"2024-12-19T10:47:15.252639Z","iopub.status.idle":"2024-12-19T10:47:15.258392Z","shell.execute_reply.started":"2024-12-19T10:47:15.252613Z","shell.execute_reply":"2024-12-19T10:47:15.257319Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"bbox_counts = count_bboxes_per_category(new_coco_json_pth)\n\n# Stampa i risultati\nfor category, count in bbox_counts.items():\n    print(f\"Categoria: {category}, Numero di bbox: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:15.259688Z","iopub.execute_input":"2024-12-19T10:47:15.260349Z","iopub.status.idle":"2024-12-19T10:47:18.615178Z","shell.execute_reply.started":"2024-12-19T10:47:15.260309Z","shell.execute_reply":"2024-12-19T10:47:18.614336Z"}},"outputs":[{"name":"stdout","text":"Categoria: Passenger Vehicle, Numero di bbox: 224911\nCategoria: Building, Numero di bbox: 384929\nCategoria: Truck, Numero di bbox: 34345\nCategoria: Engineering Vehicle, Numero di bbox: 5477\nCategoria: Shipping Container, Numero di bbox: 5388\nCategoria: Maritime Vessel, Numero di bbox: 6329\nCategoria: Railway Vehicle, Numero di bbox: 4233\nCategoria: Storage Tank, Numero di bbox: 2033\nCategoria: Aircraft, Numero di bbox: 1708\nCategoria: Pylon, Numero di bbox: 470\nCategoria: Helipad, Numero di bbox: 152\nCategoria: background, Numero di bbox: 13691\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"def filter_invalid_boxes(annotations):\n    \"\"\"Filtra le annotazioni con bounding box non validi.\"\"\"\n    valid_annotations = []\n    for annotation in annotations:\n        # Estrai le coordinate del bounding box\n        bbox = annotation['bbox']\n        if isinstance(bbox, str):\n            try:\n                # Tenta di convertire la stringa in una lista\n                bbox = json.loads(bbox)\n            except json.JSONDecodeError:\n                raise ValueError(f\"Bounding box non valido: {bbox} (conversione da stringa fallita).\")\n\n        # Verifica se il bounding box ha coordinate valide\n        if len(bbox) == 4:\n            x_min, y_min, width, height = bbox\n            if width > 0 and height > 0:\n                # Converti in formato [x_min, x_max, y_min, y_max]\n                annotation['bbox'] = [x_min, x_min + width, y_min, y_min + height]\n                valid_annotations.append(annotation)\n\n    return valid_annotations\n\n\ndef split(json_file, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n    # Carica il JSON\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    \n    # Filtra le annotazioni con bounding box non validi\n    valid_annotations = filter_invalid_boxes(data['annotations'])\n    \n    # Ottieni la lista delle immagini\n    images = data['images']\n    \n    # Mescola casualmente gli ID delle immagini\n    random.shuffle(images)\n    \n    # Calcola i limiti per train, validation e test\n    total_images = len(images)\n    total_annotations = len(valid_annotations)\n    train_end = int(total_images * train_ratio)\n    val_end = int(total_images * (train_ratio + val_ratio))\n    \n    # Suddividi le immagini nei rispettivi set\n    train_images = images[:train_end]\n    val_images = images[train_end:val_end]\n    test_images = images[val_end:]\n    \n    # Raggruppa gli ID delle immagini per i rispettivi set\n    train_image_ids = {image['id'] for image in train_images}\n    val_image_ids = {image['id'] for image in val_images}\n    test_image_ids = {image['id'] for image in test_images}\n    \n    # Filtra le annotazioni per i rispettivi set di immagini\n    train_annotations = [ann for ann in valid_annotations if ann['image_id'] in train_image_ids]\n    val_annotations = [ann for ann in valid_annotations if ann['image_id'] in val_image_ids]\n    test_annotations = [ann for ann in valid_annotations if ann['image_id'] in test_image_ids]\n    \n    # Crea i nuovi JSON per train, validation e test\n    train_data = {'images': train_images, 'annotations': train_annotations, 'categories': data['categories']}\n    val_data = {'images': val_images, 'annotations': val_annotations, 'categories': data['categories']}\n    test_data = {'images': test_images, 'annotations': test_annotations, 'categories': data['categories']}\n    \n    # Salva i file JSON\n    with open('train.json', 'w') as f:\n        json.dump(train_data, f, indent=4)\n    \n    with open('val.json', 'w') as f:\n        json.dump(val_data, f, indent=4)\n    \n    with open('test.json', 'w') as f:\n        json.dump(test_data, f, indent=4)\n    \n    # Controlla la proporzione delle immagini e delle annotazioni\n    check_split_proportions(total_images, total_annotations, \n                            len(train_images), len(val_images), len(test_images), \n                            len(train_annotations), len(val_annotations), len(test_annotations), \n                            train_ratio, val_ratio, test_ratio, \n                            train_annotations, val_annotations, test_annotations, data['categories'])\n\n\ndef check_split_proportions(total_images, total_annotations, train_count, val_count, test_count, \n                            train_bbox_count, val_bbox_count, test_bbox_count, \n                            train_ratio, val_ratio, test_ratio, \n                            train_annotations, val_annotations, test_annotations, categories):\n    # Percentuali per immagini\n    train_image_percentage = (train_count / total_images) * 100\n    val_image_percentage = (val_count / total_images) * 100\n    test_image_percentage = (test_count / total_images) * 100\n    \n    # Percentuali per bbox\n    train_bbox_percentage = (train_bbox_count / total_annotations) * 100\n    val_bbox_percentage = (val_bbox_count / total_annotations) * 100\n    test_bbox_percentage = (test_bbox_count / total_annotations) * 100\n    \n    print(f\"Totale immagini: {total_images}\")\n    print(f\"Totale annotazioni (bbox): {total_annotations}\")\n    print(f\"Train: {train_count} immagini ({train_image_percentage:.2f}%) ({train_bbox_count} bbox) ({train_bbox_percentage:.2f}%)\")\n    print(f\"Val: {val_count} immagini ({val_image_percentage:.2f}%) ({val_bbox_count} bbox) ({val_bbox_percentage:.2f}%)\")\n    print(f\"Test: {test_count} immagini ({test_image_percentage:.2f}%) ({test_bbox_count} bbox) ({test_bbox_percentage:.2f}%)\")\n    \n    # Calcola il numero di annotazioni per categoria nei vari set\n    category_count_train = defaultdict(int)\n    category_count_val = defaultdict(int)\n    category_count_test = defaultdict(int)\n    \n    for annotation in train_annotations:\n        category_count_train[annotation['category_id']] += 1\n    for annotation in val_annotations:\n        category_count_val[annotation['category_id']] += 1\n    for annotation in test_annotations:\n        category_count_test[annotation['category_id']] += 1\n    \n    # Stampa le proporzioni per categoria\n    print(\"\\nProporzioni per categoria:\")\n    for category_dict in categories:\n        for category_id, category_name in category_dict.items():\n            # Converti category_id in intero se necessario\n            category_id = int(category_id)\n            \n            # Conta il numero di annotazioni per categoria in ogni set\n            train_cat_count = category_count_train.get(category_id, 0)\n            val_cat_count = category_count_val.get(category_id, 0)\n            test_cat_count = category_count_test.get(category_id, 0)\n            \n            # Calcola la percentuale di annotazioni per categoria\n            total_cat_annotations = train_cat_count + val_cat_count + test_cat_count\n            if total_cat_annotations > 0:\n                train_cat_percentage = (train_cat_count / total_cat_annotations) * 100\n                val_cat_percentage = (val_cat_count / total_cat_annotations) * 100\n                test_cat_percentage = (test_cat_count / total_cat_annotations) * 100\n            else:\n                train_cat_percentage = val_cat_percentage = test_cat_percentage = 0.0\n\n            print(f\"{category_name}:\")\n            print(f\"  Train: {train_cat_count} annotazioni ({train_cat_percentage:.2f}%)\")\n            print(f\"  Val: {val_cat_count} annotazioni ({val_cat_percentage:.2f}%)\")\n            print(f\"  Test: {test_cat_count} annotazioni ({test_cat_percentage:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:18.616690Z","iopub.execute_input":"2024-12-19T10:47:18.617364Z","iopub.status.idle":"2024-12-19T10:47:18.634927Z","shell.execute_reply.started":"2024-12-19T10:47:18.617319Z","shell.execute_reply":"2024-12-19T10:47:18.634018Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Chiamata della funzione\nsplit(coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:18.638616Z","iopub.execute_input":"2024-12-19T10:47:18.638886Z","iopub.status.idle":"2024-12-19T10:47:32.579690Z","shell.execute_reply.started":"2024-12-19T10:47:18.638860Z","shell.execute_reply":"2024-12-19T10:47:32.578564Z"}},"outputs":[{"name":"stdout","text":"Totale immagini: 45891\nTotale annotazioni (bbox): 669975\nTrain: 36712 immagini (80.00%) (536321 bbox) (80.05%)\nVal: 4589 immagini (10.00%) (65979 bbox) (9.85%)\nTest: 4590 immagini (10.00%) (69369 bbox) (10.35%)\n\nProporzioni per categoria:\nAircraft:\n  Train: 1394 annotazioni (81.62%)\n  Val: 169 annotazioni (9.89%)\n  Test: 145 annotazioni (8.49%)\nPassenger Vehicle:\n  Train: 179537 annotazioni (79.59%)\n  Val: 22709 annotazioni (10.07%)\n  Test: 23329 annotazioni (10.34%)\nTruck:\n  Train: 27831 annotazioni (80.63%)\n  Val: 3337 annotazioni (9.67%)\n  Test: 3351 annotazioni (9.71%)\nRailway Vehicle:\n  Train: 3378 annotazioni (79.80%)\n  Val: 340 annotazioni (8.03%)\n  Test: 515 annotazioni (12.17%)\nMaritime Vessel:\n  Train: 5025 annotazioni (79.30%)\n  Val: 567 annotazioni (8.95%)\n  Test: 745 annotazioni (11.76%)\nEngineering Vehicle:\n  Train: 4479 annotazioni (81.69%)\n  Val: 452 annotazioni (8.24%)\n  Test: 552 annotazioni (10.07%)\nBuilding:\n  Train: 308495 annotazioni (79.98%)\n  Val: 37376 annotazioni (9.69%)\n  Test: 39828 annotazioni (10.33%)\nHelipad:\n  Train: 117 annotazioni (76.97%)\n  Val: 15 annotazioni (9.87%)\n  Test: 20 annotazioni (13.16%)\nStorage Tank:\n  Train: 1548 annotazioni (76.14%)\n  Val: 232 annotazioni (11.41%)\n  Test: 253 annotazioni (12.44%)\nShipping Container:\n  Train: 4131 annotazioni (75.66%)\n  Val: 733 annotazioni (13.42%)\n  Test: 596 annotazioni (10.92%)\nPylon:\n  Train: 386 annotazioni (82.13%)\n  Val: 49 annotazioni (10.43%)\n  Test: 35 annotazioni (7.45%)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"import operator\nclass CustomDataset(Dataset):\n    def __init__(self, json_file, img_dir, aug=False):\n        \"\"\"\n        Inizializza il dataset personalizzato.\n        Args:\n        - json_file: Il file JSON preprocessato contenente immagini, annotazioni e categorie.\n        - img_dir: La directory contenente le immagini.\n        - aug: Booleano per attivare o meno l'augmentazione.\n        \"\"\"\n        # Carica il file JSON preprocessato\n        with open(json_file, 'r') as f:\n            coco_data = json.load(f)\n\n        # Estrai informazioni su immagini, annotazioni e categorie\n        self.image_info = {image['id']: image['file_name'] for image in coco_data['images']}\n        self.image_annotations = {}\n        self.image_bboxes = {}\n\n        # Estrai le classi (categorie) dal file JSON\n        self.classes = {}\n        for category_dict in coco_data['categories']:\n            for category_id, category_name in category_dict.items():\n                # Converti category_id in intero se necessario\n                self.classes[int(category_id)] = category_name\n\n        for annotation in coco_data['annotations']:\n            image_id = annotation['image_id']\n            bbox = annotation['bbox']\n\n            # Associa annotazioni e bounding box alle immagini\n            if image_id not in self.image_annotations:\n                self.image_annotations[image_id] = []\n                self.image_bboxes[image_id] = []\n            \n            self.image_annotations[image_id].append(annotation['category_id'])\n            self.image_bboxes[image_id].append(bbox)\n\n        # Configura il percorso delle immagini e seleziona solo immagini valide\n        self.img_dir = img_dir\n        self.image_paths = []\n        self.image_ids = []\n        for image_id, file_name in self.image_info.items():\n            if image_id in self.image_annotations:\n                img_path = os.path.join(img_dir, file_name)\n                if os.path.exists(img_path):\n                    self.image_paths.append(img_path)\n                    self.image_ids.append(image_id)\n\n        # Definisci trasformazioni\n        self.base_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ])\n\n        self.aug = aug\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Recupera un'immagine e le sue annotazioni.\n        \"\"\"\n        img_path = self.image_paths[index]\n        img_id = self.image_ids[index]\n\n        # Carica l'immagine\n        image = Image.open(img_path).convert('RGB')\n        original_width, original_height = image.size\n\n        # Applica le trasformazioni\n        if self.aug:\n            image_tensor = self.aug_transform(image)\n        else:\n            image_tensor = self.base_transform(image)\n\n        # Recupera le annotazioni e i bounding box\n        categories = self.image_annotations[img_id]\n        bboxes = self.image_bboxes[img_id]\n\n        # Calcola il ridimensionamento\n        scale_x = 224 / original_width\n        scale_y = 224 / original_height\n\n        # Ridimensiona i bounding box\n        scaled_bboxes = [\n            torch.tensor([\n                bbox[0] * scale_x,  # x_min\n                bbox[2] * scale_x,  # x_max\n                bbox[1] * scale_y,  # y_min\n                bbox[3] * scale_y   # y_max\n            ], dtype=torch.float32)\n            for bbox in bboxes\n        ]\n\n        # Costruisci il target\n        target = {\n            \"boxes\": torch.stack(scaled_bboxes),\n            \"labels\": torch.tensor(categories, dtype=torch.int64)\n        }\n\n        return image_tensor, target","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:32.581240Z","iopub.execute_input":"2024-12-19T10:47:32.581598Z","iopub.status.idle":"2024-12-19T10:47:32.593331Z","shell.execute_reply.started":"2024-12-19T10:47:32.581568Z","shell.execute_reply":"2024-12-19T10:47:32.592209Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione di collation per il DataLoader, utile per il batching di immagini e annotazioni.\n    La funzione restituirà un batch di immagini e un batch di target, formattato correttamente per Faster R-CNN.\n    \n    Args:\n    - batch: lista di tuple (image, target)\n    \n    Returns:\n    - images: batch di immagini\n    - targets: lista di dizionari contenenti le annotazioni per ogni immagine\n    \"\"\"\n    # Separa immagini e target\n    images, targets = zip(*batch)\n\n    # Converte la lista di immagini in un batch di immagini\n    images = list(images)\n\n    # Restituisci il batch\n    return images, list(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:32.594455Z","iopub.execute_input":"2024-12-19T10:47:32.594888Z","iopub.status.idle":"2024-12-19T10:47:32.614520Z","shell.execute_reply.started":"2024-12-19T10:47:32.594840Z","shell.execute_reply":"2024-12-19T10:47:32.613634Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Creazione dei dataset\ntrain_dataset = CustomDataset(train_path, img_dir=img_fldr, aug=True)\nvalid_dataset = CustomDataset(val_path, img_dir=img_fldr, aug=False)  \ntest_dataset = CustomDataset(test_path, img_dir=img_fldr, aug=False)  \n\n# Creazione dei DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\nval_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:47:32.615635Z","iopub.execute_input":"2024-12-19T10:47:32.615919Z","iopub.status.idle":"2024-12-19T10:48:48.950160Z","shell.execute_reply.started":"2024-12-19T10:47:32.615889Z","shell.execute_reply":"2024-12-19T10:48:48.949163Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Check DataLoader","metadata":{}},{"cell_type":"code","source":"def validate_dataloader(dataloader):\n    \"\"\"\n    Valida un DataLoader verificando che ogni immagine abbia un target associato\n    e che nessun target sia `None` o vuoto.\n    \n    Args:\n    - dataloader: Il DataLoader da verificare.\n    \n    Returns:\n    - error_messages: Lista di messaggi di errore. Vuota se tutti i dati sono validi.\n    \"\"\"\n    error_messages = []\n    for batch_idx, (images, targets) in enumerate(dataloader):\n        for idx, target in enumerate(targets):\n            if target is None:\n                error_messages.append(f\"Batch {batch_idx}, Immagine {idx}: Target è None.\")\n            elif target[\"boxes\"].numel() == 0 or target[\"labels\"].numel() == 0:\n                error_messages.append(\n                    f\"Batch {batch_idx}, Immagine {idx}: Target è vuoto o mancano 'boxes'/'labels'.\"\n                )\n    return error_messages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:48:48.951448Z","iopub.execute_input":"2024-12-19T10:48:48.952414Z","iopub.status.idle":"2024-12-19T10:48:48.958297Z","shell.execute_reply.started":"2024-12-19T10:48:48.952371Z","shell.execute_reply":"2024-12-19T10:48:48.957463Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Validazione del DataLoader di training\ntrain_errors = validate_dataloader(train_loader)\n\nif train_errors:\n    print(\"Errori nel DataLoader di training:\")\n    for error in train_errors:\n        print(error)\nelse:\n    print(\"Tutti i target nel DataLoader di training sono validi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:48:48.959645Z","iopub.execute_input":"2024-12-19T10:48:48.959938Z","iopub.status.idle":"2024-12-19T10:50:43.507916Z","shell.execute_reply.started":"2024-12-19T10:48:48.959911Z","shell.execute_reply":"2024-12-19T10:50:43.506342Z"}},"outputs":[{"name":"stdout","text":"Tutti i target nel DataLoader di training sono validi.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Validazione del DataLoader di training\nval_errors = validate_dataloader(val_loader)\n\nif val_errors:\n    print(\"Errori nel DataLoader di validation:\")\n    for error in val_errors:\n        print(error)\nelse:\n    print(\"Tutti i target nel DataLoader di validation sono validi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:50:43.509223Z","iopub.execute_input":"2024-12-19T10:50:43.509577Z","iopub.status.idle":"2024-12-19T10:50:56.337891Z","shell.execute_reply.started":"2024-12-19T10:50:43.509548Z","shell.execute_reply":"2024-12-19T10:50:56.336765Z"}},"outputs":[{"name":"stdout","text":"Tutti i target nel DataLoader di validation sono validi.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Validazione del DataLoader di training\ntest_errors = validate_dataloader(test_loader)\n\nif test_errors:\n    print(\"Errori nel DataLoader di test:\")\n    for error in test_errors:\n        print(error)\nelse:\n    print(\"Tutti i target nel DataLoader di test sono validi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:50:56.339528Z","iopub.execute_input":"2024-12-19T10:50:56.339830Z","iopub.status.idle":"2024-12-19T10:51:09.455520Z","shell.execute_reply.started":"2024-12-19T10:50:56.339803Z","shell.execute_reply":"2024-12-19T10:51:09.454466Z"}},"outputs":[{"name":"stdout","text":"Tutti i target nel DataLoader di test sono validi.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def count_images_and_targets(dataloader):\n    \"\"\"\n    Conta il numero totale di immagini e target in un DataLoader.\n    \"\"\"\n    num_images = 0\n    num_targets = 0\n\n    for images, targets in dataloader:\n        # Conta le immagini nel batch\n        num_images += len(images)\n        \n        # Conta i target per ogni immagine (numero di oggetti)\n        for target in targets:\n            num_targets += len(target[\"boxes\"])  # Ogni immagine ha un numero di bounding boxes\n    \n    return num_images, num_targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:51:09.457158Z","iopub.execute_input":"2024-12-19T10:51:09.458072Z","iopub.status.idle":"2024-12-19T10:51:09.463065Z","shell.execute_reply.started":"2024-12-19T10:51:09.458026Z","shell.execute_reply":"2024-12-19T10:51:09.462168Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"num_images_train, num_targets_train = count_images_and_targets(train_loader)\n\nprint(f\"Numero totale di immagini per il train: {num_images_train}\")\nprint(f\"Numero totale di target per il train: {num_targets_train}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:51:09.464311Z","iopub.execute_input":"2024-12-19T10:51:09.464583Z","iopub.status.idle":"2024-12-19T10:52:20.812079Z","shell.execute_reply.started":"2024-12-19T10:51:09.464560Z","shell.execute_reply":"2024-12-19T10:52:20.810994Z"}},"outputs":[{"name":"stdout","text":"Numero totale di immagini per il train: 25776\nNumero totale di target per il train: 536321\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"num_images_val, num_targets_val = count_images_and_targets(val_loader)\n\nprint(f\"Numero totale di immagini per il validation: {num_images_val}\")\nprint(f\"Numero totale di target per il validation: {num_targets_val}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:52:20.813654Z","iopub.execute_input":"2024-12-19T10:52:20.813966Z","iopub.status.idle":"2024-12-19T10:52:30.084031Z","shell.execute_reply.started":"2024-12-19T10:52:20.813937Z","shell.execute_reply":"2024-12-19T10:52:30.082923Z"}},"outputs":[{"name":"stdout","text":"Numero totale di immagini per il validation: 3183\nNumero totale di target per il validation: 65979\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"num_images_test, num_targets_test = count_images_and_targets(test_loader)\n\nprint(f\"Numero totale di immagini per il test: {num_images_test}\")\nprint(f\"Numero totale di target per il test: {num_targets_test}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:52:30.085417Z","iopub.execute_input":"2024-12-19T10:52:30.085713Z","iopub.status.idle":"2024-12-19T10:52:39.497481Z","shell.execute_reply.started":"2024-12-19T10:52:30.085685Z","shell.execute_reply":"2024-12-19T10:52:39.496311Z"}},"outputs":[{"name":"stdout","text":"Numero totale di immagini per il test: 3218\nNumero totale di target per il test: 69369\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(f\"Numero totale di immagini: {num_images_train + num_images_val +num_images_test}\")\nprint(f\"Numero totale di target: {num_targets_train + num_targets_val +num_targets_test}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:52:39.499085Z","iopub.execute_input":"2024-12-19T10:52:39.499833Z","iopub.status.idle":"2024-12-19T10:52:39.506421Z","shell.execute_reply.started":"2024-12-19T10:52:39.499788Z","shell.execute_reply":"2024-12-19T10:52:39.504568Z"}},"outputs":[{"name":"stdout","text":"Numero totale di immagini: 32177\nNumero totale di target: 671669\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Modello Faster R-CNN (Resnet50)","metadata":{}},{"cell_type":"code","source":"def compute_class_weights(dataset):\n    # Conta la frequenza di ogni classe nel dataset\n    class_counts = np.zeros(len(dataset.classes))  \n    \n    # Usa tqdm per monitorare il progresso mentre si itera sul dataset\n    for _, targets in tqdm(dataset, desc=\"Calcolo frequenze delle classi\", leave=False):\n        # Controlla se targets è None\n        if targets is None:\n            print(\"ERRORE TARGET NONE\")\n            continue\n        \n        # Assicurati che 'labels' sia un array e itera su di esso\n        if 'labels' in targets:\n            for target in targets['labels']:  \n                class_counts[target] += 1\n\n    # Calcola i pesi per le classi \n    total_count = sum(class_counts)  \n\n    # Calcola i pesi inversamente proporzionali alla frequenza\n    class_weights = np.divide(total_count, class_counts)\n\n    return class_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:52:39.507681Z","iopub.execute_input":"2024-12-19T10:52:39.508082Z","iopub.status.idle":"2024-12-19T10:52:39.518708Z","shell.execute_reply.started":"2024-12-19T10:52:39.508038Z","shell.execute_reply":"2024-12-19T10:52:39.517858Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def calculate_map(preds, targets, num_classes, thresholds=(0.4, 0.5, 0.6), areas=(32, 96)):\n    def compute_iou(box1, box2):\n        \"\"\"Calcola l'IoU tra due bounding boxes.\"\"\"\n        x1 = max(box1[0], box2[0])\n        y1 = max(box1[1], box2[1])\n        x2 = min(box1[2], box2[2])\n        y2 = min(box1[3], box2[3])\n\n        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n        union_area = box1_area + box2_area - inter_area\n\n        return inter_area / union_area if union_area > 0 else 0\n\n    def get_adaptive_threshold(area, thresholds, areas):\n        \"\"\"Determina la soglia di IoU adattiva in base all'area dell'oggetto.\"\"\"\n        small_threshold, medium_threshold, large_threshold = thresholds\n        small_area, medium_area = areas\n\n        if area < small_area:\n            return small_threshold\n        elif area < medium_area:\n            return medium_threshold\n        else:\n            return large_threshold\n\n    aps = []  # Average Precision per classe\n\n    # Aggiungi tqdm per monitorare il ciclo per classe\n    for cls in tqdm(range(num_classes), desc=\"Calcolando mAP per le classi\"):\n        cls_gt_boxes = []\n        cls_pred_boxes = []\n        cls_scores = []\n\n        # Estrai bounding box e predizioni per la classe corrente\n        for i in range(len(targets)):\n            # Ground truth\n            gt_boxes = targets[i][\"boxes\"]\n            gt_labels = targets[i][\"labels\"]\n            cls_gt_boxes.extend([gt_boxes[j] for j in range(len(gt_labels)) if gt_labels[j] == cls])\n\n            # Predizioni\n            pred_boxes = preds[i][\"boxes\"]\n            pred_scores = preds[i][\"scores\"]\n            pred_labels = preds[i][\"labels\"]\n            for j in range(len(pred_labels)):\n                if pred_labels[j] == cls:\n                    cls_pred_boxes.append(pred_boxes[j])\n                    cls_scores.append(pred_scores[j])\n\n        # Ordina le predizioni per confidence score decrescente (utilizzando np.argsort per efficienza)\n        sorted_idx = np.argsort(cls_scores)[::-1]\n        cls_pred_boxes = np.array(cls_pred_boxes)[sorted_idx]\n        cls_scores = np.array(cls_scores)[sorted_idx]\n\n        tp = np.zeros(len(cls_pred_boxes))\n        fp = np.zeros(len(cls_pred_boxes))\n        gt_used = np.zeros(len(cls_gt_boxes))\n\n        # Aggiungi tqdm per monitorare il ciclo per ogni predizione\n        for j, pred in enumerate(cls_pred_boxes):\n            best_iou = 0\n            best_gt_idx = -1\n\n            for k, gt_box in enumerate(cls_gt_boxes):\n                iou = compute_iou(pred, gt_box)\n                area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n                adaptive_threshold = get_adaptive_threshold(area, thresholds, areas)\n\n                if iou > best_iou and iou >= adaptive_threshold and not gt_used[k]:\n                    best_iou = iou\n                    best_gt_idx = k\n\n            if best_gt_idx >= 0:\n                tp[j] = 1\n                gt_used[best_gt_idx] = 1\n            else:\n                fp[j] = 1\n\n        # Calcola Precision e Recall\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        precision = cum_tp / (cum_tp + cum_fp + 1e-6)\n        recall = cum_tp / len(cls_gt_boxes) if len(cls_gt_boxes) > 0 else np.zeros_like(cum_tp)\n\n        # Interpolazione per l'AP\n        ap = 0\n        for t in np.linspace(0, 1, 101):\n            precisions = precision[recall >= t]\n            ap += max(precisions) if len(precisions) > 0 else 0\n        ap /= 101\n\n        aps.append(ap)\n\n    return np.mean(aps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:52:39.519946Z","iopub.execute_input":"2024-12-19T10:52:39.520230Z","iopub.status.idle":"2024-12-19T10:52:39.538748Z","shell.execute_reply.started":"2024-12-19T10:52:39.520205Z","shell.execute_reply":"2024-12-19T10:52:39.537890Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def train_and_validate(model, train_loader, val_loader, optimizer, device, class_weights=None, \n                       num_epochs=10, num_classes=12, accumulation_steps=4):\n    \"\"\"\n    Funzione di training e validazione per Faster R-CNN, con valutazione basata su Precision e Recall.\n    \"\"\"\n    # Scaler per mixed precision training\n    scaler = GradScaler()\n    model.to(device)\n\n    train_losses = []\n    val_losses = []\n    train_precision = []\n    val_precision = []\n    train_recall = []\n    val_recall = []\n\n    # Converte class_weights in tensor e sposta sul dispositivo\n    if class_weights is not None:\n        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoca {epoch + 1}/{num_epochs}\")\n\n        # --------------------\n        # Training\n        # --------------------\n        model.train()\n        total_train_loss = 0.0\n        train_loop = tqdm(train_loader, desc=\"Training\", leave=False)\n\n        for i, (images, targets) in enumerate(train_loop):\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n            with autocast('cuda'):\n                # Forward pass\n                loss_output = model(images, targets)\n                \n                # Controlla se `loss_output` è una lista e convertila in un dizionario\n                if isinstance(loss_output, list):\n                    loss_dict = {f'loss_{idx}': loss for idx, loss in enumerate(loss_output)}\n                elif isinstance(loss_output, dict):\n                    loss_dict = loss_output\n                else:\n                    raise ValueError(f\"Formato di loss_output non supportato: {type(loss_output)}\")\n            \n                total_weighted_loss = 0\n                total_weight = 0\n            \n                # Itera su tutte le perdite\n                for key, loss in loss_dict.items():\n                    if isinstance(loss, torch.Tensor):\n                        if key == 'loss_classifier' and class_weights is not None:\n                            # Applica i pesi delle classi solo alla loss_classifier\n                            labels = torch.cat([t['labels'] for t in targets])  # Ottieni le etichette\n                            weighted_loss = loss * class_weights[labels]  # Pesi basati sulle etichette\n                            total_weighted_loss += weighted_loss.sum()  # Somma la perdita pesata\n                            total_weight += class_weights[labels].sum()  # Somma i pesi per il calcolo totale\n                        else:\n                            # Entra qui per loss_box_reg, loss_objectness e loss_rpn_box_reg\n                            total_weighted_loss += loss.sum()  # Somma la perdita normale\n                            total_weight += loss.numel()  # Somma il numero di elementi\n            \n                # Controlla che `total_weight` non sia zero prima della divisione\n                if total_weight > 0:\n                    losses = total_weighted_loss / total_weight  # Calcola la perdita totale normalizzata\n                else:\n                    losses = torch.tensor(0.0, device=device)  # Imposta una perdita di default\n            \n            # Backward pass con gradient scaling\n            scaler.scale(losses).backward()\n        \n            # Gradient accumulation\n            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n        \n            total_train_loss += losses.item() * accumulation_steps\n            train_loop.set_postfix(loss=losses.item() * accumulation_steps)\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        print(f\"Perdita media di training: {avg_train_loss:.4f}\")\n\n        # Calcolo Precision e Recall per il training\n        train_prec, train_rec = evaluate_precision_recall(model, train_loader, device, num_classes)\n        train_precision.append(train_prec)\n        train_recall.append(train_rec)\n        print(f\"Precision di training: {train_prec:.4f}, Recall di training: {train_rec:.4f}\")\n        \n        # --------------------\n        # Validazione\n        # --------------------\n        model.eval()\n        total_val_loss = 0.0\n        val_loop = tqdm(val_loader, desc=\"Validazione\", leave=False)\n\n        with torch.no_grad():\n            for images, targets in val_loop:\n                images = [img.to(device) for img in images]\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n                with autocast('cuda'):\n                    # Forward pass\n                    loss_output = model(images, targets)\n                    \n                    # Controlla se `loss_output` è una lista e convertila in un dizionario\n                    if isinstance(loss_output, list):\n                        loss_dict = {f'loss_{idx}': loss for idx, loss in enumerate(loss_output)}\n                    elif isinstance(loss_output, dict):\n                        loss_dict = loss_output\n                    else:\n                        raise ValueError(f\"Formato di loss_output non supportato: {type(loss_output)}\")\n        \n                    total_weighted_loss = 0\n                    total_weight = 0\n                    for key, loss in loss_dict.items():\n                        if isinstance(loss, torch.Tensor):\n                            total_weighted_loss += loss.sum()\n                            total_weight += loss.numel()\n        \n                    # Controlla che `total_weight` non sia zero prima della divisione\n                    if total_weight > 0:\n                        losses = total_weighted_loss / total_weight\n                    else:\n                        losses = torch.tensor(0.0, device=device)  # Imposta una perdita di default\n        \n                total_val_loss += losses.item()\n        \n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        print(f\"Perdita media di validazione: {avg_val_loss:.4f}\")\n\n        # Calcolo Precision e Recall per la validazione\n        val_prec, val_rec = evaluate_precision_recall(model, val_loader, device, num_classes)\n        val_precision.append(val_prec)\n        val_recall.append(val_rec)\n        print(f\"Precision di validazione: {val_prec:.4f}, Recall di validazione: {val_rec:.4f}\")\n\n        # --------------------\n        # Salvataggio del modello\n        # --------------------\n        torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")\n        print(f\"Modello salvato: model_epoch_{epoch + 1}.pth\")\n\n    return train_losses, val_losses, train_precision, val_precision, train_recall, val_recall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:29:46.674931Z","iopub.execute_input":"2024-12-19T13:29:46.675377Z","iopub.status.idle":"2024-12-19T13:29:46.695308Z","shell.execute_reply.started":"2024-12-19T13:29:46.675343Z","shell.execute_reply":"2024-12-19T13:29:46.694310Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def plot_metrics(train_losses, val_losses, train_precision, val_precision, train_recall, val_recall, num_epochs):\n    \"\"\"\n    Funzione per plottare le metriche di training e validazione (Loss e mAP).\n    \"\"\"\n    epochs_range = range(1, num_epochs + 1)\n\n    plt.figure(figsize=(12, 8))\n\n    # Plot della Loss\n    plt.subplot(2, 1, 1)\n    plt.plot(epochs_range, train_losses, label='Training Loss', color='blue')\n    plt.plot(epochs_range, val_losses, label='Validation Loss', color='red')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss per Epoca')\n\n    # Plot della precision\n    plt.subplot(2, 1, 2)\n    plt.plot(epochs_range, train_precision, label='Training precision', color='green')\n    plt.plot(epochs_range, val_precision, label='Validation precision', color='orange')\n    plt.xlabel('Epochs')\n    plt.ylabel('Precision')\n    plt.legend()\n    plt.title('Precision per Epoca')\n\n    # Plot della recall\n    plt.subplot(2, 1, 2)\n    plt.plot(epochs_range, train_recall, label='Training recall', color='green')\n    plt.plot(epochs_range, val_recall, label='Validation recall', color='orange')\n    plt.xlabel('Epochs')\n    plt.ylabel('Recall')\n    plt.legend()\n    plt.title('Recall per Epoca')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:29:46.792810Z","iopub.execute_input":"2024-12-19T13:29:46.793086Z","iopub.status.idle":"2024-12-19T13:29:46.801455Z","shell.execute_reply.started":"2024-12-19T13:29:46.793060Z","shell.execute_reply":"2024-12-19T13:29:46.800599Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"def visualize_predictions(image, boxes, labels, scores, class_names, threshold=0.5):\n    \"\"\"\n    Visualizza le predizioni (bounding boxes e etichette) su una singola immagine.\n    \"\"\"\n    fig, ax = plt.subplots(1, figsize=(12, 9))\n    ax.imshow(image)\n\n    # Filtra predizioni con punteggio sopra la soglia\n    for box, label, score in zip(boxes, labels, scores):\n        if score >= threshold:\n            # Rettangolo per la bounding box\n            rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n                                      linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n\n            # Etichetta con classe e punteggio\n            ax.text(box[0], box[1] - 10, f'{class_names[label]}: {score:.2f}', color='r',\n                    fontsize=12, fontweight='bold', backgroundcolor='white')\n\n    plt.show()\n\n\ndef test_model(model, test_loader, device, class_names, calculate_map=None, num_classes=12, num_visualizations=5):\n    \"\"\"\n    Funzione per il testing del modello Faster R-CNN.\n    \"\"\"\n    model.to(device)\n    model.eval()\n    predictions = []\n    all_preds = []  # Per calcolo mAP\n    all_targets = []  # Per calcolo mAP\n\n    print(\"\\nInizio testing...\")\n    test_loop = tqdm(test_loader, desc=\"Testing\", leave=False)\n    visualized = 0  # Contatore per visualizzazioni\n\n    with torch.no_grad():\n        for images, targets in test_loop:\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Targets su dispositivo\n\n            # Predizioni dal modello\n            preds = model(images)\n\n            for pred, target in zip(preds, targets):\n                predictions.append({\n                    'boxes': pred['boxes'].cpu().numpy(),\n                    'labels': pred['labels'].cpu().numpy(),\n                    'scores': pred['scores'].cpu().numpy()\n                })\n\n                # Raccogli predizioni e target per calcolo mAP\n                all_preds.append(pred)\n                all_targets.append(target)\n\n                # Visualizza immagini con bounding box\n                if visualized < num_visualizations:\n                    visualize_predictions(\n                        images[0].cpu().numpy().transpose(1, 2, 0),  # Converti (C, H, W) -> (H, W, C)\n                        pred['boxes'].cpu().numpy(),\n                        pred['labels'].cpu().numpy(),\n                        pred['scores'].cpu().numpy(),\n                        class_names\n                    )\n                    visualized += 1\n\n            # Aggiorna barra di avanzamento\n            test_loop.set_postfix(processed=len(predictions))\n\n    # Calcola mAP se fornito\n    mAP = None\n    if calculate_map is not None:\n        mAP = calculate_map(all_preds, all_targets, num_classes=num_classes)\n        print(f\"mAP di testing: {mAP:.4f}\")\n\n    print(\"Testing completato.\")\n    return predictions, mAP\n\n\ndef evaluate_precision_recall(model, data_loader, device, num_classes, iou_threshold=0.5, score_threshold=0.5):\n    \"\"\"\n    Calcola Precision e Recall basati su IoU per Faster R-CNN.\n    \"\"\"\n    model.eval()\n    true_positives = [0] * num_classes\n    false_positives = [0] * num_classes\n    false_negatives = [0] * num_classes\n\n    with torch.no_grad():\n        for images, targets in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # Predizioni\n            outputs = model(images)\n\n            for target, output in zip(targets, outputs):\n                target_boxes = target['boxes'].cpu()\n                target_labels = target['labels'].cpu()\n                pred_boxes = output['boxes'].cpu()\n                pred_labels = output['labels'].cpu()\n                pred_scores = output['scores'].cpu()\n\n                # Filtra predizioni con punteggio basso\n                high_score_indices = pred_scores > score_threshold\n                pred_boxes = pred_boxes[high_score_indices]\n                pred_labels = pred_labels[high_score_indices]\n\n                # Confronta predizioni e target\n                if len(pred_boxes) > 0 and len(target_boxes) > 0:\n                    ious = box_iou(pred_boxes, target_boxes)  # IoU\n                    for pred_idx, pred_label in enumerate(pred_labels):\n                        iou_values = ious[pred_idx]\n                        max_iou_idx = torch.argmax(iou_values)\n                        max_iou = iou_values[max_iou_idx]\n\n                        if max_iou >= iou_threshold and pred_label == target_labels[max_iou_idx]:\n                            true_positives[pred_label] += 1\n                            ious[:, max_iou_idx] = 0  # Disabilita target già usati\n                        else:\n                            false_positives[pred_label] += 1\n\n                # False negatives\n                for target_label in target_labels:\n                    if target_label not in pred_labels:\n                        false_negatives[target_label] += 1\n\n    # Precision e Recall\n    precision = sum(true_positives) / (sum(true_positives) + sum(false_positives) + 1e-6)\n    recall = sum(true_positives) / (sum(true_positives) + sum(false_negatives) + 1e-6)\n\n    return precision, recall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:29:47.392479Z","iopub.execute_input":"2024-12-19T13:29:47.393018Z","iopub.status.idle":"2024-12-19T13:29:47.414824Z","shell.execute_reply.started":"2024-12-19T13:29:47.392974Z","shell.execute_reply":"2024-12-19T13:29:47.413726Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# Carica il modello Faster R-CNN con ResNet50 e FPN\nmodel = fasterrcnn_resnet50_fpn(weights=None)\n\nnum_classes = 12\n\n# Modifica il numero di classi in output\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\n# Congela i layer della backbone (ResNet50)\nfor param in model.backbone.parameters():\n    param.requires_grad = False\n\n# Imposta il dispositivo (GPU o CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Configurazione training\nnum_epochs = 2\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\n\n# Sposta il modello su GPU o CPU\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:29:47.452183Z","iopub.execute_input":"2024-12-19T13:29:47.452497Z","iopub.status.idle":"2024-12-19T13:29:48.136881Z","shell.execute_reply.started":"2024-12-19T13:29:47.452470Z","shell.execute_reply":"2024-12-19T13:29:48.136004Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=12, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=48, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"# Calcola i pesi delle classi\nclass_weights = compute_class_weights(train_loader.dataset)\n\nprint(class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:29:48.138828Z","iopub.execute_input":"2024-12-19T13:29:48.139200Z","iopub.status.idle":"2024-12-19T13:31:46.301646Z","shell.execute_reply.started":"2024-12-19T13:29:48.139161Z","shell.execute_reply":"2024-12-19T13:31:46.300797Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"                                                                                      ","output_type":"stream"},{"name":"stdout","text":"[3.84735294e+02 2.98724497e+00 1.92706335e+01 1.58768798e+02\n 1.06730547e+02 1.19741237e+02 1.73850792e+00 4.58394017e+03\n 3.46460594e+02 1.29828371e+02 1.38943264e+03]\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"train_losses, val_losses, train_precision, val_precision, train_recall, val_recall = train_and_validate(model, train_loader, val_loader, optimizer, device, class_weights, num_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:31:46.302796Z","iopub.execute_input":"2024-12-19T13:31:46.303045Z"}},"outputs":[{"name":"stdout","text":"\nEpoca 1/2\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Perdita media di training: 1.6738\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Precision di training: 0.2521, Recall di training: 0.5094\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Perdita media di validazione: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Precision di validazione: 0.2525, Recall di validazione: 0.5169\nModello salvato: model_epoch_1.pth\n\nEpoca 2/2\n","output_type":"stream"},{"name":"stderr","text":"Training:  65%|██████▍   | 523/806 [17:25<09:21,  1.99s/it, loss=1.25] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"plot_metrics(train_losses, val_losses, train_precision, val_precision, train_recall, val_recall, num_epochs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions, mAP = test_model(model, test_loader, device, class_names=class_names, calculate_map=calculate_map, num_classes=num_classes, num_visualizations=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}