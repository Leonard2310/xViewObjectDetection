{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10182328,"sourceType":"datasetVersion","datasetId":6242793},{"sourceId":211538,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":180326,"modelId":202589}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import delle librerie","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport random\nimport xml.etree.ElementTree as ET\nimport torchvision.transforms.functional as FT\n\nimport torch\nfrom tqdm import tqdm\nfrom pprint import PrettyPrinter\nfrom torch import nn\nimport torchvision\n\nimport json\nimport random\nfrom collections import defaultdict\n\nimport sys\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms.functional as FT\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:33.057204Z","iopub.execute_input":"2024-12-30T19:37:33.057468Z","iopub.status.idle":"2024-12-30T19:37:40.276526Z","shell.execute_reply.started":"2024-12-30T19:37:33.057440Z","shell.execute_reply":"2024-12-30T19:37:40.275855Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Path","metadata":{}},{"cell_type":"code","source":"# path del dataset\nbase_dict = '/kaggle/input/our-xview-dataset'\n\n# path della cartella contenente le immagini\nimg_dict = '/kaggle/input/our-xview-dataset/images'\n\n# path di output\noutput_folder = '/kaggle/working/'\n\n# path contenente le annotazioni in formato .json\ncoco_json_path = os.path.join(base_dict, 'COCO_annotations_new.json') \nnew_coco_json_path = os.path.join(output_folder, 'mod_COCO_annotations.json') \n\n# path file per il training\ntrain_image = os.path.join(output_folder, 'TRAIN_images.json')\ntrain_bbox = os.path.join(output_folder, 'TRAIN_objects.json')\n\n# path file per la validation\nval_image = os.path.join(output_folder, 'VAL_images.json')\nval_bbox = os.path.join(output_folder, 'VAL_objects.json')\n\n# path file per il test\ntest_image = os.path.join(output_folder, 'TEST_images.json')\ntest_bbox = os.path.join(output_folder, 'TEST_objects.json')\n\ncheckpoint_path = '/kaggle/input/ssd_checkpoint/pytorch/ssd/3/checkpoint_ssd300.pth.tar'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:40.277744Z","iopub.execute_input":"2024-12-30T19:37:40.278098Z","iopub.status.idle":"2024-12-30T19:37:40.283587Z","shell.execute_reply.started":"2024-12-30T19:37:40.278071Z","shell.execute_reply":"2024-12-30T19:37:40.282769Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:40.284536Z","iopub.execute_input":"2024-12-30T19:37:40.284809Z","iopub.status.idle":"2024-12-30T19:37:40.380149Z","shell.execute_reply.started":"2024-12-30T19:37:40.284784Z","shell.execute_reply":"2024-12-30T19:37:40.379313Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# COCO Preprocessing","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    data = load_json(input_path)\n\n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n \n    for category in tqdm(raw_categories, desc=\"Processing Categories\"):\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n \n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n \n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n \n    # Preprocessa le annotazioni in un dizionario per immagini\n    image_annotations_dict = {}\n    for annotation in tqdm(data.get('annotations', []), desc=\"Building Image Annotations Dictionary\"):\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_dict:\n            image_annotations_dict[image_id] = []\n        image_annotations_dict[image_id].append(annotation)\n \n    # Elenco di annotazioni da mantenere (solo quelle valide)\n    valid_annotations = []\n    annotations_to_remove = set()\n \n    # Controllo dei bounding box\n    for annotation in tqdm(data.get('annotations', []), desc=\"Processing Annotations\"):\n        if annotation['category_id'] == 0:  # Se è Aircraft\n            annotation['category_id'] = 11\n        \n        # Converte il formato del bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n        \n        x, y, width, height = annotation['bbox']\n        xmin, xmax = x, x + width\n        ymin, ymax = y, y + height\n        \n        # Verifica che xmin < xmax e ymin < ymax, e che la larghezza e altezza siano sufficienti\n        if xmin >= xmax or ymin >= ymax or width <= 10 or height <= 10:\n            annotations_to_remove.add(annotation['id'])\n        else:\n            annotation['bbox'] = [xmin, ymin, xmax, ymax]\n            valid_annotations.append(annotation)\n \n    # Rimuovi le annotazioni non valide\n    data['annotations'] = valid_annotations\n \n    # Verifica se ci sono immagini senza annotazioni (usando il dizionario delle annotazioni)\n    new_annotations = []\n    for image in tqdm(data.get('images', []), desc=\"Processing Images\"):\n        if image['id'] not in image_annotations_dict:  # Se l'immagine non ha annotazioni\n            # Aggiungi la categoria \"background\"\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, 0.0, image['width'], image['height']],  # Background con bbox che copre tutta l'immagine\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n \n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n \n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n \n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:40.382313Z","iopub.execute_input":"2024-12-30T19:37:40.382666Z","iopub.status.idle":"2024-12-30T19:37:40.395344Z","shell.execute_reply.started":"2024-12-30T19:37:40.382638Z","shell.execute_reply":"2024-12-30T19:37:40.394688Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_json(file_path):\n    \"\"\"\n    Carica un file JSON dal percorso specificato.\n\n    :param file_path: Percorso al file JSON da caricare.\n    :return: Dati contenuti nel file JSON (come dizionario o lista).\n    \"\"\"\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:40.396301Z","iopub.execute_input":"2024-12-30T19:37:40.396625Z","iopub.status.idle":"2024-12-30T19:37:40.409810Z","shell.execute_reply.started":"2024-12-30T19:37:40.396588Z","shell.execute_reply":"2024-12-30T19:37:40.409207Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"process_custom_coco_json(coco_json_path, new_coco_json_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:40.410604Z","iopub.execute_input":"2024-12-30T19:37:40.410865Z","iopub.status.idle":"2024-12-30T19:37:53.223433Z","shell.execute_reply.started":"2024-12-30T19:37:40.410840Z","shell.execute_reply":"2024-12-30T19:37:53.222381Z"}},"outputs":[{"name":"stderr","text":"Processing Categories: 100%|██████████| 11/11 [00:00<00:00, 142399.21it/s]\nBuilding Image Annotations Dictionary: 100%|██████████| 669983/669983 [00:00<00:00, 1648402.42it/s]\nProcessing Annotations: 100%|██████████| 669983/669983 [00:03<00:00, 217598.24it/s]\nProcessing Images: 100%|██████████| 45891/45891 [00:00<00:00, 952927.16it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"def split_coco_and_check(coco_file, output_path, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n    # Carica il file COCO\n    with open(coco_file, 'r') as f:\n        coco_data = json.load(f)\n\n    images = coco_data['images']\n    annotations = coco_data['annotations']\n    categories = coco_data['categories']\n\n    total_images = len(images)\n    total_annotations = len(annotations)\n\n    # Shuffle delle immagini per garantire casualità\n    random.shuffle(images)\n\n    # Calcola i numeri di immagini per train, val e test\n    train_count = int(train_ratio * total_images)\n    val_count = int(val_ratio * total_images)\n    test_count = total_images - train_count - val_count\n\n    train_images = images[:train_count]\n    val_images = images[train_count:train_count + val_count]\n    test_images = images[train_count + val_count:]\n\n    # Crea set di ID immagini\n    train_ids = {img['id'] for img in train_images}\n    val_ids = {img['id'] for img in val_images}\n    test_ids = {img['id'] for img in test_images}\n\n    # Divide le annotazioni\n    train_annotations = [ann for ann in annotations if ann['image_id'] in train_ids]\n    val_annotations = [ann for ann in annotations if ann['image_id'] in val_ids]\n    test_annotations = [ann for ann in annotations if ann['image_id'] in test_ids]\n\n    train_bbox_count = len(train_annotations)\n    val_bbox_count = len(val_annotations)\n    test_bbox_count = len(test_annotations)\n\n    # Salva i file di output\n    def save_split(file_name, images_split):\n        with open(file_name, 'w') as f:\n            for img in images_split:\n                f.write(f\"{output_path}/{img['file_name']}\\n\")\n\n    save_split('train.txt', train_images)\n    save_split('val.txt', val_images)\n    save_split('test.txt', test_images)\n\n    # Controlla le proporzioni\n    check_split_proportions(\n        total_images, total_annotations,\n        len(train_images), len(val_images), len(test_images),\n        train_bbox_count, val_bbox_count, test_bbox_count,\n        train_ratio, val_ratio, test_ratio,\n        train_annotations, val_annotations, test_annotations,\n        categories\n    )\n\ndef check_split_proportions(total_images, total_annotations, train_count, val_count, test_count, \n                            train_bbox_count, val_bbox_count, test_bbox_count, \n                            train_ratio, val_ratio, test_ratio, \n                            train_annotations, val_annotations, test_annotations, categories):\n    # Percentuali per immagini\n    train_image_percentage = (train_count / total_images) * 100\n    val_image_percentage = (val_count / total_images) * 100\n    test_image_percentage = (test_count / total_images) * 100\n\n    # Percentuali per bbox\n    train_bbox_percentage = (train_bbox_count / total_annotations) * 100\n    val_bbox_percentage = (val_bbox_count / total_annotations) * 100\n    test_bbox_percentage = (test_bbox_count / total_annotations) * 100\n\n    print(f\"Totale immagini: {total_images}\")\n    print(f\"Totale annotazioni (bbox): {total_annotations}\")\n    print(f\"Train: {train_count} immagini ({train_image_percentage:.2f}%) ({train_bbox_count} bbox) ({train_bbox_percentage:.2f}%)\")\n    print(f\"Val: {val_count} immagini ({val_image_percentage:.2f}%) ({val_bbox_count} bbox) ({val_bbox_percentage:.2f}%)\")\n    print(f\"Test: {test_count} immagini ({test_image_percentage:.2f}%) ({test_bbox_count} bbox) ({test_bbox_percentage:.2f}%)\")\n\n    # Calcola il numero di annotazioni per categoria nei vari set\n    category_count_train = defaultdict(int)\n    category_count_val = defaultdict(int)\n    category_count_test = defaultdict(int)\n\n    for annotation in train_annotations:\n        category_count_train[annotation['category_id']] += 1\n    for annotation in val_annotations:\n        category_count_val[annotation['category_id']] += 1\n    for annotation in test_annotations:\n        category_count_test[annotation['category_id']] += 1\n\n    # Stampa le proporzioni per categoria\n    print(\"\\nProporzioni per categoria:\")\n    for category in categories:\n        category_id = category['id']\n        category_name = category['name']\n\n        # Conta il numero di annotazioni per categoria in ogni set\n        train_cat_count = category_count_train.get(category_id, 0)\n        val_cat_count = category_count_val.get(category_id, 0)\n        test_cat_count = category_count_test.get(category_id, 0)\n\n        # Calcola la percentuale di annotazioni per categoria\n        total_cat_annotations = train_cat_count + val_cat_count + test_cat_count\n        if total_cat_annotations > 0:\n            train_cat_percentage = (train_cat_count / total_cat_annotations) * 100\n            val_cat_percentage = (val_cat_count / total_cat_annotations) * 100\n            test_cat_percentage = (test_cat_count / total_cat_annotations) * 100\n        else:\n            train_cat_percentage = val_cat_percentage = test_cat_percentage = 0.0\n\n        print(f\"{category_name}:\")\n        print(f\"  Train: {train_cat_count} annotazioni ({train_cat_percentage:.2f}%)\")\n        print(f\"  Val: {val_cat_count} annotazioni ({val_cat_percentage:.2f}%)\")\n        print(f\"  Test: {test_cat_count} annotazioni ({test_cat_percentage:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:53.224772Z","iopub.execute_input":"2024-12-30T19:37:53.225333Z","iopub.status.idle":"2024-12-30T19:37:53.239520Z","shell.execute_reply.started":"2024-12-30T19:37:53.225293Z","shell.execute_reply":"2024-12-30T19:37:53.238527Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"split_coco_and_check(new_coco_json_path, img_dict, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:53.240542Z","iopub.execute_input":"2024-12-30T19:37:53.240835Z","iopub.status.idle":"2024-12-30T19:37:56.181919Z","shell.execute_reply.started":"2024-12-30T19:37:53.240809Z","shell.execute_reply":"2024-12-30T19:37:56.180857Z"}},"outputs":[{"name":"stdout","text":"Totale immagini: 45891\nTotale annotazioni (bbox): 497406\nTrain: 27534 immagini (60.00%) (298555 bbox) (60.02%)\nVal: 9178 immagini (20.00%) (99360 bbox) (19.98%)\nTest: 9179 immagini (20.00%) (100803 bbox) (20.27%)\n\nProporzioni per categoria:\nAircraft:\n  Train: 951 annotazioni (60.92%)\n  Val: 288 annotazioni (18.45%)\n  Test: 322 annotazioni (20.63%)\nPassenger Vehicle:\n  Train: 57572 annotazioni (61.16%)\n  Val: 17622 annotazioni (18.72%)\n  Test: 18945 annotazioni (20.12%)\nTruck:\n  Train: 14434 annotazioni (58.53%)\n  Val: 4841 annotazioni (19.63%)\n  Test: 5387 annotazioni (21.84%)\nRailway Vehicle:\n  Train: 2273 annotazioni (61.58%)\n  Val: 793 annotazioni (21.48%)\n  Test: 625 annotazioni (16.93%)\nMaritime Vessel:\n  Train: 3264 annotazioni (63.17%)\n  Val: 965 annotazioni (18.68%)\n  Test: 938 annotazioni (18.15%)\nEngineering Vehicle:\n  Train: 2892 annotazioni (61.06%)\n  Val: 1084 annotazioni (22.89%)\n  Test: 760 annotazioni (16.05%)\nBuilding:\n  Train: 204995 annotazioni (59.56%)\n  Val: 69540 annotazioni (20.20%)\n  Test: 69658 annotazioni (20.24%)\nHelipad:\n  Train: 74 annotazioni (54.41%)\n  Val: 37 annotazioni (27.21%)\n  Test: 25 annotazioni (18.38%)\nStorage Tank:\n  Train: 1007 annotazioni (57.77%)\n  Val: 368 annotazioni (21.11%)\n  Test: 368 annotazioni (21.11%)\nShipping Container:\n  Train: 2598 annotazioni (56.70%)\n  Val: 986 annotazioni (21.52%)\n  Test: 998 annotazioni (21.78%)\nPylon:\n  Train: 252 annotazioni (60.72%)\n  Val: 95 annotazioni (22.89%)\n  Test: 68 annotazioni (16.39%)\nbackground:\n  Train: 8243 annotazioni (60.20%)\n  Val: 2741 annotazioni (20.02%)\n  Test: 2709 annotazioni (19.78%)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"def parse_coco_annotation(annotation_data):\n    \"\"\"\n    Converti le annotazioni COCO in una struttura utile.\n    \"\"\"\n    boxes = []\n    labels = []\n\n    for ann in annotation_data:\n        category_id = ann['category_id']  # Usa direttamente il category_id come etichetta\n        bbox = ann['bbox']  # I bounding box sono già nel formato [xmin, ymin, xmax, ymax]\n        xmin, ymin, xmax, ymax = bbox\n\n        boxes.append([xmin, ymin, xmax, ymax])\n        labels.append(category_id)\n\n    return {'boxes': boxes, 'labels': labels}\n\ndef create_coco_data_lists(coco_file, splits_path, output_folder):\n    \"\"\"\n    Converte i dati COCO e split in liste per train, val e test.\n    \"\"\"\n    with open(coco_file, 'r') as f:\n        coco_data = json.load(f)\n\n    # Prepara mappature da immagini e annotazioni\n    images = {img['file_name']: img for img in coco_data['images']}  # Mappa file_name -> immagine\n    annotations_by_image = defaultdict(list)\n    for ann in coco_data['annotations']:\n        annotations_by_image[ann['image_id']].append(ann)\n\n    # Genera i dati per ciascuno split\n    for split in ['train', 'val', 'test']:\n        split_file = os.path.join(splits_path, f\"{split}.txt\")\n        with open(split_file, 'r') as f:\n            image_files = [line.strip() for line in f.readlines()]\n\n        image_list = []\n        objects_list = []\n\n        for image_file in image_files:\n            file_name = os.path.basename(image_file)  # Ottieni solo il nome del file\n            if file_name not in images:\n                continue\n\n            image_info = images[file_name]\n            image_id = image_info['id']\n            annotations = annotations_by_image[image_id]\n            objects = parse_coco_annotation(annotations)\n\n            if not objects['boxes']:\n                continue\n\n            image_list.append(image_file)\n            objects_list.append(objects)\n\n        # Salva i risultati\n        with open(os.path.join(output_folder, f\"{split.upper()}_images.json\"), 'w') as j:\n            json.dump(image_list, j)\n        with open(os.path.join(output_folder, f\"{split.upper()}_objects.json\"), 'w') as j:\n            json.dump(objects_list, j)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:56.183247Z","iopub.execute_input":"2024-12-30T19:37:56.183550Z","iopub.status.idle":"2024-12-30T19:37:56.194113Z","shell.execute_reply.started":"2024-12-30T19:37:56.183521Z","shell.execute_reply":"2024-12-30T19:37:56.192876Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"create_coco_data_lists(new_coco_json_path, output_folder, output_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:56.197153Z","iopub.execute_input":"2024-12-30T19:37:56.197507Z","iopub.status.idle":"2024-12-30T19:38:03.010353Z","shell.execute_reply.started":"2024-12-30T19:37:56.197479Z","shell.execute_reply":"2024-12-30T19:38:03.009222Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset class to be used in a DataLoader for batching.\n    \"\"\"\n\n    def __init__(self, path_image, path_bbox):\n        \"\"\"\n        :param path_image: Path to the JSON file containing image paths.\n        :param path_bbox: Path to the JSON file containing bounding boxes and labels.\n        \"\"\"\n        # Load data\n        with open(path_image, 'r') as j:\n            self.images = json.load(j)\n        with open(path_bbox, 'r') as j:\n            self.objects = json.load(j)\n\n        # Ensure the lengths match\n        assert len(self.images) == len(self.objects), \"Images and annotations must have the same length.\"\n\n    def __transform(self, image, boxes, labels):\n        \"\"\"\n        Apply transformations to the image, boxes, and labels.\n        :param image: A PIL Image.\n        :param boxes: Bounding boxes as a tensor of dimensions (n_objects, 4).\n        :param labels: Labels as a tensor of dimensions (n_objects).\n        :return: Transformed image, boxes, and labels.\n        \"\"\"\n        def resize(image, boxes, dims=(300, 300)):\n            # Resize image\n            new_image = FT.resize(image, dims)\n\n            # Normalize bounding boxes\n            old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n            new_boxes = boxes / old_dims  # Percent coordinates\n\n            return new_image, new_boxes\n\n        # ImageNet normalization values\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n\n        # Resize image and normalize boxes\n        image, boxes = resize(image, boxes)\n\n        # Convert image to tensor\n        image = FT.to_tensor(image)\n\n        # Normalize image\n        image = FT.normalize(image, mean=mean, std=std)\n\n        return image, boxes, labels\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieve an image and its corresponding objects.\n        :param idx: Index of the data point.\n        :return: Transformed image, bounding boxes, and labels.\n        \"\"\"\n        # Load image\n        image = Image.open(self.images[idx]).convert('RGB')\n\n        # Load objects\n        objects = self.objects[idx]\n        boxes = torch.FloatTensor(objects['boxes'])  # (n_objects, 4)\n        labels = torch.LongTensor(objects['labels'])  # (n_objects)\n\n        # Apply transformations\n        image, boxes, labels = self.__transform(image, boxes, labels)\n\n        return image, boxes, labels\n\n    def __len__(self):\n        \"\"\"\n        Total number of data points.\n        :return: Length of the dataset.\n        \"\"\"\n        return len(self.images)\n\n    def collate_fn(self, batch):\n        \"\"\"\n        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n\n        This describes how to combine these tensors of different sizes. We use lists.\n\n        :param batch: an iterable of N sets from __getitem__()\n        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels.\n        \"\"\"\n\n        images = list()\n        boxes = list()\n        labels = list()\n\n        for b in batch:\n            images.append(b[0])\n            boxes.append(b[1])\n            labels.append(b[2])\n\n        images = torch.stack(images, dim=0)\n\n        return images, boxes, labels  # tensor (N, 3, 300, 300), 2 lists of N tensors each","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:03.011661Z","iopub.execute_input":"2024-12-30T19:38:03.012051Z","iopub.status.idle":"2024-12-30T19:38:03.023336Z","shell.execute_reply.started":"2024-12-30T19:38:03.012014Z","shell.execute_reply":"2024-12-30T19:38:03.022384Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_dataset = CustomDataset(train_image, train_bbox)\nval_dataset = CustomDataset(val_image, val_bbox)\ntest_dataset = CustomDataset(test_image, test_bbox)\n\n# Recupera un campione\nimage, boxes, labels = train_dataset[0]\n\n# Dimensioni\nprint(\"Image shape:\", image.shape)  # Torch tensor di dimensione (3, 300, 300)\nprint(\"Boxes:\", boxes)             # Bounding box normalizzati\nprint(\"Labels:\", labels)           # Etichette","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:03.024442Z","iopub.execute_input":"2024-12-30T19:38:03.024775Z","iopub.status.idle":"2024-12-30T19:38:04.362239Z","shell.execute_reply.started":"2024-12-30T19:38:03.024705Z","shell.execute_reply":"2024-12-30T19:38:04.361303Z"}},"outputs":[{"name":"stdout","text":"Image shape: torch.Size([3, 300, 300])\nBoxes: tensor([[0.4125, 0.5562, 0.4656, 0.6594],\n        [0.5594, 0.6625, 0.6656, 0.7125],\n        [0.6875, 0.5125, 0.7625, 0.6094],\n        [0.4313, 0.0000, 1.0000, 0.5375],\n        [0.1813, 0.7937, 0.2250, 0.8594],\n        [0.3906, 0.8469, 0.4469, 0.9344],\n        [0.4750, 0.8750, 0.5344, 0.9656],\n        [0.0000, 0.7937, 0.6875, 1.0000],\n        [0.5125, 0.7875, 1.0000, 1.0031],\n        [0.2688, 0.8281, 0.3125, 0.8906]])\nLabels: tensor([2, 2, 2, 6, 2, 2, 2, 6, 6, 2])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, \n                                               collate_fn=train_dataset.collate_fn, num_workers=3, pin_memory=True)\nval_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, \n                                             collate_fn=train_dataset.collate_fn, num_workers=3, pin_memory=True)\ntest_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, \n                                              collate_fn=train_dataset.collate_fn, num_workers=3, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:04.363661Z","iopub.execute_input":"2024-12-30T19:38:04.363994Z","iopub.status.idle":"2024-12-30T19:38:04.369650Z","shell.execute_reply.started":"2024-12-30T19:38:04.363961Z","shell.execute_reply":"2024-12-30T19:38:04.368792Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"fare l'import da GitHub https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py","metadata":{}},{"cell_type":"code","source":"# Aggiungi la directory che contiene 'model.py' alla variabile sys.path\nsys.path.append('/kaggle/input/ssd_checkpoint/pytorch/ssd/3')\n\n# Ora importa il modulo 'model' che si trova nella stessa cartella\nfrom model import *  \n\n# Carica il checkpoint\ncheckpoint = torch.load(checkpoint_path, map_location='cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:04.370789Z","iopub.execute_input":"2024-12-30T19:38:04.371150Z","iopub.status.idle":"2024-12-30T19:38:05.800187Z","shell.execute_reply.started":"2024-12-30T19:38:04.371110Z","shell.execute_reply":"2024-12-30T19:38:05.799431Z"}},"outputs":[{"name":"stderr","text":"/kaggle/input/ssd_checkpoint/pytorch/ssd/3/utils.py:570: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if d.__name__ is 'adjust_hue':\n/tmp/ipykernel_23/81667907.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1189: SourceChangeWarning: source code of class 'model.SSD300' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1189: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1189: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Verifica le chiavi nel checkpoint (opzionale)\nprint(checkpoint.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:05.801090Z","iopub.execute_input":"2024-12-30T19:38:05.801318Z","iopub.status.idle":"2024-12-30T19:38:05.805875Z","shell.execute_reply.started":"2024-12-30T19:38:05.801295Z","shell.execute_reply":"2024-12-30T19:38:05.805014Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['epoch', 'model'])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Crea un'istanza del modello\nmodel = SSD300(21)  # Assicurati che la definizione del modello sia corretta\n\n# Carica i pesi nel modello\n#model.load_state_dict(checkpoint['model'].state_dict())\n\n# Carica i pesi pre-addestrati per il backbone e altre componenti comuni\npretrained_dict = checkpoint['model'].state_dict()\nmodel_dict = model.state_dict()\n\n# Mantieni solo i pesi che corrispondono tra il modello pre-addestrato e il nuovo\npretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\nmodel_dict.update(pretrained_dict)\nmodel.load_state_dict(model_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:05.806934Z","iopub.execute_input":"2024-12-30T19:38:05.807217Z","iopub.status.idle":"2024-12-30T19:38:10.719359Z","shell.execute_reply.started":"2024-12-30T19:38:05.807175Z","shell.execute_reply":"2024-12-30T19:38:10.718409Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 222MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nLoaded base model.\n\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def find_intersection(set_1, set_2):\n    \"\"\"\n    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # PyTorch auto-broadcasts singleton dimensions\n    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n\n\ndef find_jaccard_overlap(set_1, set_2):\n    \"\"\"\n    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # Find intersections\n    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n\n    # Find areas of each box in both sets\n    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n\n    # Find the union\n    # PyTorch auto-broadcasts singleton dimensions\n    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n\n    return intersection / union  # (n1, n2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:10.720503Z","iopub.execute_input":"2024-12-30T19:38:10.720875Z","iopub.status.idle":"2024-12-30T19:38:10.727540Z","shell.execute_reply.started":"2024-12-30T19:38:10.720847Z","shell.execute_reply":"2024-12-30T19:38:10.726673Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"def adjust_learning_rate(optimizer, scale):\n    \"\"\"\n    Scale learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param scale: factor to multiply learning rate with.\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * scale\n    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))\n\n\ndef accuracy(scores, targets, k):\n    \"\"\"\n    Computes top-k accuracy, from predicted and true labels.\n\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    \"\"\"\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 / batch_size)\n\n\ndef save_checkpoint(epoch, model, optimizer):\n    \"\"\"\n    Save model checkpoint.\n\n    :param epoch: epoch number\n    :param model: model\n    :param optimizer: optimizer\n    \"\"\"\n    state = {'epoch': epoch,\n             'model': model,\n             'optimizer': optimizer}\n    filename = 'checkpoint_ssd300.pth.tar'\n    torch.save(state, filename)\n\ndef clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{}},{"cell_type":"markdown","source":"class Trainer:\n    def __init__(self, model, train_dataset, train_dataloader, criterion, optimizer, batch_size, num_workers, device, \n                 grad_clip=None, print_freq=10, iterations=120000, decay_lr_at=None, decay_lr_to=0.1, \n                 momentum=0.9, weight_decay=5e-4):\n        \"\"\"\n        Initialize the Trainer.\n        \n        :param model: SSD300 model instance\n        :param train_dataset: Dataset object\n        :param criterion: Loss function\n        :param optimizer: Optimizer\n        :param batch_size: Training batch size\n        :param num_workers: Number of data loading workers\n        :param device: Device to use for training ('cuda' or 'cpu')\n        :param grad_clip: Gradient clipping value (default: None)\n        :param print_freq: Frequency of printing training progress\n        :param iterations: Total number of training iterations\n        :param decay_lr_at: Iterations to decay learning rate\n        :param decay_lr_to: Learning rate decay factor\n        :param momentum: Momentum for optimizer\n        :param weight_decay: Weight decay for optimizer\n        \"\"\"\n        self.model = model\n        self.train_dataset = train_dataset\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n        self.grad_clip = grad_clip\n        self.print_freq = print_freq\n        self.iterations = iterations\n        self.decay_lr_at = decay_lr_at if decay_lr_at is not None else [80000, 100000]\n        self.decay_lr_to = decay_lr_to\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n\n        # Prepare dataloader\n        self.train_loader = train_dataloader\n\n        # Calculate epochs and decay epochs\n        self.epochs = iterations // (len(train_dataset) // 32)\n        self.decay_epochs = [it // (len(train_dataset) // 32) for it in self.decay_lr_at]\n\n    def adjust_learning_rate(self, epoch):\n        \"\"\"\n        Adjust the learning rate at specific epochs.\n        \"\"\"\n        if epoch in self.decay_epochs:\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = param_group['lr'] * self.decay_lr_to\n            print(f\"Learning rate adjusted to {param_group['lr']} at epoch {epoch}\")\n\n    def train_one_epoch(self, epoch):\n        \"\"\"\n        Perform one epoch of training.\n        \"\"\"\n        self.model.train()\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n\n        start = time.time()\n\n        for i, (images, boxes, labels) in enumerate(self.train_loader):\n            data_time.update(time.time() - start)\n\n            # Move to device\n            images = images.to(self.device)\n            boxes = [b.to(self.device) for b in boxes]\n            labels = [l.to(self.device) for l in labels]\n\n            # Forward pass\n            predicted_locs, predicted_scores = self.model(images)\n\n            # Compute loss\n            loss = self.criterion(predicted_locs, predicted_scores, boxes, labels)\n\n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n\n            # Gradient clipping\n            if self.grad_clip is not None:\n                clip_gradient(self.optimizer, self.grad_clip)\n\n            # Update model parameters\n            self.optimizer.step()\n\n            # Update metrics\n            losses.update(loss.item(), images.size(0))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            # Print status\n            if i % self.print_freq == 0:\n                print('Epoch: [{0}][{1}/{2}]\\t'\n                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(self.train_loader),\n                                                                      batch_time=batch_time,\n                                                                      data_time=data_time,\n                                                                      loss=losses))\n\n        del predicted_locs, predicted_scores, images, boxes, labels\n\n    def save_checkpoint(self, epoch):\n        \"\"\"\n        Save model checkpoint.\n        \"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model': self.model,\n            'optimizer': self.optimizer,\n        }, f'checkpoint_epoch_{epoch}.pth')\n        print(f\"Checkpoint saved for epoch {epoch}.\")\n\n    def train(self, start_epoch=0):\n        \"\"\"\n        Train the model across all epochs.\n        \"\"\"\n        for epoch in range(start_epoch, self.epochs):\n            self.adjust_learning_rate(epoch)\n            self.train_one_epoch(epoch)\n            self.save_checkpoint(epoch)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-30T19:43:19.266925Z","iopub.execute_input":"2024-12-30T19:43:19.267364Z","iopub.status.idle":"2024-12-30T19:43:19.281590Z","shell.execute_reply.started":"2024-12-30T19:43:19.267330Z","shell.execute_reply":"2024-12-30T19:43:19.280644Z"}}},{"cell_type":"code","source":"from tqdm import tqdm\n\nclass Trainer:\n    def __init__(self, model, train_dataset, train_dataloader, criterion, optimizer, batch_size, num_workers, device, \n                 grad_clip=None, print_freq=10, iterations=120000, decay_lr_at=None, decay_lr_to=0.1, \n                 momentum=0.9, weight_decay=5e-4):\n        \"\"\"\n        Initialize the Trainer.\n        \n        :param model: SSD300 model instance\n        :param train_dataset: Dataset object\n        :param criterion: Loss function\n        :param optimizer: Optimizer\n        :param batch_size: Training batch size\n        :param num_workers: Number of data loading workers\n        :param device: Device to use for training ('cuda' or 'cpu')\n        :param grad_clip: Gradient clipping value (default: None)\n        :param print_freq: Frequency of printing training progress\n        :param iterations: Total number of training iterations\n        :param decay_lr_at: Iterations to decay learning rate\n        :param decay_lr_to: Learning rate decay factor\n        :param momentum: Momentum for optimizer\n        :param weight_decay: Weight decay for optimizer\n        \"\"\"\n        self.model = model\n        self.train_dataset = train_dataset\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n        self.grad_clip = grad_clip\n        self.print_freq = print_freq\n        self.iterations = iterations\n        self.decay_lr_at = decay_lr_at if decay_lr_at is not None else [80000, 100000]\n        self.decay_lr_to = decay_lr_to\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n\n        # Prepare dataloader\n        self.train_loader = train_dataloader\n\n        # Calculate epochs and decay epochs\n        #self.epochs = iterations // (len(train_dataset) // 32)\n        self.epochs = 10\n        self.decay_epochs = [it // (len(train_dataset) // 32) for it in self.decay_lr_at]\n\n    def adjust_learning_rate(self, epoch):\n        \"\"\"\n        Adjust the learning rate at specific epochs.\n        \"\"\"\n        if epoch in self.decay_epochs:\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = param_group['lr'] * self.decay_lr_to\n            print(f\"Learning rate adjusted to {param_group['lr']} at epoch {epoch}\")\n\n    def train_one_epoch(self, epoch):\n        \"\"\"\n        Perform one epoch of training.\n        \"\"\"\n        self.model.train()\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n\n        start = time.time()\n\n        with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch}/{self.epochs}\", unit=\"batch\") as pbar:\n            for i, (images, boxes, labels) in enumerate(self.train_loader):\n                data_time.update(time.time() - start)\n\n                # Move to device\n                images = images.to(self.device)\n                boxes = [b.to(self.device) for b in boxes]\n                labels = [l.to(self.device) for l in labels]\n\n                # Forward pass\n                predicted_locs, predicted_scores = self.model(images)\n\n                # Compute loss\n                loss = self.criterion(predicted_locs, predicted_scores, boxes, labels)\n\n                # Backward pass\n                self.optimizer.zero_grad()\n                loss.backward()\n\n                # Gradient clipping\n                if self.grad_clip is not None:\n                    clip_gradient(self.optimizer, self.grad_clip)\n\n                # Update model parameters\n                self.optimizer.step()\n\n                # Update metrics\n                losses.update(loss.item(), images.size(0))\n                batch_time.update(time.time() - start)\n\n                # Update progress bar\n                pbar.set_postfix({\n                    \"Batch Time\": f\"{batch_time.val:.3f}s\",\n                    \"Data Time\": f\"{data_time.val:.3f}s\",\n                    \"Loss\": f\"{losses.val:.4f} (avg: {losses.avg:.4f})\"\n                })\n                pbar.update(1)\n\n                start = time.time()\n\n        del predicted_locs, predicted_scores, images, boxes, labels\n\n    def save_checkpoint(self, epoch):\n        \"\"\"\n        Save model checkpoint.\n        \"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model': self.model,\n            'optimizer': self.optimizer,\n        }, f'checkpoint_epoch_{epoch}.pth')\n        print(f\"Checkpoint saved for epoch {epoch}.\")\n\n    def train(self, start_epoch=0):\n        \"\"\"\n        Train the model across all epochs.\n        \"\"\"\n        for epoch in range(start_epoch, self.epochs):\n            self.adjust_learning_rate(epoch)\n            self.train_one_epoch(epoch)\n            self.save_checkpoint(epoch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:01.022420Z","iopub.execute_input":"2024-12-30T19:56:01.022971Z","iopub.status.idle":"2024-12-30T19:56:01.038319Z","shell.execute_reply.started":"2024-12-30T19:56:01.022935Z","shell.execute_reply":"2024-12-30T19:56:01.037548Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class MultiBoxLoss(nn.Module):\n    \"\"\"\n    The MultiBox loss, a loss function for object detection.\n\n    This is a combination of:\n    (1) a localization loss for the predicted locations of the boxes, and\n    (2) a confidence loss for the predicted class scores.\n    \"\"\"\n\n    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n        super(MultiBoxLoss, self).__init__()\n        self.priors_cxcy = priors_cxcy\n        self.priors_xy = cxcy_to_xy(priors_cxcy)\n        self.threshold = threshold\n        self.neg_pos_ratio = neg_pos_ratio\n        self.alpha = alpha\n\n        self.smooth_l1 = nn.L1Loss()  # *smooth* L1 loss in the paper; see Remarks section in the tutorial\n        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n\n    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n        \"\"\"\n        Forward propagation.\n\n        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n        :param boxes: true  object bounding boxes in boundary coordinates, a list of N tensors\n        :param labels: true object labels, a list of N tensors\n        :return: multibox loss, a scalar\n        \"\"\"\n        batch_size = predicted_locs.size(0)\n        n_priors = self.priors_cxcy.size(0)\n        n_classes = predicted_scores.size(2)\n\n        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n\n        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)  # (N, 8732)\n\n        # For each image\n        for i in range(batch_size):\n            n_objects = boxes[i].size(0)\n\n            overlap = find_jaccard_overlap(boxes[i],\n                                           self.priors_xy)  # (n_objects, 8732)\n\n            # For each prior, find the object that has the maximum overlap\n            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)  # (8732)\n\n            # We don't want a situation where an object is not represented in our positive (non-background) priors -\n            # 1. An object might not be the best object for all priors, and is therefore not in object_for_each_prior.\n            # 2. All priors with the object may be assigned as background based on the threshold (0.5).\n\n            # To remedy this -\n            # First, find the prior that has the maximum overlap for each object.\n            _, prior_for_each_object = overlap.max(dim=1)  # (N_o)\n\n            # Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)\n            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n\n            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n            overlap_for_each_prior[prior_for_each_object] = 1.\n\n            # Labels for each prior\n            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n\n            # Store\n            true_classes[i] = label_for_each_prior\n\n            # Encode center-size object coordinates into the form we regressed predicted boxes to\n            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n\n        # Identify priors that are positive (object/non-background)\n        positive_priors = true_classes != 0  # (N, 8732)\n\n        # LOCALIZATION LOSS\n\n        # Localization loss is computed only over positive (non-background) priors\n        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n\n        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n\n        # CONFIDENCE LOSS\n\n        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n        # That is, FOR EACH IMAGE,\n        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n\n        # Number of positive and hard-negative priors per image\n        n_positives = positive_priors.sum(dim=1)  # (N)\n        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n\n        # First, find the loss for all priors\n        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n\n        # We already know which priors are positive\n        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n\n        # Next, find which priors are hard-negative\n        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n\n        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n\n        # TOTAL LOSS\n\n        return conf_loss + self.alpha * loc_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:01.039804Z","iopub.execute_input":"2024-12-30T19:56:01.040055Z","iopub.status.idle":"2024-12-30T19:56:01.061782Z","shell.execute_reply.started":"2024-12-30T19:56:01.040031Z","shell.execute_reply":"2024-12-30T19:56:01.061197Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:01.126053Z","iopub.execute_input":"2024-12-30T19:56:01.126316Z","iopub.status.idle":"2024-12-30T19:56:01.130596Z","shell.execute_reply.started":"2024-12-30T19:56:01.126291Z","shell.execute_reply":"2024-12-30T19:56:01.129728Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Model parameters\nn_classes = 12  # number of different types of objects\n\n# Learning parameters\ncheckpoint = None  # path to model checkpoint, None if none\nbatch_size = 8  # batch size\niterations = 10  # number of iterations to train\nworkers = 4  # number of workers for loading data in the DataLoader\nprint_freq = 200  # print training status every __ batches\nlr = 1e-3  # learning rate\ndecay_lr_at = [80000, 100000]  # decay learning rate after these many iterations\ndecay_lr_to = 0.1  # decay learning rate to this fraction of the existing learning rate\nmomentum = 0.9  # momentum\nweight_decay = 5e-4  # weight decay\ngrad_clip = 0.8  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\nmodel = SSD300(n_classes=n_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:01.132029Z","iopub.execute_input":"2024-12-30T19:56:01.132249Z","iopub.status.idle":"2024-12-30T19:56:03.172062Z","shell.execute_reply.started":"2024-12-30T19:56:01.132227Z","shell.execute_reply":"2024-12-30T19:56:03.171308Z"}},"outputs":[{"name":"stdout","text":"\nLoaded base model.\n\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n\n# Ottimizzatore\nbiases = [param for name, param in model.named_parameters() if param.requires_grad and name.endswith('.bias')]\nnot_biases = [param for name, param in model.named_parameters() if param.requires_grad and not name.endswith('.bias')]\noptimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n                            lr=lr, momentum=momentum, weight_decay=weight_decay)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:03.173214Z","iopub.execute_input":"2024-12-30T19:56:03.173567Z","iopub.status.idle":"2024-12-30T19:56:03.180851Z","shell.execute_reply.started":"2024-12-30T19:56:03.173530Z","shell.execute_reply":"2024-12-30T19:56:03.180091Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Configurazione del dispositivo\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Training on {device}\")\n\n# Preparazione del modello, dataset e dataloader\nmodel = model.to(device)  # Sposta il modello sul dispositivo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:03.181871Z","iopub.execute_input":"2024-12-30T19:56:03.182149Z","iopub.status.idle":"2024-12-30T19:56:03.230306Z","shell.execute_reply.started":"2024-12-30T19:56:03.182121Z","shell.execute_reply":"2024-12-30T19:56:03.229483Z"}},"outputs":[{"name":"stdout","text":"Training on cuda\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import time\n# Creazione del Trainer\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,\n    train_dataloader=train_dataloader,\n    criterion=criterion,\n    optimizer=optimizer,\n    batch_size=batch_size,\n    num_workers=workers,\n    device=device,\n    grad_clip=grad_clip\n)\n\n# Avvio del training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T21:16:15.247429Z","iopub.status.idle":"2024-12-30T21:16:15.247808Z","shell.execute_reply.started":"2024-12-30T21:16:15.247638Z","shell.execute_reply":"2024-12-30T21:16:15.247655Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing sulle predizioni","metadata":{}},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model, test_dataset, batch_size, num_workers, device):\n        \"\"\"\n        Initialize the Evaluator.\n        \n        :param model: Trained SSD model to be evaluated\n        :param test_dataset: Dataset object for testing\n        :param batch_size: Batch size for evaluation\n        :param num_workers: Number of data loading workers\n        :param device: Device to use for evaluation ('cuda' or 'cpu')\n        \"\"\"\n        self.model = model.to(device)\n        self.test_dataset = test_dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n        self.pp = PrettyPrinter()  # For printing APs nicely\n\n        # Prepare dataloader\n        self.test_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            collate_fn=test_dataset.collate_fn,\n            num_workers=num_workers,\n            pin_memory=True\n        )\n\n    def evaluate(self):\n        \"\"\"\n        Perform evaluation and compute mAP.\n        \"\"\"\n        self.model.eval()\n\n        # Lists to store detected and true boxes, labels, scores\n        det_boxes = list()\n        det_labels = list()\n        det_scores = list()\n        true_boxes = list()\n        true_labels = list()\n\n        with torch.no_grad():\n            for i, (images, boxes, labels) in enumerate(tqdm(self.test_loader, desc='Evaluating')):\n                images = images.to(self.device)\n\n                # Forward pass\n                predicted_locs, predicted_scores = self.model(images)\n\n                # Detect objects\n                det_boxes_batch, det_labels_batch, det_scores_batch = self.model.detect_objects(\n                    predicted_locs, predicted_scores,\n                    min_score=0.01, max_overlap=0.45, top_k=200\n                )\n\n                # Store this batch's results\n                boxes = [b.to(self.device) for b in boxes]\n                labels = [l.to(self.device) for l in labels]\n\n                det_boxes.extend(det_boxes_batch)\n                det_labels.extend(det_labels_batch)\n                det_scores.extend(det_scores_batch)\n                true_boxes.extend(boxes)\n                true_labels.extend(labels)\n\n        # Calculate mAP\n        APs, mAP = self.calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels)\n\n        # Print AP for each class\n        self.pp.pprint(APs)\n        print('\\nMean Average Precision (mAP): %.3f' % mAP)\n\n    @staticmethod\n    def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels):\n        \"\"\"\n        Calculate Mean Average Precision (mAP).\n        Placeholder for an actual implementation.\n        \n        :param det_boxes: Detected boxes\n        :param det_labels: Detected labels\n        :param det_scores: Detected scores\n        :param true_boxes: Ground truth boxes\n        :param true_labels: Ground truth labels\n        :param true_difficulties: Ground truth difficulties\n        :return: APs and mAP\n        \"\"\"\n        # Replace this with your actual mAP calculation logic\n        APs = {f'class_{i}': 0.0 for i in range(1, 21)}  # Dummy values for each class\n        mAP = 0.0  # Dummy value for mAP\n        return APs, mAP\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T21:16:50.423825Z","iopub.execute_input":"2024-12-30T21:16:50.424176Z","iopub.status.idle":"2024-12-30T21:16:50.435389Z","shell.execute_reply.started":"2024-12-30T21:16:50.424143Z","shell.execute_reply":"2024-12-30T21:16:50.434546Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels):\n    \"\"\"\n    Calculate the Mean Average Precision (mAP) of detected objects.\n\n    See https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173 for an explanation\n\n    :param det_boxes: list of tensors, one tensor for each image containing detected objects' bounding boxes\n    :param det_labels: list of tensors, one tensor for each image containing detected objects' labels\n    :param det_scores: list of tensors, one tensor for each image containing detected objects' labels' scores\n    :param true_boxes: list of tensors, one tensor for each image containing actual objects' bounding boxes\n    :param true_labels: list of tensors, one tensor for each image containing actual objects' labels\n    :return: list of average precisions for all classes, mean average precision (mAP)\n    \"\"\"\n    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(true_labels)  # these are all lists of tensors of the same length, i.e. number of images\n    n_classes = len(label_map)\n\n    # Store all (true) objects in a single continuous tensor while keeping track of the image it is from\n    true_images = list()\n    for i in range(len(true_labels)):\n        true_images.extend([i] * true_labels[i].size(0))\n    true_images = torch.LongTensor(true_images).to(\n        device)  # (n_objects), n_objects is the total no. of objects across all images\n    true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n    true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n\n    assert true_images.size(0) == true_boxes.size(0) == true_labels.size(0)\n\n    # Store all detections in a single continuous tensor while keeping track of the image it is from\n    det_images = list()\n    for i in range(len(det_labels)):\n        det_images.extend([i] * det_labels[i].size(0))\n    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n    det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n    det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n    det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n\n    assert det_images.size(0) == det_boxes.size(0) == det_labels.size(0) == det_scores.size(0)\n\n    # Calculate APs for each class (except background)\n    average_precisions = torch.zeros((n_classes - 1), dtype=torch.float)  # (n_classes - 1)\n    for c in range(1, n_classes):\n        # Extract only objects with this class\n        true_class_images = true_images[true_labels == c]  # (n_class_objects)\n        true_class_boxes = true_boxes[true_labels == c]  # (n_class_objects, 4)\n        n_easy_class_objects = (1 - true_class_difficulties).sum().item()  # ignore difficult objects\n\n        # Keep track of which true objects with this class have already been 'detected'\n        # So far, none\n        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(\n            device)  # (n_class_objects)\n\n        # Extract only detections with this class\n        det_class_images = det_images[det_labels == c]  # (n_class_detections)\n        det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n        det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n        n_class_detections = det_class_boxes.size(0)\n        if n_class_detections == 0:\n            continue\n\n        # Sort detections in decreasing order of confidence/scores\n        det_class_scores, sort_ind = torch.sort(det_class_scores, dim=0, descending=True)  # (n_class_detections)\n        det_class_images = det_class_images[sort_ind]  # (n_class_detections)\n        det_class_boxes = det_class_boxes[sort_ind]  # (n_class_detections, 4)\n\n        # In the order of decreasing scores, check if true or false positive\n        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n        for d in range(n_class_detections):\n            this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n            this_image = det_class_images[d]  # (), scalar\n\n            # Find objects in the same image with this class, their difficulties, and whether they have been detected before\n            object_boxes = true_class_boxes[true_class_images == this_image]  # (n_class_objects_in_img)\n            # If no such object in this image, then the detection is a false positive\n            if object_boxes.size(0) == 0:\n                false_positives[d] = 1\n                continue\n\n            # Find maximum overlap of this detection with objects in this image of this class\n            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars\n\n            # 'ind' is the index of the object in these image-level tensors 'object_boxes', 'object_difficulties'\n            # In the original class-level tensors 'true_class_boxes', etc., 'ind' corresponds to object with index...\n            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n            # We need 'original_ind' to update 'true_class_boxes_detected'\n\n            # If the maximum overlap is greater than the threshold of 0.5, it's a match\n            if max_overlap.item() > 0.5:\n                # If this object has already not been detected, it's a true positive\n                if true_class_boxes_detected[original_ind] == 0:\n                    true_positives[d] = 1\n                    true_class_boxes_detected[original_ind] = 1  # this object has now been detected/accounted for\n                # Otherwise, it's a false positive (since this object is already accounted for)\n                else:\n                    false_positives[d] = 1\n            # Otherwise, the detection occurs in a different location than the actual object, and is a false positive\n            else:\n                false_positives[d] = 1\n\n        # Compute cumulative precision and recall at each detection in the order of decreasing scores\n        cumul_true_positives = torch.cumsum(true_positives, dim=0)  # (n_class_detections)\n        cumul_false_positives = torch.cumsum(false_positives, dim=0)  # (n_class_detections)\n        cumul_precision = cumul_true_positives / (\n                cumul_true_positives + cumul_false_positives + 1e-10)  # (n_class_detections)\n        cumul_recall = cumul_true_positives / n_easy_class_objects  # (n_class_detections)\n\n        # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'\n        recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()  # (11)\n        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)  # (11)\n        for i, t in enumerate(recall_thresholds):\n            recalls_above_t = cumul_recall >= t\n            if recalls_above_t.any():\n                precisions[i] = cumul_precision[recalls_above_t].max()\n            else:\n                precisions[i] = 0.\n        average_precisions[c - 1] = precisions.mean()  # c is in [1, n_classes - 1]\n\n    # Calculate Mean Average Precision (mAP)\n    mean_average_precision = average_precisions.mean().item()\n\n    # Keep class-wise average precisions in a dictionary\n    average_precisions = {rev_label_map[c + 1]: v for c, v in enumerate(average_precisions.tolist())}\n\n    return average_precisions, mean_average_precision\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T21:16:50.525512Z","iopub.execute_input":"2024-12-30T21:16:50.525797Z","iopub.status.idle":"2024-12-30T21:16:50.542835Z","shell.execute_reply.started":"2024-12-30T21:16:50.525772Z","shell.execute_reply":"2024-12-30T21:16:50.542038Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Caricamento del modello\ncheckpoint = torch.load(checkpoint_path)\nmodel = checkpoint['model']\n\n\n# Creazione e avvio del valutatore\nevaluator = Evaluator(model=model, test_dataset=test_dataset, batch_size=64, num_workers=4, device=device)\nevaluator.evaluate()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T21:16:50.544512Z","iopub.execute_input":"2024-12-30T21:16:50.545188Z","iopub.status.idle":"2024-12-30T21:16:56.742455Z","shell.execute_reply.started":"2024-12-30T21:16:50.545162Z","shell.execute_reply":"2024-12-30T21:16:56.741238Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/2275293420.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path)\nEvaluating:   0%|          | 0/140 [00:00<?, ?it/s]/kaggle/input/ssd_checkpoint/pytorch/ssd/3/model.py:501: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n  image_boxes.append(class_decoded_locs[1 - suppress])\n/kaggle/input/ssd_checkpoint/pytorch/ssd/3/model.py:503: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n  image_scores.append(class_scores[1 - suppress])\nEvaluating:   0%|          | 0/140 [00:05<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Creazione e avvio del valutatore\u001b[39;00m\n\u001b[1;32m      7\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator(model\u001b[38;5;241m=\u001b[39mmodel, test_dataset\u001b[38;5;241m=\u001b[39mtest_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[47], line 59\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m boxes \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m boxes]\n\u001b[1;32m     58\u001b[0m labels \u001b[38;5;241m=\u001b[39m [l\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[0;32m---> 59\u001b[0m difficulties \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdifficulties\u001b[49m]\n\u001b[1;32m     61\u001b[0m det_boxes\u001b[38;5;241m.\u001b[39mextend(det_boxes_batch)\n\u001b[1;32m     62\u001b[0m det_labels\u001b[38;5;241m.\u001b[39mextend(det_labels_batch)\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'difficulties' referenced before assignment"],"ename":"UnboundLocalError","evalue":"local variable 'difficulties' referenced before assignment","output_type":"error"}],"execution_count":49}]}