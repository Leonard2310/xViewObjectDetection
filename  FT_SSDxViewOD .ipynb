{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10182328,"sourceType":"datasetVersion","datasetId":6242793}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import delle librerie","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport random\nimport xml.etree.ElementTree as ET\nimport torchvision.transforms.functional as FT\n\nimport torch\nfrom tqdm import tqdm\nfrom pprint import PrettyPrinter\nfrom torch import nn\nimport torchvision\n\nimport json\nimport random\nfrom collections import defaultdict\n\nimport sys\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms.functional as FT\n\nimport time\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\n\nimport random\nimport ast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T14:43:00.837720Z","iopub.execute_input":"2024-12-31T14:43:00.838959Z","iopub.status.idle":"2024-12-31T14:43:00.848777Z","shell.execute_reply.started":"2024-12-31T14:43:00.838886Z","shell.execute_reply":"2024-12-31T14:43:00.847601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Path","metadata":{}},{"cell_type":"code","source":"# path del dataset\nbase_dict = '/kaggle/input/our-xview-dataset'\n\n# path della cartella contenente le immagini\nimg_dict = '/kaggle/input/our-xview-dataset/images'\n\n# path di output\noutput_folder = '/kaggle/working/'\n\n# path contenente le annotazioni in formato .json\ncoco_json_path = os.path.join(base_dict, 'COCO_annotations_new.json') \nnew_coco_json_path = os.path.join(output_folder, 'mod_COCO_annotations.json') \n\n# path file per il training\ntrain_image = os.path.join(output_folder, 'TRAIN_images.json')\ntrain_bbox = os.path.join(output_folder, 'TRAIN_objects.json')\n\n# path file per la validation\nval_image = os.path.join(output_folder, 'VAL_images.json')\nval_bbox = os.path.join(output_folder, 'VAL_objects.json')\n\n# path file per il test\ntest_image = os.path.join(output_folder, 'TEST_images.json')\ntest_bbox = os.path.join(output_folder, 'TEST_objects.json')\n\ncheckpoint_path = '/kaggle/input/ssd_checkpoint/pytorch/ssd/3/checkpoint_ssd300.pth.tar'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T14:41:57.560744Z","iopub.execute_input":"2024-12-31T14:41:57.561182Z","iopub.status.idle":"2024-12-31T14:41:57.569070Z","shell.execute_reply.started":"2024-12-31T14:41:57.561144Z","shell.execute_reply":"2024-12-31T14:41:57.567634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T14:42:00.202463Z","iopub.execute_input":"2024-12-31T14:42:00.203394Z","iopub.status.idle":"2024-12-31T14:42:00.212923Z","shell.execute_reply.started":"2024-12-31T14:42:00.203287Z","shell.execute_reply":"2024-12-31T14:42:00.211419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utility","metadata":{}},{"cell_type":"code","source":"def load_json(file_path):\n    \"\"\"\n    Carica un file JSON dal percorso specificato.\n\n    :param file_path: Percorso al file JSON da caricare.\n    :return: Dati contenuti nel file JSON (come dizionario o lista).\n    \"\"\"\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_categories_from_custom_coco_json(json_path):\n    \"\"\"\n    Estrae i nomi delle categorie da un file JSON COCO personalizzato, ordinati per ID.\n\n    Args:\n        json_path (str): Path del file JSON COCO personalizzato.\n\n    Returns:\n        list: Lista di nomi delle categorie ordinate per ID.\n    \"\"\"\n    # Leggi il file JSON\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    # Ottieni le categorie dal JSON\n    raw_categories = data.get('categories', [])\n    \n    # Crea una lista di categorie con id e nome\n    categories = []\n    for category in raw_categories:\n        # Si assume che category sia un dizionario con \"id\" e \"name\"\n        if isinstance(category, dict) and 'id' in category and 'name' in category:\n            categories.append(category)\n        else:\n            print(f\"Errore nel formato della categoria: {category}\")\n    \n    # Ordina le categorie in base all'ID\n    categories = sorted(categories, key=lambda cat: cat[\"id\"])\n    \n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.insert(0, {\"id\": 0, \"name\": \"background\"})\n    \n    # Estrai solo i nomi delle categorie\n    category_names = [cat[\"name\"] for cat in categories]\n    \n    return category_names","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Dataset","metadata":{}},{"cell_type":"code","source":"def display_images_with_bboxes(json_file, specific_images, images_folder, mode = 2):\n    \"\"\"\n    Visualizza le immagini specificate con tutti i bounding box sopra di esse.\n\n    :param json_file: percorso del file JSON contenente le immagini, annotazioni e categorie\n    :param specific_images: lista di nomi delle immagini da visualizzare\n    :param images_folder: percorso della cartella che contiene le immagini\n    \"\"\"\n    # Carica il JSON\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    # Estrai le immagini e le annotazioni\n    images = data[\"images\"]\n    annotations = data[\"annotations\"]\n\n    # Crea un dizionario per mappare l'id delle immagini al nome del file\n    image_dict = {image[\"id\"]: image[\"file_name\"] for image in images}\n\n    # Filtra le annotazioni per le immagini specifiche\n    specific_annotations = [ann for ann in annotations if image_dict[ann[\"image_id\"]] in specific_images]\n\n    # Creiamo un dizionario per raccogliere tutte le annotazioni per ciascuna immagine\n    image_bboxes = {}\n    for annotation in specific_annotations:\n        image_name = image_dict[annotation[\"image_id\"]]\n        if image_name not in image_bboxes:\n            image_bboxes[image_name] = []\n        if mode == 1:\n            bbox = ast.literal_eval(annotation[\"bbox\"])\n        else:\n            bbox = annotation[\"bbox\"]\n        category_id = annotation[\"category_id\"]\n        image_bboxes[image_name].append((bbox, category_id))\n\n    # Visualizza tutte le immagini con tutti i bounding box\n    for image_name, bboxes in image_bboxes.items():\n        # Carica l'immagine\n        image_path = f'{images_folder}/{image_name}'  # Usa il percorso corretto per le immagini\n        image = Image.open(image_path)\n\n        # Crea la figura per la visualizzazione\n        plt.figure(figsize=(8, 8))\n        plt.imshow(image)\n\n        # Aggiungi tutti i bounding box e il category_id\n        if mode == 1:\n            for bbox, category_id in bboxes:\n                x, y, w, h = bbox\n                plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none'))\n                plt.text(x, y - 5, f'Category: {category_id}', color='red', fontsize=10, backgroundcolor='white')\n        else:\n            for bbox, category_id in bboxes:\n                x, y, x2, y2 = bbox\n                w = x2 - x\n                h = y2 - y\n                plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none'))\n                plt.text(x, y - 5, f'Category: {category_id}', color='red', fontsize=10, backgroundcolor='white')\n\n        # Imposta il titolo e disattiva gli assi\n        plt.title(f\"Image: {image_name}\")\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:08:52.076882Z","iopub.execute_input":"2024-12-31T15:08:52.077606Z","iopub.status.idle":"2024-12-31T15:08:52.095249Z","shell.execute_reply.started":"2024-12-31T15:08:52.077544Z","shell.execute_reply":"2024-12-31T15:08:52.093432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"specific_images = ['img_2020_1280_320.jpg','img_1473_1600_960.jpg', 'img_1088_1920_320.jpg', 'img_1141_640_2880.jpg']\n\ndisplay_images_with_bboxes(coco_json_path, specific_images, img_dict, mode = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:01:39.868588Z","iopub.execute_input":"2024-12-31T15:01:39.869410Z","iopub.status.idle":"2024-12-31T15:01:43.664012Z","shell.execute_reply.started":"2024-12-31T15:01:39.869369Z","shell.execute_reply":"2024-12-31T15:01:43.662779Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# COCO Preprocessing","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    data = load_json(input_path)\n\n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n \n    for category in tqdm(raw_categories, desc=\"Processing Categories\"):\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n \n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n \n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n \n    # Preprocessa le annotazioni in un dizionario per immagini\n    image_annotations_dict = {}\n    for annotation in tqdm(data.get('annotations', []), desc=\"Building Image Annotations Dictionary\"):\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_dict:\n            image_annotations_dict[image_id] = []\n        image_annotations_dict[image_id].append(annotation)\n \n    # Elenco di annotazioni da mantenere (solo quelle valide)\n    valid_annotations = []\n    annotations_to_remove = set()\n \n    # Controllo dei bounding box\n    for annotation in tqdm(data.get('annotations', []), desc=\"Processing Annotations\"):\n        if annotation['category_id'] == 0:  # Se è Aircraft\n            annotation['category_id'] = 11\n        \n        # Converte il formato del bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n        \n        x, y, width, height = annotation['bbox']\n        xmin, xmax = x, x + width\n        ymin, ymax = y, y + height\n        \n        # Verifica che xmin < xmax e ymin < ymax, e che la larghezza e altezza siano sufficienti\n        if xmin >= xmax or ymin >= ymax or width <= 10 or height <= 10:\n            annotations_to_remove.add(annotation['id'])\n        else:\n            annotation['bbox'] = [xmin, ymin, xmax, ymax]\n            valid_annotations.append(annotation)\n \n    # Rimuovi le annotazioni non valide\n    data['annotations'] = valid_annotations\n \n    # Verifica se ci sono immagini senza annotazioni (usando il dizionario delle annotazioni)\n    new_annotations = []\n    for image in tqdm(data.get('images', []), desc=\"Processing Images\"):\n        if image['id'] not in image_annotations_dict:  # Se l'immagine non ha annotazioni\n            # Aggiungi la categoria \"background\"\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, 0.0, image['width'], image['height']],  # Background con bbox che copre tutta l'immagine\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n \n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n \n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n \n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T14:45:09.978622Z","iopub.execute_input":"2024-12-31T14:45:09.979039Z","iopub.status.idle":"2024-12-31T14:45:09.995406Z","shell.execute_reply.started":"2024-12-31T14:45:09.979003Z","shell.execute_reply":"2024-12-31T14:45:09.993926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_custom_coco_json(coco_json_path, new_coco_json_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T14:45:16.967672Z","iopub.execute_input":"2024-12-31T14:45:16.968960Z","iopub.status.idle":"2024-12-31T14:45:32.279461Z","shell.execute_reply.started":"2024-12-31T14:45:16.968909Z","shell.execute_reply":"2024-12-31T14:45:32.278160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Estrai i nomi delle categorie\ncategories = extract_categories_from_custom_coco_json(new_coco_json_pth)\n# Mostra i nomi delle categorie\nprint(categories)\n\n# Creazione del label map e reverse label map\nlabel_map = {idx: category for idx, category in enumerate(categories)}\nrev_label_map = {category: idx for idx, category in enumerate(categories)}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"specific_images = ['img_2020_1280_320.jpg','img_1473_1600_960.jpg', 'img_2122_640_0.jpg', 'img_1141_640_2880.jpg']\n\ndisplay_images_with_bboxes(new_coco_json_path, specific_images, img_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T15:09:06.550109Z","iopub.execute_input":"2024-12-31T15:09:06.550597Z","iopub.status.idle":"2024-12-31T15:09:12.377376Z","shell.execute_reply.started":"2024-12-31T15:09:06.550558Z","shell.execute_reply":"2024-12-31T15:09:12.375871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"def split_coco_and_check(coco_file, output_path, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1): #modifica in modo da fare lo split solo train e test\n    # Carica il file COCO\n    with open(coco_file, 'r') as f:\n        coco_data = json.load(f)\n\n    images = coco_data['images']\n    annotations = coco_data['annotations']\n    categories = coco_data['categories']\n\n    total_images = len(images)\n    total_annotations = len(annotations)\n\n    # Shuffle delle immagini per garantire casualità\n    random.shuffle(images)\n\n    # Calcola i numeri di immagini per train, val e test\n    train_count = int(train_ratio * total_images)\n    val_count = int(val_ratio * total_images)\n    test_count = total_images - train_count - val_count\n\n    train_images = images[:train_count]\n    val_images = images[train_count:train_count + val_count]\n    test_images = images[train_count + val_count:]\n\n    # Crea set di ID immagini\n    train_ids = {img['id'] for img in train_images}\n    val_ids = {img['id'] for img in val_images}\n    test_ids = {img['id'] for img in test_images}\n\n    # Divide le annotazioni\n    train_annotations = [ann for ann in annotations if ann['image_id'] in train_ids]\n    val_annotations = [ann for ann in annotations if ann['image_id'] in val_ids]\n    test_annotations = [ann for ann in annotations if ann['image_id'] in test_ids]\n\n    train_bbox_count = len(train_annotations)\n    val_bbox_count = len(val_annotations)\n    test_bbox_count = len(test_annotations)\n\n    # Salva i file di output\n    def save_split(file_name, images_split):\n        with open(file_name, 'w') as f:\n            for img in images_split:\n                f.write(f\"{output_path}/{img['file_name']}\\n\")\n\n    save_split('train.txt', train_images)\n    save_split('val.txt', val_images)\n    save_split('test.txt', test_images)\n\n    # Controlla le proporzioni\n    check_split_proportions(\n        total_images, total_annotations,\n        len(train_images), len(val_images), len(test_images),\n        train_bbox_count, val_bbox_count, test_bbox_count,\n        train_ratio, val_ratio, test_ratio,\n        train_annotations, val_annotations, test_annotations,\n        categories\n    )\n\ndef check_split_proportions(total_images, total_annotations, train_count, val_count, test_count, \n                            train_bbox_count, val_bbox_count, test_bbox_count, \n                            train_ratio, val_ratio, test_ratio, \n                            train_annotations, val_annotations, test_annotations, categories):\n    # Percentuali per immagini\n    train_image_percentage = (train_count / total_images) * 100\n    val_image_percentage = (val_count / total_images) * 100\n    test_image_percentage = (test_count / total_images) * 100\n\n    # Percentuali per bbox\n    train_bbox_percentage = (train_bbox_count / total_annotations) * 100\n    val_bbox_percentage = (val_bbox_count / total_annotations) * 100\n    test_bbox_percentage = (test_bbox_count / total_annotations) * 100\n\n    print(f\"Totale immagini: {total_images}\")\n    print(f\"Totale annotazioni (bbox): {total_annotations}\")\n    print(f\"Train: {train_count} immagini ({train_image_percentage:.2f}%) ({train_bbox_count} bbox) ({train_bbox_percentage:.2f}%)\")\n    print(f\"Val: {val_count} immagini ({val_image_percentage:.2f}%) ({val_bbox_count} bbox) ({val_bbox_percentage:.2f}%)\")\n    print(f\"Test: {test_count} immagini ({test_image_percentage:.2f}%) ({test_bbox_count} bbox) ({test_bbox_percentage:.2f}%)\")\n\n    # Calcola il numero di annotazioni per categoria nei vari set\n    category_count_train = defaultdict(int)\n    category_count_val = defaultdict(int)\n    category_count_test = defaultdict(int)\n\n    for annotation in train_annotations:\n        category_count_train[annotation['category_id']] += 1\n    for annotation in val_annotations:\n        category_count_val[annotation['category_id']] += 1\n    for annotation in test_annotations:\n        category_count_test[annotation['category_id']] += 1\n\n    # Stampa le proporzioni per categoria\n    print(\"\\nProporzioni per categoria:\")\n    for category in categories:\n        category_id = category['id']\n        category_name = category['name']\n\n        # Conta il numero di annotazioni per categoria in ogni set\n        train_cat_count = category_count_train.get(category_id, 0)\n        val_cat_count = category_count_val.get(category_id, 0)\n        test_cat_count = category_count_test.get(category_id, 0)\n\n        # Calcola la percentuale di annotazioni per categoria\n        total_cat_annotations = train_cat_count + val_cat_count + test_cat_count\n        if total_cat_annotations > 0:\n            train_cat_percentage = (train_cat_count / total_cat_annotations) * 100\n            val_cat_percentage = (val_cat_count / total_cat_annotations) * 100\n            test_cat_percentage = (test_cat_count / total_cat_annotations) * 100\n        else:\n            train_cat_percentage = val_cat_percentage = test_cat_percentage = 0.0\n\n        print(f\"{category_name}:\")\n        print(f\"  Train: {train_cat_count} annotazioni ({train_cat_percentage:.2f}%)\")\n        print(f\"  Val: {val_cat_count} annotazioni ({val_cat_percentage:.2f}%)\")\n        print(f\"  Test: {test_cat_count} annotazioni ({test_cat_percentage:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T13:39:59.686159Z","iopub.execute_input":"2024-12-31T13:39:59.687279Z","iopub.status.idle":"2024-12-31T13:39:59.702483Z","shell.execute_reply.started":"2024-12-31T13:39:59.687226Z","shell.execute_reply":"2024-12-31T13:39:59.701478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_coco_and_check(new_coco_json_path, img_dict, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T13:40:05.888607Z","iopub.execute_input":"2024-12-31T13:40:05.888960Z","iopub.status.idle":"2024-12-31T13:40:08.910962Z","shell.execute_reply.started":"2024-12-31T13:40:05.888932Z","shell.execute_reply":"2024-12-31T13:40:08.910081Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"def parse_coco_annotation(annotation_data):\n    \"\"\"\n    Converti le annotazioni COCO in una struttura utile.\n    \"\"\"\n    boxes = []\n    labels = []\n\n    for ann in annotation_data:\n        category_id = ann['category_id']  # Usa direttamente il category_id come etichetta\n        bbox = ann['bbox']  # I bounding box sono già nel formato [xmin, ymin, xmax, ymax]\n        xmin, ymin, xmax, ymax = bbox\n\n        boxes.append([xmin, ymin, xmax, ymax])\n        labels.append(category_id)\n\n    return {'boxes': boxes, 'labels': labels}\n\ndef create_coco_data_lists(coco_file, splits_path, output_folder):\n    \"\"\"\n    Converte i dati COCO e split in liste per train, val e test.\n    \"\"\"\n    with open(coco_file, 'r') as f:\n        coco_data = json.load(f)\n\n    # Prepara mappature da immagini e annotazioni\n    images = {img['file_name']: img for img in coco_data['images']}  # Mappa file_name -> immagine\n    annotations_by_image = defaultdict(list)\n    for ann in coco_data['annotations']:\n        annotations_by_image[ann['image_id']].append(ann)\n\n    # Genera i dati per ciascuno split\n    for split in ['train', 'val', 'test']:\n        split_file = os.path.join(splits_path, f\"{split}.txt\")\n        with open(split_file, 'r') as f:\n            image_files = [line.strip() for line in f.readlines()]\n\n        image_list = []\n        objects_list = []\n\n        for image_file in image_files:\n            file_name = os.path.basename(image_file)  # Ottieni solo il nome del file\n            if file_name not in images:\n                continue\n\n            image_info = images[file_name]\n            image_id = image_info['id']\n            annotations = annotations_by_image[image_id]\n            objects = parse_coco_annotation(annotations)\n\n            if not objects['boxes']:\n                continue\n\n            image_list.append(image_file)\n            objects_list.append(objects)\n\n        # Salva i risultati\n        with open(os.path.join(output_folder, f\"{split.upper()}_images.json\"), 'w') as j:\n            json.dump(image_list, j)\n        with open(os.path.join(output_folder, f\"{split.upper()}_objects.json\"), 'w') as j:\n            json.dump(objects_list, j)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T13:40:13.764179Z","iopub.execute_input":"2024-12-31T13:40:13.764890Z","iopub.status.idle":"2024-12-31T13:40:13.774605Z","shell.execute_reply.started":"2024-12-31T13:40:13.764853Z","shell.execute_reply":"2024-12-31T13:40:13.773585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"create_coco_data_lists(new_coco_json_path, output_folder, output_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T13:40:20.922566Z","iopub.execute_input":"2024-12-31T13:40:20.922959Z","iopub.status.idle":"2024-12-31T13:40:27.697717Z","shell.execute_reply.started":"2024-12-31T13:40:20.922925Z","shell.execute_reply":"2024-12-31T13:40:27.696707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset class to be used in a DataLoader for batching.\n    \"\"\"\n\n    def __init__(self, path_image, path_bbox):\n        \"\"\"\n        :param path_image: Path to the JSON file containing image paths.\n        :param path_bbox: Path to the JSON file containing bounding boxes and labels.\n        \"\"\"\n        # Load data\n        with open(path_image, 'r') as j:\n            self.images = json.load(j)\n        with open(path_bbox, 'r') as j:\n            self.objects = json.load(j)\n\n        # Ensure the lengths match\n        assert len(self.images) == len(self.objects), \"Images and annotations must have the same length.\"\n\n    def __transform(self, image, boxes, labels):\n        \"\"\"\n        Apply transformations to the image, boxes, and labels.\n        :param image: A PIL Image.\n        :param boxes: Bounding boxes as a tensor of dimensions (n_objects, 4).\n        :param labels: Labels as a tensor of dimensions (n_objects).\n        :return: Transformed image, boxes, and labels.\n        \"\"\"\n        def resize(image, boxes, dims=(300, 300)):\n            # Resize image\n            new_image = FT.resize(image, dims)\n\n            # Normalize bounding boxes\n            old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n            new_boxes = boxes / old_dims  # Percent coordinates\n            new_dims = torch.FloatTensor([dims[0], dims[1], dims[0], dims[1]]).unsqueeze(0)\n            new_boxes = new_boxes * new_dims\n\n            return new_image, new_boxes\n\n        # ImageNet normalization values\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n\n        # Resize image and normalize boxes\n        image, boxes = resize(image, boxes)\n\n        # Convert image to tensor\n        image = FT.to_tensor(image)\n\n        # Normalize image\n        image = FT.normalize(image, mean=mean, std=std)\n\n        return image, boxes, labels\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieve an image and its corresponding objects.\n        :param idx: Index of the data point.\n        :return: Transformed image, bounding boxes, and labels.\n        \"\"\"\n        # Load image\n        image = Image.open(self.images[idx]).convert('RGB')\n\n        # Load objects\n        objects = self.objects[idx]\n        boxes = torch.FloatTensor(objects['boxes'])  # (n_objects, 4)\n        labels = torch.LongTensor(objects['labels'])  # (n_objects)\n\n        # Apply transformations\n        image, boxes, labels = self.__transform(image, boxes, labels)\n\n        return image, boxes, labels\n\n    def __len__(self):\n        \"\"\"\n        Total number of data points.\n        :return: Length of the dataset.\n        \"\"\"\n        return len(self.images)\n\n    def collate_fn(self, batch):\n        \"\"\"\n        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n\n        This describes how to combine these tensors of different sizes. We use lists.\n\n        :param batch: an iterable of N sets from __getitem__()\n        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels.\n        \"\"\"\n\n        images = list()\n        boxes = list()\n        labels = list()\n\n        for b in batch:\n            images.append(b[0])\n            boxes.append(b[1])\n            labels.append(b[2])\n\n        images = torch.stack(images, dim=0)\n\n        return images, boxes, labels  # tensor (N, 3, 300, 300), 2 lists of N tensors each\n\n    def visualize_samples(self, indices, label_map=None):\n        \"\"\"\n        Visualize multiple images with bounding boxes and labels before and after transformation.\n    \n        :param indices: List of indices of the data points to visualize.\n        :param label_map: Optional dictionary to map label indices to class names.\n        \"\"\"\n        num_samples = len(indices)\n        fig, axes = plt.subplots(num_samples, 2, figsize=(15, 7 * num_samples))\n    \n        if num_samples == 1:\n            axes = [axes]  # Ensure axes is iterable when there's only one sample\n    \n        for i, idx in enumerate(indices):\n            # Load image and objects\n            original_image = Image.open(self.images[idx]).convert('RGB')\n            objects = self.objects[idx]\n            boxes = torch.FloatTensor(objects['boxes'])  # (n_objects, 4)\n            labels = torch.LongTensor(objects['labels'])  # (n_objects)\n    \n            # Plot original image with bounding boxes\n            axes[i][0].imshow(original_image)\n            axes[i][0].set_title(f\"Original Image {idx}\")\n            for box, label in zip(boxes, labels):\n                xmin, ymin, xmax, ymax = box\n                rect = patches.Rectangle(\n                    (xmin, ymin),\n                    xmax - xmin,\n                    ymax - ymin,\n                    linewidth=2,\n                    edgecolor='r',\n                    facecolor='none'\n                )\n                axes[i][0].add_patch(rect)\n                label_name = label_map[label.item()] if label_map else str(label.item())\n                axes[i][0].text(xmin, ymin, label_name, color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n    \n            # Apply transformations\n            transformed_image, transformed_boxes, transformed_labels = self.__transform(original_image, boxes, labels)\n            transformed_image = transformed_image.permute(1, 2, 0).numpy()  # Convert to (H, W, C) for plotting\n    \n            # Denormalize image for visualization\n            mean = [0.485, 0.456, 0.406]\n            std = [0.229, 0.224, 0.225]\n            transformed_image = std * transformed_image + mean  # Reverse normalization\n            transformed_image = (transformed_image * 255).astype(np.uint8)\n    \n            # Plot transformed image with bounding boxes\n            axes[i][1].imshow(transformed_image)\n            axes[i][1].set_title(f\"Transformed Image {idx}\")\n            for box, label in zip(transformed_boxes, transformed_labels):\n                xmin, ymin, xmax, ymax = box\n                rect = patches.Rectangle(\n                    (xmin, ymin),\n                    xmax - xmin,\n                    ymax - ymin,\n                    linewidth=2,\n                    edgecolor='g',\n                    facecolor='none'\n                )\n                axes[i][1].add_patch(rect)\n                label_name = label_map[label.item()] if label_map else str(label.item())\n                axes[i][1].text(xmin, ymin, label_name, color='white', fontsize=12, bbox=dict(facecolor='green', alpha=0.5))\n    \n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T14:08:10.120875Z","iopub.execute_input":"2024-12-31T14:08:10.121234Z","iopub.status.idle":"2024-12-31T14:08:10.140523Z","shell.execute_reply.started":"2024-12-31T14:08:10.121200Z","shell.execute_reply":"2024-12-31T14:08:10.139384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = CustomDataset(train_image, train_bbox)\nval_dataset = CustomDataset(val_image, val_bbox)\ntest_dataset = CustomDataset(test_image, test_bbox)\n\n# Recupera un campione\nimage, boxes, labels = train_dataset[0]\n\n# Dimensioni\nprint(\"Image shape:\", image.shape)  # Torch tensor di dimensione (3, 300, 300)\nprint(\"Boxes:\", boxes)             # Bounding box normalizzati\nprint(\"Labels:\", labels)           # Etichette","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T14:08:10.142239Z","iopub.execute_input":"2024-12-31T14:08:10.142595Z","iopub.status.idle":"2024-12-31T14:08:11.402690Z","shell.execute_reply.started":"2024-12-31T14:08:10.142567Z","shell.execute_reply":"2024-12-31T14:08:11.401722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"indices = [0, 1, 2, 3]  # Sostituisci con gli indici desiderati\ntrain_dataset.visualize_samples(indices, label_map=label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T14:08:11.403821Z","iopub.execute_input":"2024-12-31T14:08:11.404133Z","iopub.status.idle":"2024-12-31T14:08:15.523096Z","shell.execute_reply.started":"2024-12-31T14:08:11.404105Z","shell.execute_reply":"2024-12-31T14:08:15.521806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, \n                                               collate_fn=train_dataset.collate_fn, num_workers=3, pin_memory=True)\nval_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, \n                                             collate_fn=train_dataset.collate_fn, num_workers=3, pin_memory=True)\ntest_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, \n                                              collate_fn=train_dataset.collate_fn, num_workers=3, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T13:37:58.958645Z","iopub.status.idle":"2024-12-31T13:37:58.958992Z","shell.execute_reply.started":"2024-12-31T13:37:58.958830Z","shell.execute_reply":"2024-12-31T13:37:58.958848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"fare l'import da GitHub https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py","metadata":{}},{"cell_type":"code","source":"# Aggiungi la directory che contiene 'model.py' alla variabile sys.path\nsys.path.append('/kaggle/input/ssd_checkpoint/pytorch/ssd/3')\n\n# Ora importa il modulo 'model' che si trova nella stessa cartella\nfrom model import *  \n\n# Carica il checkpoint\ncheckpoint = torch.load(checkpoint_path, map_location='cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:04.370789Z","iopub.execute_input":"2024-12-30T19:38:04.371150Z","iopub.status.idle":"2024-12-30T19:38:05.800187Z","shell.execute_reply.started":"2024-12-30T19:38:04.371110Z","shell.execute_reply":"2024-12-30T19:38:05.799431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verifica le chiavi nel checkpoint (opzionale)\nprint(checkpoint.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:05.801090Z","iopub.execute_input":"2024-12-30T19:38:05.801318Z","iopub.status.idle":"2024-12-30T19:38:05.805875Z","shell.execute_reply.started":"2024-12-30T19:38:05.801295Z","shell.execute_reply":"2024-12-30T19:38:05.805014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Crea un'istanza del modello\nmodel = SSD300(21)  # Assicurati che la definizione del modello sia corretta\n\n# Carica i pesi nel modello\n#model.load_state_dict(checkpoint['model'].state_dict())\n\n# Carica i pesi pre-addestrati per il backbone e altre componenti comuni\npretrained_dict = checkpoint['model'].state_dict()\nmodel_dict = model.state_dict()\n\n# Mantieni solo i pesi che corrispondono tra il modello pre-addestrato e il nuovo\npretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\nmodel_dict.update(pretrained_dict)\nmodel.load_state_dict(model_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:05.806934Z","iopub.execute_input":"2024-12-30T19:38:05.807217Z","iopub.status.idle":"2024-12-30T19:38:10.719359Z","shell.execute_reply.started":"2024-12-30T19:38:05.807175Z","shell.execute_reply":"2024-12-30T19:38:10.718409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def find_intersection(set_1, set_2):\n    \"\"\"\n    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # PyTorch auto-broadcasts singleton dimensions\n    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n\n\ndef find_jaccard_overlap(set_1, set_2):\n    \"\"\"\n    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # Find intersections\n    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n\n    # Find areas of each box in both sets\n    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n\n    # Find the union\n    # PyTorch auto-broadcasts singleton dimensions\n    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n\n    return intersection / union  # (n1, n2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:10.720503Z","iopub.execute_input":"2024-12-30T19:38:10.720875Z","iopub.status.idle":"2024-12-30T19:38:10.727540Z","shell.execute_reply.started":"2024-12-30T19:38:10.720847Z","shell.execute_reply":"2024-12-30T19:38:10.726673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def adjust_learning_rate(optimizer, scale):\n    \"\"\"\n    Scale learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param scale: factor to multiply learning rate with.\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * scale\n    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))\n\n\ndef accuracy(scores, targets, k):\n    \"\"\"\n    Computes top-k accuracy, from predicted and true labels.\n\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    \"\"\"\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 / batch_size)\n\n\ndef save_checkpoint(epoch, model, optimizer):\n    \"\"\"\n    Save model checkpoint.\n\n    :param epoch: epoch number\n    :param model: model\n    :param optimizer: optimizer\n    \"\"\"\n    state = {'epoch': epoch,\n             'model': model,\n             'optimizer': optimizer}\n    filename = 'checkpoint_ssd300.pth.tar'\n    torch.save(state, filename)\n\ndef clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{}},{"cell_type":"markdown","source":"class Trainer:\n    def __init__(self, model, train_dataset, train_dataloader, criterion, optimizer, batch_size, num_workers, device, \n                 grad_clip=None, print_freq=10, iterations=120000, decay_lr_at=None, decay_lr_to=0.1, \n                 momentum=0.9, weight_decay=5e-4):\n        \"\"\"\n        Initialize the Trainer.\n        \n        :param model: SSD300 model instance\n        :param train_dataset: Dataset object\n        :param criterion: Loss function\n        :param optimizer: Optimizer\n        :param batch_size: Training batch size\n        :param num_workers: Number of data loading workers\n        :param device: Device to use for training ('cuda' or 'cpu')\n        :param grad_clip: Gradient clipping value (default: None)\n        :param print_freq: Frequency of printing training progress\n        :param iterations: Total number of training iterations\n        :param decay_lr_at: Iterations to decay learning rate\n        :param decay_lr_to: Learning rate decay factor\n        :param momentum: Momentum for optimizer\n        :param weight_decay: Weight decay for optimizer\n        \"\"\"\n        self.model = model\n        self.train_dataset = train_dataset\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n        self.grad_clip = grad_clip\n        self.print_freq = print_freq\n        self.iterations = iterations\n        self.decay_lr_at = decay_lr_at if decay_lr_at is not None else [80000, 100000]\n        self.decay_lr_to = decay_lr_to\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n\n        # Prepare dataloader\n        self.train_loader = train_dataloader\n\n        # Calculate epochs and decay epochs\n        self.epochs = iterations // (len(train_dataset) // 32)\n        self.decay_epochs = [it // (len(train_dataset) // 32) for it in self.decay_lr_at]\n\n    def adjust_learning_rate(self, epoch):\n        \"\"\"\n        Adjust the learning rate at specific epochs.\n        \"\"\"\n        if epoch in self.decay_epochs:\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = param_group['lr'] * self.decay_lr_to\n            print(f\"Learning rate adjusted to {param_group['lr']} at epoch {epoch}\")\n\n    def train_one_epoch(self, epoch):\n        \"\"\"\n        Perform one epoch of training.\n        \"\"\"\n        self.model.train()\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n\n        start = time.time()\n\n        for i, (images, boxes, labels) in enumerate(self.train_loader):\n            data_time.update(time.time() - start)\n\n            # Move to device\n            images = images.to(self.device)\n            boxes = [b.to(self.device) for b in boxes]\n            labels = [l.to(self.device) for l in labels]\n\n            # Forward pass\n            predicted_locs, predicted_scores = self.model(images)\n\n            # Compute loss\n            loss = self.criterion(predicted_locs, predicted_scores, boxes, labels)\n\n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n\n            # Gradient clipping\n            if self.grad_clip is not None:\n                clip_gradient(self.optimizer, self.grad_clip)\n\n            # Update model parameters\n            self.optimizer.step()\n\n            # Update metrics\n            losses.update(loss.item(), images.size(0))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            # Print status\n            if i % self.print_freq == 0:\n                print('Epoch: [{0}][{1}/{2}]\\t'\n                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(self.train_loader),\n                                                                      batch_time=batch_time,\n                                                                      data_time=data_time,\n                                                                      loss=losses))\n\n        del predicted_locs, predicted_scores, images, boxes, labels\n\n    def save_checkpoint(self, epoch):\n        \"\"\"\n        Save model checkpoint.\n        \"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model': self.model,\n            'optimizer': self.optimizer,\n        }, f'checkpoint_epoch_{epoch}.pth')\n        print(f\"Checkpoint saved for epoch {epoch}.\")\n\n    def train(self, start_epoch=0):\n        \"\"\"\n        Train the model across all epochs.\n        \"\"\"\n        for epoch in range(start_epoch, self.epochs):\n            self.adjust_learning_rate(epoch)\n            self.train_one_epoch(epoch)\n            self.save_checkpoint(epoch)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-30T19:43:19.266925Z","iopub.execute_input":"2024-12-30T19:43:19.267364Z","iopub.status.idle":"2024-12-30T19:43:19.281590Z","shell.execute_reply.started":"2024-12-30T19:43:19.267330Z","shell.execute_reply":"2024-12-30T19:43:19.280644Z"}}},{"cell_type":"code","source":"from tqdm import tqdm\n\nclass Trainer:\n    def __init__(self, model, train_dataset, train_dataloader, criterion, optimizer, batch_size, num_workers, device, \n                 grad_clip=None, print_freq=10, iterations=120000, decay_lr_at=None, decay_lr_to=0.1, \n                 momentum=0.9, weight_decay=5e-4):\n        \"\"\"\n        Initialize the Trainer.\n        \n        :param model: SSD300 model instance\n        :param train_dataset: Dataset object\n        :param criterion: Loss function\n        :param optimizer: Optimizer\n        :param batch_size: Training batch size\n        :param num_workers: Number of data loading workers\n        :param device: Device to use for training ('cuda' or 'cpu')\n        :param grad_clip: Gradient clipping value (default: None)\n        :param print_freq: Frequency of printing training progress\n        :param iterations: Total number of training iterations\n        :param decay_lr_at: Iterations to decay learning rate\n        :param decay_lr_to: Learning rate decay factor\n        :param momentum: Momentum for optimizer\n        :param weight_decay: Weight decay for optimizer\n        \"\"\"\n        self.model = model\n        self.train_dataset = train_dataset\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n        self.grad_clip = grad_clip\n        self.print_freq = print_freq\n        self.iterations = iterations\n        self.decay_lr_at = decay_lr_at if decay_lr_at is not None else [80000, 100000]\n        self.decay_lr_to = decay_lr_to\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n\n        # Prepare dataloader\n        self.train_loader = train_dataloader\n\n        # Calculate epochs and decay epochs\n        #self.epochs = iterations // (len(train_dataset) // 32)\n        self.epochs = 10\n        self.decay_epochs = [it // (len(train_dataset) // 32) for it in self.decay_lr_at]\n\n    def adjust_learning_rate(self, epoch):\n        \"\"\"\n        Adjust the learning rate at specific epochs.\n        \"\"\"\n        if epoch in self.decay_epochs:\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = param_group['lr'] * self.decay_lr_to\n            print(f\"Learning rate adjusted to {param_group['lr']} at epoch {epoch}\")\n\n    def train_one_epoch(self, epoch):\n        \"\"\"\n        Perform one epoch of training.\n        \"\"\"\n        self.model.train()\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n\n        start = time.time()\n\n        with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch}/{self.epochs}\", unit=\"batch\") as pbar:\n            for i, (images, boxes, labels) in enumerate(self.train_loader):\n                data_time.update(time.time() - start)\n\n                # Move to device\n                images = images.to(self.device)\n                boxes = [b.to(self.device) for b in boxes]\n                labels = [l.to(self.device) for l in labels]\n\n                # Forward pass\n                predicted_locs, predicted_scores = self.model(images)\n\n                # Compute loss\n                loss = self.criterion(predicted_locs, predicted_scores, boxes, labels)\n\n                # Backward pass\n                self.optimizer.zero_grad()\n                loss.backward()\n\n                # Gradient clipping\n                if self.grad_clip is not None:\n                    clip_gradient(self.optimizer, self.grad_clip)\n\n                # Update model parameters\n                self.optimizer.step()\n\n                # Update metrics\n                losses.update(loss.item(), images.size(0))\n                batch_time.update(time.time() - start)\n\n                # Update progress bar\n                pbar.set_postfix({\n                    \"Batch Time\": f\"{batch_time.val:.3f}s\",\n                    \"Data Time\": f\"{data_time.val:.3f}s\",\n                    \"Loss\": f\"{losses.val:.4f} (avg: {losses.avg:.4f})\"\n                })\n                pbar.update(1)\n\n                start = time.time()\n\n        del predicted_locs, predicted_scores, images, boxes, labels\n\n    def save_checkpoint(self, epoch):\n        \"\"\"\n        Save model checkpoint.\n        \"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model': self.model,\n            'optimizer': self.optimizer,\n        }, f'checkpoint_epoch_{epoch}.pth')\n        print(f\"Checkpoint saved for epoch {epoch}.\")\n\n    def train(self, start_epoch=0):\n        \"\"\"\n        Train the model across all epochs.\n        \"\"\"\n        for epoch in range(start_epoch, self.epochs):\n            self.adjust_learning_rate(epoch)\n            self.train_one_epoch(epoch)\n            self.save_checkpoint(epoch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:01.022420Z","iopub.execute_input":"2024-12-30T19:56:01.022971Z","iopub.status.idle":"2024-12-30T19:56:01.038319Z","shell.execute_reply.started":"2024-12-30T19:56:01.022935Z","shell.execute_reply":"2024-12-30T19:56:01.037548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiBoxLoss(nn.Module):\n    \"\"\"\n    The MultiBox loss, a loss function for object detection.\n\n    This is a combination of:\n    (1) a localization loss for the predicted locations of the boxes, and\n    (2) a confidence loss for the predicted class scores.\n    \"\"\"\n\n    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n        super(MultiBoxLoss, self).__init__()\n        self.priors_cxcy = priors_cxcy\n        self.priors_xy = cxcy_to_xy(priors_cxcy)\n        self.threshold = threshold\n        self.neg_pos_ratio = neg_pos_ratio\n        self.alpha = alpha\n\n        self.smooth_l1 = nn.L1Loss()  # *smooth* L1 loss in the paper; see Remarks section in the tutorial\n        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n\n    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n        \"\"\"\n        Forward propagation.\n\n        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n        :param boxes: true  object bounding boxes in boundary coordinates, a list of N tensors\n        :param labels: true object labels, a list of N tensors\n        :return: multibox loss, a scalar\n        \"\"\"\n        batch_size = predicted_locs.size(0)\n        n_priors = self.priors_cxcy.size(0)\n        n_classes = predicted_scores.size(2)\n\n        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n\n        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)  # (N, 8732)\n\n        # For each image\n        for i in range(batch_size):\n            n_objects = boxes[i].size(0)\n\n            overlap = find_jaccard_overlap(boxes[i],\n                                           self.priors_xy)  # (n_objects, 8732)\n\n            # For each prior, find the object that has the maximum overlap\n            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)  # (8732)\n\n            # We don't want a situation where an object is not represented in our positive (non-background) priors -\n            # 1. An object might not be the best object for all priors, and is therefore not in object_for_each_prior.\n            # 2. All priors with the object may be assigned as background based on the threshold (0.5).\n\n            # To remedy this -\n            # First, find the prior that has the maximum overlap for each object.\n            _, prior_for_each_object = overlap.max(dim=1)  # (N_o)\n\n            # Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)\n            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n\n            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n            overlap_for_each_prior[prior_for_each_object] = 1.\n\n            # Labels for each prior\n            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n\n            # Store\n            true_classes[i] = label_for_each_prior\n\n            # Encode center-size object coordinates into the form we regressed predicted boxes to\n            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n\n        # Identify priors that are positive (object/non-background)\n        positive_priors = true_classes != 0  # (N, 8732)\n\n        # LOCALIZATION LOSS\n\n        # Localization loss is computed only over positive (non-background) priors\n        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n\n        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n\n        # CONFIDENCE LOSS\n\n        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n        # That is, FOR EACH IMAGE,\n        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n\n        # Number of positive and hard-negative priors per image\n        n_positives = positive_priors.sum(dim=1)  # (N)\n        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n\n        # First, find the loss for all priors\n        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n\n        # We already know which priors are positive\n        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n\n        # Next, find which priors are hard-negative\n        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n\n        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n\n        # TOTAL LOSS\n\n        return conf_loss + self.alpha * loc_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:01.039804Z","iopub.execute_input":"2024-12-30T19:56:01.040055Z","iopub.status.idle":"2024-12-30T19:56:01.061782Z","shell.execute_reply.started":"2024-12-30T19:56:01.040031Z","shell.execute_reply":"2024-12-30T19:56:01.061197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:01.126053Z","iopub.execute_input":"2024-12-30T19:56:01.126316Z","iopub.status.idle":"2024-12-30T19:56:01.130596Z","shell.execute_reply.started":"2024-12-30T19:56:01.126291Z","shell.execute_reply":"2024-12-30T19:56:01.129728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model parameters\nn_classes = 12  # number of different types of objects\n\n# Learning parameters\ncheckpoint = None  # path to model checkpoint, None if none\nbatch_size = 8  # batch size\niterations = 10  # number of iterations to train\nworkers = 4  # number of workers for loading data in the DataLoader\nprint_freq = 200  # print training status every __ batches\nlr = 1e-3  # learning rate\ndecay_lr_at = [80000, 100000]  # decay learning rate after these many iterations\ndecay_lr_to = 0.1  # decay learning rate to this fraction of the existing learning rate\nmomentum = 0.9  # momentum\nweight_decay = 5e-4  # weight decay\ngrad_clip = 0.8  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\nmodel = SSD300(n_classes=n_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:01.132029Z","iopub.execute_input":"2024-12-30T19:56:01.132249Z","iopub.status.idle":"2024-12-30T19:56:03.172062Z","shell.execute_reply.started":"2024-12-30T19:56:01.132227Z","shell.execute_reply":"2024-12-30T19:56:03.171308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n\n# Ottimizzatore\nbiases = [param for name, param in model.named_parameters() if param.requires_grad and name.endswith('.bias')]\nnot_biases = [param for name, param in model.named_parameters() if param.requires_grad and not name.endswith('.bias')]\noptimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n                            lr=lr, momentum=momentum, weight_decay=weight_decay)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:03.173214Z","iopub.execute_input":"2024-12-30T19:56:03.173567Z","iopub.status.idle":"2024-12-30T19:56:03.180851Z","shell.execute_reply.started":"2024-12-30T19:56:03.173530Z","shell.execute_reply":"2024-12-30T19:56:03.180091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configurazione del dispositivo\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Training on {device}\")\n\n# Preparazione del modello, dataset e dataloader\nmodel = model.to(device)  # Sposta il modello sul dispositivo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:56:03.181871Z","iopub.execute_input":"2024-12-30T19:56:03.182149Z","iopub.status.idle":"2024-12-30T19:56:03.230306Z","shell.execute_reply.started":"2024-12-30T19:56:03.182121Z","shell.execute_reply":"2024-12-30T19:56:03.229483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione del Trainer\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,\n    train_dataloader=train_dataloader,\n    criterion=criterion,\n    optimizer=optimizer,\n    batch_size=batch_size,\n    num_workers=workers,\n    device=device,\n    grad_clip=grad_clip\n)\n\n# Avvio del training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T21:16:15.247429Z","iopub.status.idle":"2024-12-30T21:16:15.247808Z","shell.execute_reply.started":"2024-12-30T21:16:15.247638Z","shell.execute_reply":"2024-12-30T21:16:15.247655Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing sulle predizioni","metadata":{}},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model, test_dataset, label_map, rev_label_map, batch_size, num_workers, device):\n        \"\"\"\n        Initialize the Evaluator.\n        \n        :param model: Trained SSD model to be evaluated\n        :param test_dataset: Dataset object for testing\n        :param batch_size: Batch size for evaluation\n        :param num_workers: Number of data loading workers\n        :param device: Device to use for evaluation ('cuda' or 'cpu')\n        \"\"\"\n        self.model = model.to(device)\n        self.test_dataset = test_dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.device = device\n        self.pp = PrettyPrinter()  # For printing APs nicely\n\n        # Prepare dataloader\n        self.test_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            collate_fn=test_dataset.collate_fn,\n            num_workers=num_workers,\n            pin_memory=True\n        )\n\n    self.label_map = label_map\n    self.rev_label_map = rev_label_map\n\n    def evaluate(self):\n        \"\"\"\n        Perform evaluation and compute mAP.\n        \"\"\"\n        self.model.eval()\n\n        # Lists to store detected and true boxes, labels, scores\n        det_boxes = list()\n        det_labels = list()\n        det_scores = list()\n        true_boxes = list()\n        true_labels = list()\n\n        with torch.no_grad():\n            for i, (images, boxes, labels) in enumerate(tqdm(self.test_loader, desc='Evaluating')):\n                images = images.to(self.device)\n\n                # Forward pass\n                predicted_locs, predicted_scores = self.model(images)\n\n                # Detect objects\n                det_boxes_batch, det_labels_batch, det_scores_batch = self.model.detect_objects(\n                    predicted_locs, predicted_scores,\n                    min_score=0.01, max_overlap=0.45, top_k=200\n                )\n\n                # Store this batch's results\n                boxes = [b.to(self.device) for b in boxes]\n                labels = [l.to(self.device) for l in labels]\n\n                det_boxes.extend(det_boxes_batch)\n                det_labels.extend(det_labels_batch)\n                det_scores.extend(det_scores_batch)\n                true_boxes.extend(boxes)\n                true_labels.extend(labels)\n\n        # Visualize Confusion Matrix\n        self.plot_confusion_matrix(det_labels, true_labels)\n\n        # Calculate mAP\n        APs, mAP = self.calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, self.label_map, self.rev_label_map, self.device)\n\n        # Print AP for each class\n        self.pp.pprint(APs)\n        print('\\nMean Average Precision (mAP): %.3f' % mAP)\n\n        # Visualize Precision-Recall\n        self.plot_precision_recall(APs)\n\n    @staticmethod\n    def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, label_map, rev_label_map, device):\n        \"\"\"\n        Calculate the Mean Average Precision (mAP) of detected objects.\n    \n        :param det_boxes: list of tensors, one tensor for each image containing detected objects' bounding boxes\n        :param det_labels: list of tensors, one tensor for each image containing detected objects' labels\n        :param det_scores: list of tensors, one tensor for each image containing detected objects' scores\n        :param true_boxes: list of tensors, one tensor for each image containing actual objects' bounding boxes\n        :param true_labels: list of tensors, one tensor for each image containing actual objects' labels\n        :param label_map: Mapping of label indices to human-readable class names\n        :param rev_label_map: Reverse mapping from class names to label indices\n        :param device: The device (e.g., 'cuda' or 'cpu')\n        :return: dictionary of average precisions for all classes, mean average precision (mAP)\n        \"\"\"\n        assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(true_labels)\n        n_classes = 12\n    \n        # Store all (true) objects in a single continuous tensor\n        true_images = []\n        for i in range(len(true_labels)):\n            true_images.extend([i] * true_labels[i].size(0))\n        true_images = torch.LongTensor(true_images).to(device)  # (n_objects)\n        true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n        true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n    \n        # Store all detections in a single continuous tensor\n        det_images = []\n        for i in range(len(det_labels)):\n            det_images.extend([i] * det_labels[i].size(0))\n        det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n        det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n        det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n        det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n    \n        # Calculate APs for each class (except background)\n        average_precisions = torch.zeros(n_classes - 1, dtype=torch.float).to(device)  # (n_classes - 1)\n    \n        for c in range(1, n_classes):  # Classes are 1-indexed (background is 0)\n            # Extract true objects for class 'c'\n            true_class_images = true_images[true_labels == c]  # (n_class_objects)\n            true_class_boxes = true_boxes[true_labels == c]  # (n_class_objects, 4)\n            n_easy_class_objects = true_class_boxes.size(0)  # total number of objects for this class\n    \n            # Track detected objects for class 'c'\n            det_class_images = det_images[det_labels == c]  # (n_class_detections)\n            det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n            det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n            n_class_detections = det_class_boxes.size(0)\n    \n            if n_class_detections == 0:\n                continue\n    \n            # Sort detections by score\n            det_class_scores, sort_ind = torch.sort(det_class_scores, descending=True)\n            det_class_images = det_class_images[sort_ind]\n            det_class_boxes = det_class_boxes[sort_ind]\n    \n            # Initialize true positives and false positives\n            true_positives = torch.zeros(n_class_detections, dtype=torch.float).to(device)\n            false_positives = torch.zeros(n_class_detections, dtype=torch.float).to(device)\n    \n            true_class_boxes_detected = torch.zeros(n_easy_class_objects, dtype=torch.uint8).to(device)\n    \n            # Iterate through detections\n            for d in range(n_class_detections):\n                this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n                this_image = det_class_images[d]\n    \n                # Find objects in the same image with this class\n                object_boxes = true_class_boxes[true_class_images == this_image]\n                if object_boxes.size(0) == 0:\n                    false_positives[d] = 1\n                    continue\n    \n                # Find maximum IoU between detection and true objects\n                overlaps = find_jaccard_overlap(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n                max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)\n    \n                # If IoU is above threshold (0.5), it counts as a match\n                if max_overlap.item() > 0.5:\n                    if true_class_boxes_detected[ind] == 0:\n                        true_positives[d] = 1\n                        true_class_boxes_detected[ind] = 1  # Mark as detected\n                    else:\n                        false_positives[d] = 1\n                else:\n                    false_positives[d] = 1\n    \n            # Calculate precision and recall\n            cumul_true_positives = torch.cumsum(true_positives, dim=0)\n            cumul_false_positives = torch.cumsum(false_positives, dim=0)\n            cumul_precision = cumul_true_positives / (cumul_true_positives + cumul_false_positives + 1e-10)\n            cumul_recall = cumul_true_positives / n_easy_class_objects\n    \n            # Calculate AP for this class\n            recall_thresholds = torch.arange(0, 1.1, 0.1).tolist()\n            precisions = torch.zeros(len(recall_thresholds), dtype=torch.float).to(device)\n    \n            for i, t in enumerate(recall_thresholds):\n                recalls_above_t = cumul_recall >= t\n                if recalls_above_t.any():\n                    precisions[i] = cumul_precision[recalls_above_t].max()\n                else:\n                    precisions[i] = 0.\n    \n            average_precisions[c - 1] = precisions.mean()\n    \n        # Calculate mean Average Precision (mAP)\n        mAP = average_precisions.mean().item()\n    \n        # Map class-wise APs to human-readable labels\n        average_precisions = {rev_label_map[c + 1]: v.item() for c, v in enumerate(average_precisions)}\n    \n        return average_precisions, mAP\n\n    def plot_precision_recall(self, det_boxes, det_labels, det_scores, true_boxes, true_labels):\n        \"\"\"\n        Plot Precision-Recall curves for each class based on the detection results.\n        \"\"\"\n        n_classes = 12\n        plt.figure(figsize=(12, 8))\n    \n        for c in range(1, n_classes):  # Skip background (class 0)\n            # Calcola precisione e richiamo per questa classe\n            precisions, recalls, _ = precision_recall_curve(\n                y_true=(torch.cat(true_labels).cpu().numpy() == c).astype(int),\n                probas_pred=torch.cat(det_scores).cpu().numpy()\n            )\n            plt.plot(recalls, precisions, label=f'Class {c}')\n    \n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.title('Precision-Recall Curve')\n        plt.legend()\n        plt.grid()\n        plt.show()\n\n\n    def plot_confusion_matrix(self, det_labels, true_labels):\n        \"\"\"\n        Plot confusion matrix.\n        :param det_labels: Detected labels\n        :param true_labels: Ground truth labels\n        \"\"\"\n        all_det_labels = torch.cat(det_labels).cpu().numpy()\n        all_true_labels = torch.cat(true_labels).cpu().numpy()\n\n        # Compute confusion matrix\n        cm = confusion_matrix(all_true_labels, all_det_labels, labels=list(range(1, 12)))\n\n        plt.figure(figsize=(12, 10))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 12), yticklabels=range(1, 12))\n        plt.xlabel('Predicted Labels')\n        plt.ylabel('True Labels')\n        plt.title('Confusion Matrix')\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T21:42:02.270458Z","iopub.execute_input":"2024-12-30T21:42:02.270951Z","iopub.status.idle":"2024-12-30T21:42:02.281765Z","shell.execute_reply.started":"2024-12-30T21:42:02.270915Z","shell.execute_reply":"2024-12-30T21:42:02.280866Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, label_map, rev_label_map, device):\n    \"\"\"\n    Calculate the Mean Average Precision (mAP) of detected objects.\n\n    :param det_boxes: list of tensors, one tensor for each image containing detected objects' bounding boxes\n    :param det_labels: list of tensors, one tensor for each image containing detected objects' labels\n    :param det_scores: list of tensors, one tensor for each image containing detected objects' scores\n    :param true_boxes: list of tensors, one tensor for each image containing actual objects' bounding boxes\n    :param true_labels: list of tensors, one tensor for each image containing actual objects' labels\n    :param label_map: Mapping of label indices to human-readable class names\n    :param rev_label_map: Reverse mapping from class names to label indices\n    :param device: The device (e.g., 'cuda' or 'cpu')\n    :return: dictionary of average precisions for all classes, mean average precision (mAP)\n    \"\"\"\n    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(true_labels)\n    n_classes = len(label_map)\n\n    # Store all (true) objects in a single continuous tensor\n    true_images = []\n    for i in range(len(true_labels)):\n        true_images.extend([i] * true_labels[i].size(0))\n    true_images = torch.LongTensor(true_images).to(device)  # (n_objects)\n    true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n    true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n\n    # Store all detections in a single continuous tensor\n    det_images = []\n    for i in range(len(det_labels)):\n        det_images.extend([i] * det_labels[i].size(0))\n    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n    det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n    det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n    det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n\n    # Calculate APs for each class (except background)\n    average_precisions = torch.zeros(n_classes - 1, dtype=torch.float).to(device)  # (n_classes - 1)\n\n    for c in range(1, n_classes):  # Classes are 1-indexed (background is 0)\n        # Extract true objects for class 'c'\n        true_class_images = true_images[true_labels == c]  # (n_class_objects)\n        true_class_boxes = true_boxes[true_labels == c]  # (n_class_objects, 4)\n        n_easy_class_objects = true_class_boxes.size(0)  # total number of objects for this class\n\n        # Track detected objects for class 'c'\n        det_class_images = det_images[det_labels == c]  # (n_class_detections)\n        det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n        det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n        n_class_detections = det_class_boxes.size(0)\n\n        if n_class_detections == 0:\n            continue\n\n        # Sort detections by score\n        det_class_scores, sort_ind = torch.sort(det_class_scores, descending=True)\n        det_class_images = det_class_images[sort_ind]\n        det_class_boxes = det_class_boxes[sort_ind]\n\n        # Initialize true positives and false positives\n        true_positives = torch.zeros(n_class_detections, dtype=torch.float).to(device)\n        false_positives = torch.zeros(n_class_detections, dtype=torch.float).to(device)\n\n        true_class_boxes_detected = torch.zeros(n_easy_class_objects, dtype=torch.uint8).to(device)\n\n        # Iterate through detections\n        for d in range(n_class_detections):\n            this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n            this_image = det_class_images[d]\n\n            # Find objects in the same image with this class\n            object_boxes = true_class_boxes[true_class_images == this_image]\n            if object_boxes.size(0) == 0:\n                false_positives[d] = 1\n                continue\n\n            # Find maximum IoU between detection and true objects\n            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)\n\n            # If IoU is above threshold (0.5), it counts as a match\n            if max_overlap.item() > 0.5:\n                if true_class_boxes_detected[ind] == 0:\n                    true_positives[d] = 1\n                    true_class_boxes_detected[ind] = 1  # Mark as detected\n                else:\n                    false_positives[d] = 1\n            else:\n                false_positives[d] = 1\n\n        # Calculate precision and recall\n        cumul_true_positives = torch.cumsum(true_positives, dim=0)\n        cumul_false_positives = torch.cumsum(false_positives, dim=0)\n        cumul_precision = cumul_true_positives / (cumul_true_positives + cumul_false_positives + 1e-10)\n        cumul_recall = cumul_true_positives / n_easy_class_objects\n\n        # Calculate AP for this class\n        recall_thresholds = torch.arange(0, 1.1, 0.1).tolist()\n        precisions = torch.zeros(len(recall_thresholds), dtype=torch.float).to(device)\n\n        for i, t in enumerate(recall_thresholds):\n            recalls_above_t = cumul_recall >= t\n            if recalls_above_t.any():\n                precisions[i] = cumul_precision[recalls_above_t].max()\n            else:\n                precisions[i] = 0.\n\n        average_precisions[c - 1] = precisions.mean()\n\n    # Calculate mean Average Precision (mAP)\n    mAP = average_precisions.mean().item()\n\n    # Map class-wise APs to human-readable labels\n    average_precisions = {rev_label_map[c + 1]: v.item() for c, v in enumerate(average_precisions)}\n\n    return average_precisions, mAP\n","metadata":{"execution":{"iopub.status.busy":"2024-12-30T21:42:02.331659Z","iopub.execute_input":"2024-12-30T21:42:02.331958Z","iopub.status.idle":"2024-12-30T21:42:02.346476Z","shell.execute_reply.started":"2024-12-30T21:42:02.331932Z","shell.execute_reply":"2024-12-30T21:42:02.345668Z"}}},{"cell_type":"code","source":"# Caricamento del modello\ncheckpoint = torch.load(checkpoint_path)\nmodel = checkpoint['model']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T21:42:02.371314Z","iopub.execute_input":"2024-12-30T21:42:02.371571Z","iopub.status.idle":"2024-12-30T21:50:02.813257Z","shell.execute_reply.started":"2024-12-30T21:42:02.371547Z","shell.execute_reply":"2024-12-30T21:50:02.812202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creazione e avvio del valutatore\nevaluator = Evaluator(model=model, test_dataset=test_dataset, label_map=label_map, rev_label_map=rev_label_map, batch_size=64, num_workers=4, device=device)\nevaluator.evaluate()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}