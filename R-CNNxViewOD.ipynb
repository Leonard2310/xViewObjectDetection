{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10182328,"sourceType":"datasetVersion","datasetId":6242793},{"sourceId":10311668,"sourceType":"datasetVersion","datasetId":6295408},{"sourceId":10322992,"sourceType":"datasetVersion","datasetId":6258380},{"sourceId":10329194,"sourceType":"datasetVersion","datasetId":6395444},{"sourceId":213827,"sourceType":"modelInstanceVersion","modelInstanceId":182143,"modelId":204371}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:56.666679Z","iopub.execute_input":"2024-12-28T23:07:56.667003Z","iopub.status.idle":"2024-12-28T23:07:58.491938Z","shell.execute_reply.started":"2024-12-28T23:07:56.666976Z","shell.execute_reply":"2024-12-28T23:07:58.490903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Librerie per l'ottimizzazione e la gestione delle dipendenze\nimport selectivesearch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport zipfile\nimport torchvision\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport matplotlib.patches as patches\nimport torch.nn.utils as utils\nimport ast\nimport itertools","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:56:05.210922Z","iopub.execute_input":"2024-12-31T09:56:05.211241Z","iopub.status.idle":"2024-12-31T09:56:11.738974Z","shell.execute_reply.started":"2024-12-31T09:56:05.211208Z","shell.execute_reply":"2024-12-31T09:56:11.737701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nCOCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_COCO_JSON_NM = 'mod_COCO_annotations.json'\nOUT_IMAGE_FLDR_NM = 'images'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nprop_dataset_pth = Path('/kaggle/input/proposals-xview-dataset-r-cnn')\nactreg_dataset_pth = Path('/kaggle/input/activeregion-xviewdataset')\nresnet_pth = Path('/kaggle/input/resnet-50-xview/pytorch/default')\nregbox_dataset_pth = Path('/kaggle/input/regbboxdataset-xview')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / COCO_JSON_NM\nnew_coco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\nin_new_coco_json_pth = prop_dataset_pth / OUT_COCO_JSON_NM\n\n# PROPOSALS\nPROP_COCO_JSON_NM = 'proposals.json'\nproposals_json = prop_dataset_pth / PROP_COCO_JSON_NM\nout_proposals_json = out_dataset_pth / PROP_COCO_JSON_NM\nPROPOSALS = 'proposals'\nprop_fldr = out_dataset_pth / PROPOSALS\n\n\n# ACTIVE REGIONS\nACTPROP_COCO_JSON_NM ='active_regions.json'\nactproposals_json = out_dataset_pth / ACTPROP_COCO_JSON_NM\nin_actproposals_json = actreg_dataset_pth / ACTPROP_COCO_JSON_NM\nout_act_reg_folder = out_dataset_pth / PROPOSALS\nact_reg_folder = actreg_dataset_pth / PROPOSALS\n\n#DATASET\ntrain_path = out_dataset_pth / 'train.json'\ntest_path = out_dataset_pth / 'test.json'\nval_path = out_dataset_pth / 'val.json'\nreg_path = out_dataset_pth / 'reg.json'\n\n# Percorso per salvare il modello\npath_min_loss = '/kaggle/working/ResNet50.pth'\n\ntrain_features_path = out_dataset_pth / 'train_features.npy'\n\nresnet_min_loss_path =  resnet_pth / '3/ResNet50.pth'\n\nin_train_path = regbox_dataset_pth / 'train.json'\nin_reg_path = regbox_dataset_pth / 'reg.json'\nin_train_features_path = regbox_dataset_pth / 'train_features.npy'\n\nout_reg_path = out_dataset_pth / 'new_reg.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:56:46.971600Z","iopub.execute_input":"2024-12-31T09:56:46.972131Z","iopub.status.idle":"2024-12-31T09:56:46.985484Z","shell.execute_reply.started":"2024-12-31T09:56:46.972082Z","shell.execute_reply":"2024-12-31T09:56:46.984282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T00:02:20.793347Z","iopub.execute_input":"2024-12-30T00:02:20.794082Z","iopub.status.idle":"2024-12-30T00:02:20.800239Z","shell.execute_reply.started":"2024-12-30T00:02:20.794042Z","shell.execute_reply":"2024-12-30T00:02:20.799432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utility","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")\n\n# Sopprime i warning relativi al parametro 'pretrained' deprecato\nwarnings.filterwarnings(\"ignore\", \n    message=\"The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\")\n\n# Sopprime i warning relativi agli argomenti diversi da weight enum o None\nwarnings.filterwarnings(\"ignore\", \n    message=\"Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future.*\")\n\n# Sopprime i warning relativi al parametro 'verbose' deprecato\nwarnings.filterwarnings(\"ignore\", \n    message=\"The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\")\n\n# Sopprime i warning relativi a `torch.load` e `weights_only=False`\nwarnings.filterwarnings(\"ignore\", \n    message=\"You are using `torch.load` with `weights_only=False`.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:56:54.833303Z","iopub.execute_input":"2024-12-31T09:56:54.834430Z","iopub.status.idle":"2024-12-31T09:56:54.841255Z","shell.execute_reply.started":"2024-12-31T09:56:54.834383Z","shell.execute_reply":"2024-12-31T09:56:54.840160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_json(file_path):\n    \"\"\"\n    Carica un file JSON dal percorso specificato.\n\n    :param file_path: Percorso al file JSON da caricare.\n    :return: Dati contenuti nel file JSON (come dizionario o lista).\n    \"\"\"\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:56:57.071308Z","iopub.execute_input":"2024-12-31T09:56:57.071692Z","iopub.status.idle":"2024-12-31T09:56:57.076905Z","shell.execute_reply.started":"2024-12-31T09:56:57.071659Z","shell.execute_reply":"2024-12-31T09:56:57.075886Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Dataset","metadata":{}},{"cell_type":"code","source":"def display_images_with_bboxes(json_file, specific_images, images_folder):\n    \"\"\"\n    Visualizza le immagini specificate con tutti i bounding box sopra di esse.\n    \n    :param json_file: percorso del file JSON contenente le immagini, annotazioni e categorie\n    :param specific_images: lista di nomi delle immagini da visualizzare\n    :param images_folder: percorso della cartella che contiene le immagini\n    \"\"\"\n    # Carica il JSON\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    \n    # Estrai le immagini e le annotazioni\n    images = data[\"images\"]\n    annotations = data[\"annotations\"]\n    \n    # Crea un dizionario per mappare l'id delle immagini al nome del file\n    image_dict = {image[\"id\"]: image[\"file_name\"] for image in images}\n    \n    # Filtra le annotazioni per le immagini specifiche\n    specific_annotations = [ann for ann in annotations if image_dict[ann[\"image_id\"]] in specific_images]\n\n    # Creiamo un dizionario per raccogliere tutte le annotazioni per ciascuna immagine\n    image_bboxes = {}\n    for annotation in specific_annotations:\n        image_name = image_dict[annotation[\"image_id\"]]\n        if image_name not in image_bboxes:\n            image_bboxes[image_name] = []\n        bbox = ast.literal_eval(annotation[\"bbox\"])\n        image_bboxes[image_name].append(bbox)\n    \n    # Visualizza tutte le immagini con tutti i bounding box\n    for image_name, bboxes in image_bboxes.items():\n        # Carica l'immagine\n        image_path = f'{images_folder}/{image_name}'  # Usa il percorso corretto per le immagini\n        image = Image.open(image_path)\n\n        # Crea la figura per la visualizzazione\n        plt.figure(figsize=(8, 8))\n        plt.imshow(image)\n\n        # Aggiungi tutti i bounding box\n        for bbox in bboxes:\n            x, y, w, h = bbox\n            plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none'))\n\n        # Imposta il titolo e disattiva gli assi\n        plt.title(f\"Image: {image_name}\")\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:14:42.690436Z","iopub.execute_input":"2024-12-31T11:14:42.691201Z","iopub.status.idle":"2024-12-31T11:14:42.702312Z","shell.execute_reply.started":"2024-12-31T11:14:42.691158Z","shell.execute_reply":"2024-12-31T11:14:42.701134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"specific_images = ['img_418_2240_2880.jpg','img_418_2240_640.jpg']\n\ndisplay_images_with_bboxes(coco_json_pth, specific_images, img_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:45:50.230030Z","iopub.execute_input":"2024-12-31T11:45:50.230904Z","iopub.status.idle":"2024-12-31T11:45:53.474913Z","shell.execute_reply.started":"2024-12-31T11:45:50.230861Z","shell.execute_reply":"2024-12-31T11:45:53.473705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# COCO Preprocessing","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    data = load_json(input_path)\n\n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n \n    for category in tqdm(raw_categories, desc=\"Processing Categories\"):\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n \n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n \n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n \n    # Preprocessa le annotazioni in un dizionario per immagini\n    image_annotations_dict = {}\n    for annotation in tqdm(data.get('annotations', []), desc=\"Building Image Annotations Dictionary\"):\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_dict:\n            image_annotations_dict[image_id] = []\n        image_annotations_dict[image_id].append(annotation)\n \n    # Elenco di annotazioni da mantenere (solo quelle valide)\n    valid_annotations = []\n    annotations_to_remove = set()\n \n    # Controllo dei bounding box\n    for annotation in tqdm(data.get('annotations', []), desc=\"Processing Annotations\"):\n        if annotation['category_id'] == 0:  # Se è Aircraft\n            annotation['category_id'] = 11\n        \n        # Converte il formato del bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n        \n        x, y, width, height = annotation['bbox']\n        xmin, xmax = x, x + width\n        ymin, ymax = y, y + height\n        \n        # Verifica che xmin < xmax e ymin < ymax, e che la larghezza e altezza siano sufficienti\n        if xmin >= xmax or ymin >= ymax or width <= 10 or height <= 10:\n            annotations_to_remove.add(annotation['id'])\n        else:\n            annotation['bbox'] = [xmin, ymin, xmax, ymax]\n            valid_annotations.append(annotation)\n \n    # Rimuovi le annotazioni non valide\n    data['annotations'] = valid_annotations\n \n    # Verifica se ci sono immagini senza annotazioni (usando il dizionario delle annotazioni)\n    new_annotations = []\n    for image in tqdm(data.get('images', []), desc=\"Processing Images\"):\n        if image['id'] not in image_annotations_dict:  # Se l'immagine non ha annotazioni\n            # Aggiungi la categoria \"background\"\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, 0.0, image['width'], image['height']],  # Background con bbox che copre tutta l'immagine\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n \n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n\n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n\n    # **Seleziona casualmente il 50% delle immagini**\n    all_images = data.get('images', [])\n    selected_images = random.sample(all_images, len(all_images) // 2)\n    selected_image_ids = {img['id'] for img in selected_images}\n\n    # Filtra immagini e annotazioni\n    data['images'] = selected_images\n    data['annotations'] = [ann for ann in data['annotations'] if ann['image_id'] in selected_image_ids]\n\n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.550338Z","iopub.status.idle":"2024-12-28T23:07:58.550688Z","shell.execute_reply.started":"2024-12-28T23:07:58.550523Z","shell.execute_reply":"2024-12-28T23:07:58.550541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_custom_coco_json(coco_json_pth, new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.551597Z","iopub.status.idle":"2024-12-28T23:07:58.551943Z","shell.execute_reply.started":"2024-12-28T23:07:58.551766Z","shell.execute_reply":"2024-12-28T23:07:58.551783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Category Check","metadata":{}},{"cell_type":"code","source":"def count_bounding_boxes(json_path):\n    \"\"\"\n    Conta il numero di bounding box per ogni categoria in un file COCO JSON.\n\n    Args:\n        json_path (str): Percorso del file JSON.\n\n    Returns:\n        list: Elenco di tuple con ID categoria, nome categoria e numero di bounding box.\n    \"\"\"\n    # Carica il file JSON\n    coco_data = load_json(json_path)\n\n    # Estrarre i dati principali\n    annotations = coco_data.get(\"annotations\", [])\n    categories = coco_data.get(\"categories\", [])\n\n    # Mappare id di categoria ai nomi delle categorie\n    category_id_to_name = {category[\"id\"]: category[\"name\"] for category in categories}\n\n    # Contare i bounding box per categoria\n    bbox_counts = defaultdict(int)\n    for annotation in annotations:\n        category_id = annotation[\"category_id\"]\n        bbox_counts[category_id] += 1\n\n    # Creare un elenco dei risultati\n    results = [\n        (cat_id, category_id_to_name.get(cat_id, \"Unknown\"), count)\n        for cat_id, count in bbox_counts.items()\n    ]\n    \n    # Ordinare i risultati in ordine decrescente per numero di bounding box\n    results.sort(key=lambda x: x[2], reverse=True)\n    \n    # Stampare i risultati\n    for cat_id, category_name, count in results:\n        print(f\"Categoria ID {cat_id} ('{category_name}'): {count} bounding box\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.552859Z","iopub.status.idle":"2024-12-28T23:07:58.553187Z","shell.execute_reply.started":"2024-12-28T23:07:58.553029Z","shell.execute_reply":"2024-12-28T23:07:58.553046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count_bounding_boxes(new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.554746Z","iopub.status.idle":"2024-12-28T23:07:58.555045Z","shell.execute_reply.started":"2024-12-28T23:07:58.554894Z","shell.execute_reply":"2024-12-28T23:07:58.554909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Region Proposals Generation","metadata":{}},{"cell_type":"code","source":"# Funzione per elaborare una singola immagine\ndef process_single_image(image_data, img_fldr):\n    img_id = image_data['id']\n    img_name = image_data['file_name']\n    img_path = os.path.join(img_fldr, img_name)\n\n    if not os.path.exists(img_path):\n        raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    # Carica l'immagine usando opencv (in modalità RGB)\n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n    original_height, original_width, _ = image.shape\n\n    # Ridimensiona l'immagine per velocizzare la Selective Search\n    resized_image = cv2.resize(image, (original_width // 2, original_height // 2), interpolation=cv2.INTER_AREA)\n\n    # Genera le region proposals sulla versione ridotta\n    processed_proposals = generate_and_process_proposals(resized_image, original_width // 2, original_height // 2)\n\n    # Riscalare le coordinate delle proposte alla dimensione originale\n    scaled_proposals = [[x * 2, y * 2, x_max * 2, y_max * 2] for x, y, x_max, y_max in processed_proposals]\n\n    image_data = {\n        \"image_id\": img_id,\n        \"file_name\": img_name,\n        \"original_size\": [original_width, original_height],\n        \"proposals\": []\n    }\n\n    for i, proposal in enumerate(scaled_proposals):\n        x_min, y_min, x_max, y_max = proposal\n        image_data[\"proposals\"].append({\n            \"proposal_id\": i,\n            \"coordinates\": [x_min, y_min, x_max, y_max]\n        })\n\n    return image_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.556269Z","iopub.status.idle":"2024-12-28T23:07:58.556618Z","shell.execute_reply.started":"2024-12-28T23:07:58.556413Z","shell.execute_reply":"2024-12-28T23:07:58.556464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funzione per generare le region proposals con Selective Search\ndef generate_and_process_proposals(image, img_width, img_height):\n    img_np = np.array(image, dtype=np.uint8)\n\n    # Esegui la selective search con parametri ottimizzati\n    _, regions = selectivesearch.selective_search(img_np, scale=200, sigma=0.5, min_size=10)\n    if len(regions) == 0:\n        print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n    processed_proposals = []\n\n    # Pre-filtraggio delle regioni\n    for region in regions:\n        x, y, w, h = region['rect']\n        area = w * h\n        if w >= 10 and h >= 10 and 10 <= area <= 0.8 * (img_width * img_height):\n            x_max, y_max = x + w, y + h\n            processed_proposals.append([x, y, x_max, y_max])\n\n    return processed_proposals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.557612Z","iopub.status.idle":"2024-12-28T23:07:58.557927Z","shell.execute_reply.started":"2024-12-28T23:07:58.557770Z","shell.execute_reply":"2024-12-28T23:07:58.557787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funzione per gestire i batch\ndef batch(iterable, n=1):\n    it = iter(iterable)\n    while True:\n        chunk = list(islice(it, n))\n        if not chunk:\n            break\n        yield chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.560384Z","iopub.status.idle":"2024-12-28T23:07:58.560726Z","shell.execute_reply.started":"2024-12-28T23:07:58.560575Z","shell.execute_reply":"2024-12-28T23:07:58.560592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_dataset_proposals(coco_json, img_fldr, output_dir, output_json):\n    os.makedirs(output_dir, exist_ok=True)\n    all_image_data = []\n\n    # Carica il file JSON di COCO\n    coco_data = load_json(coco_json)\n\n    # Prepara il mapping delle annotazioni per le immagini\n    image_annotations_map = {}\n    for annotation in coco_data['annotations']:\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_map:\n            image_annotations_map[image_id] = []\n        image_annotations_map[image_id].append(annotation)\n\n    # Filtra le immagini che contengono annotazioni con category_id == 0 (sfondo)\n    images_with_annotations = [\n        image_data for image_data in coco_data['images']\n        if image_data['id'] in image_annotations_map and len(image_annotations_map[image_data['id']]) > 0\n        and not any(annot['category_id'] == 0 for annot in image_annotations_map[image_data['id']])  # Escludi sfondo\n    ]\n\n    # Parametri per parallelizzazione e batch processing\n    max_workers = os.cpu_count() - 1\n    batch_size = 500\n    total_batches = len(images_with_annotations) // batch_size + (len(images_with_annotations) % batch_size > 0)\n\n    # Processa le immagini in batch con tqdm per monitorare il progresso dei batch\n    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n        for image_batch in batch(images_with_annotations, batch_size):\n            with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n                results = list(executor.map(process_single_image, image_batch, [img_fldr] * len(image_batch)))\n            all_image_data.extend(results)\n            pbar.update(1)  # Aggiorna la barra di progresso per ogni batch completato\n\n    # Salva il risultato in formato JSON usando orjson\n    with open(output_json, 'wb') as json_file:\n        json_file.write(orjson.dumps(all_image_data, option=orjson.OPT_INDENT_2))\n\n    print(f\"Creato file JSON con le region proposals: {output_json}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.561941Z","iopub.status.idle":"2024-12-28T23:07:58.562272Z","shell.execute_reply.started":"2024-12-28T23:07:58.562113Z","shell.execute_reply":"2024-12-28T23:07:58.562130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_dataset_proposals(new_coco_json_pth, img_fldr, prop_fldr, out_proposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.563851Z","iopub.status.idle":"2024-12-28T23:07:58.564168Z","shell.execute_reply.started":"2024-12-28T23:07:58.564013Z","shell.execute_reply":"2024-12-28T23:07:58.564029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Nome del file zip da creare\nzip_file_name = \"proposals-dataset.zip\"\n\n# Elenco di file e cartelle da includere nello zip\nitems_to_zip = [\n    \"mod_COCO_annotations.json\",\n    \"proposals.json\",\n]\n\n# Funzione per aggiungere file e cartelle allo zip\ndef zip_folder(zipf, folder_path, base_folder=\"\"):\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, base_folder)\n            zipf.write(file_path, arcname)\n\n# Creazione dello zip\nwith zipfile.ZipFile(zip_file_name, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.exists(item):  # Verifica che il file o la cartella esista\n            if os.path.isdir(item):  # Se è una cartella, aggiungi tutto il contenuto\n                zip_folder(zipf, item, out_dataset_pth)\n            else:  # Se è un file, aggiungilo direttamente\n                zipf.write(item)\n        else:\n            print(f\"Elemento non trovato: {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.565515Z","iopub.status.idle":"2024-12-28T23:07:58.565849Z","shell.execute_reply.started":"2024-12-28T23:07:58.565691Z","shell.execute_reply":"2024-12-28T23:07:58.565709Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Positive Region Proposals with Undersampling","metadata":{}},{"cell_type":"code","source":"def visualize_image_with_bbox(image_path, bbox, title, ax):\n    \"\"\"\n    Visualizza un'immagine con sopra disegnato un bounding box usando Matplotlib in un subplot.\n    :param image_path: Percorso all'immagine.\n    :param bbox: Bounding box in formato [xmin, ymin, xmax, ymax].\n    :param title: Titolo da mostrare sopra l'immagine.\n    :param ax: Axes del subplot dove disegnare l'immagine.\n    \"\"\"\n    # Carica l'immagine\n    image = Image.open(image_path)\n    \n    # Estrai xmin, ymin, xmax, ymax\n    xmin, ymin, xmax, ymax = bbox\n    width = xmax - xmin\n    height = ymax - ymin\n\n    # Aggiungi l'immagine e il bounding box\n    ax.imshow(image)\n    rect = patches.Rectangle(\n        (xmin, ymin), width, height,\n        linewidth=2, edgecolor='red', facecolor='none'\n    )\n    ax.add_patch(rect)\n    \n    # Aggiungi il titolo\n    ax.set_title(title, fontsize=15)\n    ax.axis('off')  # Rimuovi gli assi\n\ndef calculate_bbox_areas_for_all_categories(json_file_path, images_dir):\n    \"\"\"\n    Calcola l'area minima, massima e media per ogni categoria e disegna i bounding box \n    corrispondenti sulle immagini per visualizzarle.\n\n    :param json_file_path: Percorso al file JSON contenente le annotazioni.\n    :param images_dir: Directory contenente le immagini originali.\n    \"\"\"\n    # Carica il file JSON\n    with open(json_file_path, 'r') as f:\n        data = json.load(f)\n    \n    # Crea dizionari per mappare ID\n    areas_per_category = defaultdict(list)\n    category_map = {category['id']: category['name'] for category in data['categories']}\n    images_map = {image['id']: image for image in data['images']}\n\n    # Raccogliere le aree\n    for annotation in data.get(\"annotations\", []):\n        category_id = annotation[\"category_id\"]\n        area = annotation[\"area\"]\n        areas_per_category[category_id].append((area, annotation))\n\n    # Itera per ogni categoria\n    for category_id, areas in areas_per_category.items():\n        if not areas:\n            continue\n\n        # Trova le annotazioni con area minima, massima e calcola l'area media\n        min_area_annotation = min(areas, key=lambda x: x[0])[1]\n        max_area_annotation = max(areas, key=lambda x: x[0])[1]\n        avg_area = sum(area for area, _ in areas) / len(areas)\n\n        # Stampa le informazioni sulle aree\n        print(f\"Categoria: {category_map[category_id]}\")\n        print(f\"  Area Minima: {min_area_annotation['area']}\")\n        print(f\"  Area Massima: {max_area_annotation['area']}\")\n        print(f\"  Area Media: {avg_area:.2f}\\n\")\n\n        # Crea un subplot per visualizzare le immagini con i bounding box\n        fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n\n        # Visualizza le immagini con bounding box per l'area minima e massima\n        for annotation, label, ax_idx in [(min_area_annotation, \"Minima\", 0), (max_area_annotation, \"Massima\", 1)]:\n            image_id = annotation['image_id']\n            image_info = images_map.get(image_id)\n            if not image_info:\n                print(f\"Immagine con ID {image_id} non trovata.\")\n                continue\n\n            # Percorso immagine\n            image_path = os.path.join(images_dir, image_info['file_name'])\n            if not os.path.exists(image_path):\n                print(f\"Immagine {image_path} non trovata.\")\n                continue\n            \n            # Visualizza immagine con bounding box\n            bbox = annotation['bbox']\n            title = f\"{category_map[category_id]} ({label})\"\n            visualize_image_with_bbox(image_path, bbox, title, ax[ax_idx])\n\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:41.537994Z","iopub.execute_input":"2024-12-29T02:22:41.538367Z","iopub.status.idle":"2024-12-29T02:22:41.550875Z","shell.execute_reply.started":"2024-12-29T02:22:41.538321Z","shell.execute_reply":"2024-12-29T02:22:41.549997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esempio di utilizzo\ncalculate_bbox_areas_for_all_categories(in_new_coco_json_pth, img_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:44.715977Z","iopub.execute_input":"2024-12-29T02:22:44.716850Z","iopub.status.idle":"2024-12-29T02:22:58.003089Z","shell.execute_reply.started":"2024-12-29T02:22:44.716807Z","shell.execute_reply":"2024-12-29T02:22:58.001856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ignored_count = 0  # Contatore globale per le regioni ignorate\n\ndef get_iou(bb1, bb2):\n    global ignored_count  # Accedi alla variabile globale del contatore\n\n    try:\n        # Assicurati che le dimensioni siano corrette\n        assert bb1['x1'] < bb1['x2']\n        assert bb1['y1'] < bb1['y2']\n        assert bb2['x1'] < bb2['x2']\n        assert bb2['y1'] < bb2['y2']\n    except AssertionError:\n        # Se si verifica un errore, incrementa il contatore delle regioni ignorate\n        ignored_count += 1\n        return 0.0  # Restituisci 0.0 per l'IoU in caso di errore (nessuna sovrapposizione)\n\n    # Calcola le dimensioni dell'area comune tra i due box\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    # Se non c'è sovrapposizione, restituisci 0 come area di intersezione\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calcola l'area di intersezione\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calcola le aree individuali dei due bounding box\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n    \n    # Calcola l'area dell'unione\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    # Verifica che l'IoU sia nel range corretto\n    assert iou >= 0.0\n    assert iou <= 1.0\n\n    return iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.005203Z","iopub.execute_input":"2024-12-29T02:22:58.005664Z","iopub.status.idle":"2024-12-29T02:22:58.016873Z","shell.execute_reply.started":"2024-12-29T02:22:58.005609Z","shell.execute_reply":"2024-12-29T02:22:58.015848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_adaptive_threshold(bbox):\n    \"\"\"Calcola un threshold IoU adattivo in base all'area del bounding box e alla categoria.\"\"\"\n    # bbox è un tensore o una lista con [x1, y1, x2, y2]\n    width = bbox[2] - bbox[0]  # Calcolo larghezza\n    height = bbox[3] - bbox[1]  # Calcolo altezza\n    \n    # Calcolo dell'area\n    area = width * height\n\n    counter = 0\n    \n    # Aree medie per categoria (escludendo background)\n    area_media = {\n        2: 651.14,   # Truck\n        11: 6075.12  # Aircraft\n    }\n    \n    # Recupero delle aree minima e massima per normalizzazione\n    min_area = min(area_media.values())\n    max_area = max(area_media.values())\n    \n    # Normalizziamo l'area rispetto all'intervallo di area definito\n    normalized_area = (area - min_area) / (max_area - min_area)  # Valore tra 0 e 1\n    \n    # Clipping per evitare valori fuori intervallo\n    normalized_area = max(0, min(1, normalized_area))\n    \n    # Mappiamo il valore normalizzato su un intervallo di soglie (0.3 a 0.7)\n    threshold = 0.3 + 0.4 * normalized_area  # Da 0.3 a 0.7\n    \n    return threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.018027Z","iopub.execute_input":"2024-12-29T02:22:58.018295Z","iopub.status.idle":"2024-12-29T02:22:58.033329Z","shell.execute_reply.started":"2024-12-29T02:22:58.018270Z","shell.execute_reply":"2024-12-29T02:22:58.032583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assign_and_save_regions(region_json_path, bbox_json_path, image_dir, output_dir, output_json_path):\n    \"\"\"Associa le regioni proposte ai bounding boxes, salva le regioni positive come immagini e crea un nuovo JSON con informazioni attivate.\"\"\"\n    \n    # Carica i file JSON\n    with open(region_json_path, 'r') as f:\n        regions = json.load(f)\n\n    with open(bbox_json_path, 'r') as f:\n        bboxes = json.load(f)\n    \n    # Crea un dizionario per cercare annotations per image_id\n    annotations_by_image = {}\n    for annot in bboxes[\"annotations\"]:\n        img_id = annot[\"image_id\"]\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        bbox = annot[\"bbox\"]\n        annotations_by_image[img_id].append((torch.tensor(bbox, dtype=torch.float32), annot[\"category_id\"]))\n    \n    # Crea un dizionario per mappare category_id ai nomi delle categorie\n    category_mapping = {cat_id: name for cat_id, name in enumerate(bboxes[\"categories\"])}\n    \n    # Crea la directory di output se non esiste\n    os.makedirs(output_dir, exist_ok=True)\n\n    counter = 0  # Contatore delle immagini salvate\n    \n    active_region_data = []  # Lista per i dati delle regioni attive\n\n    # Avvolgi il ciclo principale per ogni immagine con tqdm\n    for image in tqdm(regions, desc=\"Elaborazione immagini\", total=len(regions)):\n        image_id = image[\"image_id\"]\n        file_name = image[\"file_name\"]\n        proposals = image[\"proposals\"]\n        \n        # Ottieni bounding boxes ground-truth e categorie per l'immagine corrente\n        gt_data = annotations_by_image.get(image_id, [])\n        if not gt_data:\n            # Se non ci sono bounding boxes ground-truth, salta l'immagine\n            continue\n        \n        gt_bboxes = [item[0] for item in gt_data]  # Bounding box ground truth\n        gt_categories = [item[1] for item in gt_data]  # Categorie ground truth\n        \n        # Trasforma proposals in una lista di dizionari compatibili con get_iou\n        proposal_coords = [{'x1': p[\"coordinates\"][0], 'y1': p[\"coordinates\"][1], \n                            'x2': p[\"coordinates\"][2], 'y2': p[\"coordinates\"][3]} \n                           for p in proposals]\n        \n        # Esegui undersampling per le categorie 6 prima di procedere\n        filtered_proposals = []\n        filtered_gt_bboxes = []\n        filtered_gt_categories = []\n        \n        for i, (prop, gt_bbox, gt_category) in enumerate(zip(proposal_coords, gt_bboxes, gt_categories)):\n            filtered_proposals.append(prop)\n            filtered_gt_bboxes.append(gt_bbox)\n            filtered_gt_categories.append(gt_category)\n\n        # Trasformiamo di nuovo in coordinate per IoU\n        proposal_coords = filtered_proposals\n        gt_bboxes = filtered_gt_bboxes\n        gt_categories = filtered_gt_categories\n\n        # Calcola la matrice IoU usando la funzione get_iou\n        iou_matrix = []\n        for proposal in proposal_coords:\n            iou_row = []\n            for gt_bbox in gt_bboxes:\n                gt_dict = {'x1': gt_bbox[0].item(), 'y1': gt_bbox[1].item(), \n                           'x2': gt_bbox[2].item(), 'y2': gt_bbox[3].item()}\n                iou = get_iou(proposal, gt_dict)\n                iou_row.append(iou)\n            iou_matrix.append(iou_row)\n\n        # Verifica se la matrice IoU è vuota\n        if not iou_matrix:\n            continue\n        \n        iou_matrix = torch.tensor(iou_matrix)\n\n        # Identifica le regioni positive utilizzando la soglia adattiva\n        positive_indices = []\n        zero_iou_indices = []\n        for row_idx, row in enumerate(iou_matrix):\n            for col_idx, iou in enumerate(row):\n                adaptive_threshold = get_adaptive_threshold(gt_bboxes[col_idx])\n                if iou >= adaptive_threshold:\n                    positive_indices.append((row_idx, col_idx))\n                elif iou == 0:\n                    # Accumula le immagini con IoU = 0 per selezione successiva\n                    zero_iou_indices.append((row_idx, -1))  # Assegna categoria 0 con -1\n        \n        # Seleziona casualmente il 20% delle immagini con IoU = 0\n        selected_zero_iou = random.sample(zero_iou_indices, int(0.005 * len(zero_iou_indices)))\n        \n        # Aggiungi solo le selezionate al set positivo\n        positive_indices.extend(selected_zero_iou)\n\n        # Carica l'immagine originale\n        image_path = os.path.join(image_dir, file_name)\n        original_image = cv2.imread(image_path)\n        if original_image is None:\n            print(f\"Immagine non trovata: {image_path}\")\n            continue\n\n        # Avvolgi il ciclo per ogni proposta positiva\n        for row_idx, col_idx in positive_indices:\n            if col_idx == -1:\n                category_id = 0  \n            else:\n                category_id = gt_categories[col_idx]\n            \n            # Aggiungi la condizione per includere solo il 10% delle categorie 0 e 6\n            if category_id in [0, 6]:\n                if random.random() > 0.9:  # Mantieni il 10% delle categorie 0 e 6\n                    # Calcola le coordinate del bounding box\n                    x_min, y_min, x_max, y_max = proposal_coords[row_idx].values()\n                    \n                    cropped = original_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n                    \n                    # Ridimensiona a 224x224\n                    resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n                    \n                    # Salva l'immagine\n                    output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n                    cv2.imwrite(output_path, resized)\n                    \n                    # Aggiungi la proposta attivata al nuovo JSON in formato COCO\n                    active_region_data.append({\n                        \"image_id\": image_id,\n                        \"file_name\": file_name,\n                        \"category_id\": category_id,\n                        \"proposal_id\": row_idx,\n                        \"region_bbox\": [x_min, y_min, x_max, y_max],  # Usa xmin, ymin, xmax, ymax\n                        \"original_bbox\": gt_bboxes[col_idx].tolist() if col_idx != -1 else [],  # Aggiunge il bbox originale\n                        \"saved_path\": output_path\n                    })\n                    \n                    counter += 1\n            else:\n                # Se la categoria non è 0 e 6, includi sempre la proposta\n                # Calcola le coordinate del bounding box\n                x_min, y_min, x_max, y_max = proposal_coords[row_idx].values()\n                \n                cropped = original_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n                \n                # Ridimensiona a 224x224\n                resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n                \n                # Salva l'immagine\n                output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n                cv2.imwrite(output_path, resized)\n                \n                # Aggiungi la proposta attivata al nuovo JSON in formato COCO\n                active_region_data.append({\n                    \"image_id\": image_id,\n                    \"file_name\": file_name,\n                    \"category_id\": category_id,\n                    \"proposal_id\": row_idx,\n                    \"region_bbox\": [x_min, y_min, x_max, y_max],  # Usa xmin, ymin, xmax, ymax\n                    \"original_bbox\": gt_bboxes[col_idx].tolist(),  # Aggiunge il bbox originale\n                    \"saved_path\": output_path\n                })\n                \n                counter += 1\n\n    # Salva il nuovo JSON con le regioni attive\n    with open(output_json_path, 'w') as json_file:\n        json.dump(active_region_data, json_file, indent=2)\n\n    print(counter)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.035945Z","iopub.execute_input":"2024-12-29T02:22:58.036319Z","iopub.status.idle":"2024-12-29T02:22:58.059694Z","shell.execute_reply.started":"2024-12-29T02:22:58.036290Z","shell.execute_reply":"2024-12-29T02:22:58.058891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esegui l'assegnazione e ottieni i valori IoU\n\nassign_and_save_regions(proposals_json, in_new_coco_json_pth, img_fldr, prop_fldr, actproposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.060673Z","iopub.execute_input":"2024-12-29T02:22:58.060948Z","iopub.status.idle":"2024-12-29T02:52:13.802699Z","shell.execute_reply.started":"2024-12-29T02:22:58.060921Z","shell.execute_reply":"2024-12-29T02:52:13.801875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_regions(file_path):\n    \"\"\"\n    Analizza un file JSON per ottenere il numero di regioni e le occorrenze dei category_id.\n\n    :param file_path: Percorso al file JSON contenente le annotazioni.\n    :return: Tupla contenente il numero di regioni e un dizionario con le occorrenze dei category_id.\n    \"\"\"\n    # Carica il file JSON\n    data = load_json(file_path)\n\n    # Conta il numero di regioni\n    num_regioni = len(data)\n    \n    # Ottieni le occorrenze dei category_id\n    category_ids = [entry['category_id'] for entry in data]\n    category_counts = Counter(category_ids)\n\n    # Ordina le occorrenze per ID di categoria\n    sorted_category_counts = dict(sorted(category_counts.items()))\n\n    return num_regioni, sorted_category_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:52:13.810154Z","iopub.execute_input":"2024-12-29T02:52:13.810458Z","iopub.status.idle":"2024-12-29T02:52:13.826351Z","shell.execute_reply.started":"2024-12-29T02:52:13.810409Z","shell.execute_reply":"2024-12-29T02:52:13.825390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_regioni, category_counts = analyze_regions(actproposals_json)\nprint(f\"Numero di regioni: {num_regioni}\")\nprint(\"Occorrenze dei category_id:\", category_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:52:13.827942Z","iopub.execute_input":"2024-12-29T02:52:13.828212Z","iopub.status.idle":"2024-12-29T02:52:13.977345Z","shell.execute_reply.started":"2024-12-29T02:52:13.828169Z","shell.execute_reply":"2024-12-29T02:52:13.976493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Nome del file zip da creare\nzip_file_name = \"activeregion-xview-dataset.zip\"\n\n# Elenco di file e cartelle da includere nello zip\nitems_to_zip = [\n    \"active_regions.json\",\n    \"proposals\",\n]\n\n# Funzione per aggiungere file e cartelle allo zip\ndef zip_folder(zipf, folder_path, base_folder=\"\"):\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, base_folder)\n            zipf.write(file_path, arcname)\n\n# Creazione dello zip\nwith zipfile.ZipFile(zip_file_name, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.exists(item):  # Verifica che il file o la cartella esista\n            if os.path.isdir(item):  # Se è una cartella, aggiungi tutto il contenuto\n                zip_folder(zipf, item, out_dataset_pth)\n            else:  # Se è un file, aggiungilo direttamente\n                zipf.write(item)\n        else:\n            print(f\"Elemento non trovato: {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:52:13.978349Z","iopub.execute_input":"2024-12-29T02:52:13.978639Z","iopub.status.idle":"2024-12-29T02:52:32.004881Z","shell.execute_reply.started":"2024-12-29T02:52:13.978613Z","shell.execute_reply":"2024-12-29T02:52:32.004053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"# Carica il dataset dal JSON\n\ndata = load_json(in_actproposals_json)\n\n# Converti in DataFrame per una gestione più comoda\ndf = pd.DataFrame(data)\n\n# Estrai il nome del file dal campo 'saved_path'\ndf[\"file_name\"] = df[\"saved_path\"].apply(lambda x: os.path.basename(x))\n\n# Aggiungi il percorso base al campo 'saved_path'\ndf[\"saved_path\"] = df[\"file_name\"].apply(lambda x: str(act_reg_folder / x))\n\n# Estrai i dati e le etichette\nX = df.index  # Indici delle righe\ny = df[\"category_id\"]  # Etichetta per stratificazione\n\n# Step 1: Train + Val/Test\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Step 2: Val/Test split\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n)\n\n# Creazione dei dataset finali\ntrain_data = df.loc[X_train]\nval_data = df.loc[X_val]\ntest_data = df.loc[X_test]\n\n# Salva i dataset in nuovi file JSON\ntrain_data.to_json(\"train.json\", orient=\"records\", lines=False)\nval_data.to_json(\"val.json\", orient=\"records\", lines=False)\ntest_data.to_json(\"test.json\", orient=\"records\", lines=False)\n\nprint(\"Splitting completato. File salvati: train.json, val.json, test.json.\")\n\n# Percentuale dei dati per ciascun set\ntotal_data = len(df)\nprint(\"\\nPercentuale dei dati per ciascun set:\")\nprint(f\"Train: {len(train_data) / total_data:.2%}\")\nprint(f\"Validation: {len(val_data) / total_data:.2%}\")\nprint(f\"Test: {len(test_data) / total_data:.2%}\")\n\n# Distribuzione delle classi per ciascun set\nprint(\"\\nDistribuzione delle classi:\")\ntrain_class_dist = train_data[\"category_id\"].value_counts(normalize=True) * 100\nval_class_dist = val_data[\"category_id\"].value_counts(normalize=True) * 100\ntest_class_dist = test_data[\"category_id\"].value_counts(normalize=True) * 100\n\nprint(\"Train set:\")\nprint(train_class_dist.sort_index())\n\nprint(\"\\nValidation set:\")\nprint(val_class_dist.sort_index())\n\nprint(\"\\nTest set:\")\nprint(test_class_dist.sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:15:45.653113Z","iopub.execute_input":"2024-12-29T18:15:45.654045Z","iopub.status.idle":"2024-12-29T18:15:46.446675Z","shell.execute_reply.started":"2024-12-29T18:15:45.653985Z","shell.execute_reply":"2024-12-29T18:15:46.445717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = load_json(\"train.json\")\n\n# Visualizza 3 immagini a caso dal train set\nfig, axes = plt.subplots(1, 3, figsize=(20, 10))\nfor i, ax in enumerate(axes):\n    sample_entry = random.choice(train_data)\n    image_path = sample_entry[\"saved_path\"]\n    title = f\"Category ID: {sample_entry['category_id']}\"\n    visualize_image_with_bbox(image_path, [0,0,0,0], title, ax)\n\nplt.tight_layout()\nplt.show()\n\nval_data = load_json(\"val.json\")\n\n# Visualizza 3 immagini a caso dal train set\nfig, axes = plt.subplots(1, 3, figsize=(20, 10))\nfor i, ax in enumerate(axes):\n    sample_entry = random.choice(val_data)\n    image_path = sample_entry[\"saved_path\"]\n    title = f\"Category ID: {sample_entry['category_id']}\"\n    visualize_image_with_bbox(image_path, [0,0,0,0], title, ax)\n\nplt.tight_layout()\nplt.show()\n\ntest_data = load_json(\"test.json\")\n\n# Visualizza 3 immagini a caso dal train set\nfig, axes = plt.subplots(1, 3, figsize=(20, 10))\nfor i, ax in enumerate(axes):\n    sample_entry = random.choice(test_data)\n    image_path = sample_entry[\"saved_path\"]\n    title = f\"Category ID: {sample_entry['category_id']}\"\n    visualize_image_with_bbox(image_path, [0,0,0,0], title, ax)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:09:14.036666Z","iopub.execute_input":"2024-12-29T03:09:14.037010Z","iopub.status.idle":"2024-12-29T03:09:16.612852Z","shell.execute_reply.started":"2024-12-29T03:09:14.036979Z","shell.execute_reply":"2024-12-29T03:09:16.611983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        \"\"\"\n        Inizializza il dataset.\n\n        :param json_file: Percorso del file JSON contenente le informazioni sulle regioni.\n        :param transform: Trasformazioni da applicare alle immagini. Se non fornito, vengono usate trasformazioni di default.\n        \"\"\"\n        # Carica il file JSON\n        self.data = load_json(json_file)\n        \n        # Trasformazioni di default se non vengono fornite\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),         # Ridimensiona l'immagine a 224x224\n            transforms.ToTensor(),                  # Converte l'immagine in un tensore\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizzazione per i modelli pre-addestrati\n        ])  \n\n    def __len__(self):\n        \"\"\"Restituisce il numero totale di immagini/proposte nel dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Restituisce un esempio (immagine e etichetta) per l'addestramento.\"\"\"\n        # Carica l'esempio dal file JSON\n        sample = self.data[idx]\n        \n        # Carica l'immagine\n        image = Image.open(sample[\"saved_path\"]).convert(\"RGB\")\n        \n        # Etichetta della categoria\n        label = sample[\"category_id\"]  # Categoria della proposta\n\n        # Applica le trasformazioni\n        image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:07:02.634207Z","iopub.execute_input":"2024-12-30T12:07:02.635055Z","iopub.status.idle":"2024-12-30T12:07:02.641343Z","shell.execute_reply.started":"2024-12-30T12:07:02.635020Z","shell.execute_reply":"2024-12-30T12:07:02.640428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ds = CustomDataset(test_path)\ntrain_ds = CustomDataset(train_path)\nval_ds = CustomDataset(val_path)\n\nTrainLoader = DataLoader(train_ds, batch_size=32, shuffle=True)\nValLoader = DataLoader(val_ds, batch_size=32, shuffle=False)\nTestLoader = DataLoader(test_ds, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:15:54.436480Z","iopub.execute_input":"2024-12-29T18:15:54.436855Z","iopub.status.idle":"2024-12-29T18:15:54.553912Z","shell.execute_reply.started":"2024-12-29T18:15:54.436823Z","shell.execute_reply":"2024-12-29T18:15:54.553207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(AlexNet, self).__init__()\n        self._output_num = num_classes\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n     \n        self.drop8 = nn.Dropout()\n        self.fn8 = nn.Linear(256 * 6 * 6, 4096)\n        self.active8 = nn.ReLU(inplace=True)\n        \n        self.drop9 = nn.Dropout()\n        self.fn9 = nn.Linear(4096, 4096)\n        self.active9 = nn.ReLU(inplace=True)\n        \n        self.fn10 = nn.Linear(4096, self._output_num)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.drop8(x)\n        x = self.fn8(x)\n        x = self.active8(x)\n\n        x = self.drop9(x)\n        x = self.fn9(x)\n        \n        feature = self.active9(x)  \n        final = self.fn10(feature)\n\n        return feature, final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T10:04:33.478950Z","iopub.execute_input":"2024-12-29T10:04:33.479307Z","iopub.status.idle":"2024-12-29T10:04:33.492348Z","shell.execute_reply.started":"2024-12-29T10:04:33.479276Z","shell.execute_reply":"2024-12-29T10:04:33.491384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 12 #11 classi + sfondo\nnet = AlexNet(num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T10:04:35.581384Z","iopub.execute_input":"2024-12-29T10:04:35.582026Z","iopub.status.idle":"2024-12-29T10:04:36.063292Z","shell.execute_reply.started":"2024-12-29T10:04:35.581974Z","shell.execute_reply":"2024-12-29T10:04:36.062323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(net)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:34:37.601695Z","iopub.execute_input":"2024-12-29T03:34:37.602591Z","iopub.status.idle":"2024-12-29T03:34:37.608048Z","shell.execute_reply.started":"2024-12-29T03:34:37.602528Z","shell.execute_reply":"2024-12-29T03:34:37.606853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_loss(train_losses, val_losses):\n    \"\"\"\n    Funzione per fare il plot della funzione di loss durante il training e la validazione.\n\n    :param train_losses: Lista delle perdite durante il training.\n    :param val_losses: Lista delle perdite durante la validazione.\n    \"\"\"\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", linestyle='-', marker='o')\n    plt.plot(epochs, val_losses, label=\"Validation Loss\", color=\"red\", linestyle='-', marker='x')\n    \n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T19:39:23.382361Z","iopub.execute_input":"2024-12-29T19:39:23.382720Z","iopub.status.idle":"2024-12-29T19:39:23.388496Z","shell.execute_reply.started":"2024-12-29T19:39:23.382688Z","shell.execute_reply":"2024-12-29T19:39:23.387748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(net, train_loader, val_loader, criterion, optimizer, device, epochs, path_min_loss, num_classes, clip_value=1.0):\n    min_val_loss = float('inf')\n    \n    # Liste per registrare le perdite durante il training e la validazione\n    train_losses = []\n    val_losses = []\n    \n    # Dizionario per salvare le feature\n    train_features = {}  # Chiave: epoch, Valore: lista di feature\n\n    # Aggiungi un learning rate scheduler per ridurre il learning rate quando la loss di validazione non migliora\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n\n    for epoch in range(epochs):\n        net.train()  # Modalità training\n        train_loss = 0.0\n        correct_train = 0\n        total_train = 0\n        epoch_features = []  # Per registrare le feature di ogni batch\n        class_probs = torch.zeros((len(train_loader.dataset), num_classes)).to(device)  # Probabilità per classe\n\n        # Barra di avanzamento per il training\n        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", leave=False)\n\n        for i, (images, labels) in enumerate(train_progress):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            features, outputs = net(images)\n\n            # Salva le feature per il batch corrente\n            epoch_features.append(features.detach().cpu().numpy())\n\n            # Calcolo della loss pesata\n            loss = criterion(outputs, labels)\n\n            # Predizione della classe con la probabilità massima\n            _, predicted = outputs.max(1)\n\n            # Calcola le probabilità per ciascuna classe (softmax)\n            probs = torch.softmax(outputs, dim=1)\n            class_probs[i * images.size(0):(i + 1) * images.size(0)] = probs\n\n            # Monitoraggio dei gradienti\n            optimizer.zero_grad()\n            loss.backward()\n\n            # Clipping dei gradienti\n            utils.clip_grad_norm_(net.parameters(), clip_value)\n\n            optimizer.step()\n\n            # Statistiche\n            train_loss += loss.item() * images.size(0)\n            total_train += labels.size(0)\n            correct_train += predicted.eq(labels).sum().item()\n\n            # Aggiorna la barra di avanzamento con la loss corrente\n            train_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_train / total_train)\n\n        # Salva le feature per l'epoca corrente\n        train_features[epoch] = epoch_features\n\n        avg_train_loss = train_loss / len(train_loader.dataset)\n        train_accuracy = 100. * correct_train / total_train\n        train_losses.append(avg_train_loss)\n\n        # Calcola la probabilità media per ogni classe\n        avg_class_probs = class_probs.mean(dim=0)\n        print(f\"Probabilità media per classe in epoca {epoch+1}: {avg_class_probs}\")\n\n        # Validazione\n        net.eval()  # Modalità validazione\n        val_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        # Barra di avanzamento per la validazione\n        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\", leave=False)\n\n        with torch.no_grad():\n            for images, labels in val_progress:\n                images, labels = images.to(device), labels.to(device)\n\n                # Forward pass\n                _, outputs = net(images)\n                \n                # Predizione della classe con la probabilità massima\n                _, predicted = outputs.max(1)\n\n                # Calcolo della loss\n                loss = criterion(outputs, labels)\n\n                # Statistiche\n                val_loss += loss.item() * images.size(0)\n                total_val += labels.size(0)\n                correct_val += predicted.eq(labels).sum().item()\n\n                # Aggiorna la barra di avanzamento con la loss e accuracy\n                val_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_val / total_val)\n\n        avg_val_loss = val_loss / len(val_loader.dataset)\n        val_accuracy = 100. * correct_val / total_val\n        val_losses.append(avg_val_loss)\n\n        # Salva il modello con la loss di validazione più bassa\n        if avg_val_loss < min_val_loss:\n            print(f\"Salvataggio del miglior modello: Val Loss migliorata da {min_val_loss:.4f} a {avg_val_loss:.4f}\")\n            min_val_loss = avg_val_loss\n            torch.save(net.state_dict(), path_min_loss)\n\n        # Aggiorna il learning rate in base alla loss di validazione\n        scheduler.step(avg_val_loss)\n\n        # Stampa statistiche per epoca\n        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n    print(\"Training completato!\")\n\n    # Chiamata alla funzione per tracciare il grafico\n    plot_loss(train_losses, val_losses)\n\n    return train_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:16:07.024988Z","iopub.execute_input":"2024-12-29T18:16:07.025307Z","iopub.status.idle":"2024-12-29T18:16:07.039041Z","shell.execute_reply.started":"2024-12-29T18:16:07.025281Z","shell.execute_reply":"2024-12-29T18:16:07.038159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(net, test_loader, criterion, device, path_min_loss, json_path, original_img_path, reference_json_path):\n    \"\"\"\n    Funzione per testare il modello e visualizzare i risultati su immagini originali.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    :param json_path: Percorso al file JSON contenente i dettagli delle immagini del test set.\n    :param original_img_path: Percorso al folder delle immagini originali.\n    :param reference_json_path: Percorso al file JSON contenente le informazioni delle immagini originali.\n    \"\"\"\n    # Carica il miglior modello salvato (con il parametro weights_only=True per evitare il warning)\n    net.load_state_dict(torch.load(path_min_loss, map_location=device))\n    net.eval()\n\n    test_loss = 0.0\n    correct_test = 0\n    total_test = 0\n\n    all_labels = []\n    all_predictions = []\n\n    # Carica i file JSON\n    with open(json_path, 'r') as f:\n        test_data = json.load(f)\n\n    with open(reference_json_path, 'r') as f:\n        reference_data = json.load(f)\n\n    # Crea un dizionario per una ricerca rapida delle immagini originali\n    id_to_filename = {img['id']: img['file_name'] for img in reference_data['images']}\n\n    # Crea un dizionario per raggruppare i bounding box per image_id\n    image_id_to_bboxes = {}\n    for image_info in test_data:\n        image_id = image_info['image_id']\n        region_bbox = image_info['region_bbox']\n        if image_id not in image_id_to_bboxes:\n            image_id_to_bboxes[image_id] = []\n        image_id_to_bboxes[image_id].append(region_bbox)\n\n    with torch.no_grad():\n        for idx, (images, labels) in enumerate(test_loader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            test_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_test += labels.size(0)\n            correct_test += predicted.eq(labels).sum().item()\n\n            # Salva tutte le etichette e predizioni\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n            # Mostra alcuni esempi\n            if idx < 5:  # Mostra i primi 5 batch\n                for i in range(min(len(images), 3)):  # Mostra fino a 3 immagini per batch\n                    image_info = test_data[idx * len(images) + i]  # Recupera info immagine dal JSON\n                    image_id = image_info['image_id']\n\n                    # Trova il file_name usando l'image_id\n                    file_name = id_to_filename.get(image_id)\n                    if not file_name:\n                        print(f\"Immagine con ID {image_id} non trovata nel reference JSON.\")\n                        continue\n\n                    # Percorso dell'immagine originale\n                    img_path = os.path.join(original_img_path, file_name)\n\n                    # Carica l'immagine originale\n                    img = cv2.imread(img_path)\n                    if img is None:\n                        print(f\"Immagine non trovata: {img_path}\")\n                        continue\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n                    # Disegna i bounding box per le etichette reali (blu)\n                    bboxes_real = image_id_to_bboxes.get(image_id, [])\n                    for bbox in bboxes_real:\n                        xmin, ymin, xmax, ymax = bbox\n                        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)  # Blu per reale\n\n                    # Mostra l'immagine con i bounding box (senza le etichette scritte nell'immagine)\n                    plt.figure(figsize=(6, 6))\n                    plt.imshow(img)\n                    plt.axis('off')\n                    plt.show()\n\n                    # Ora stampa le etichette predette e reali sotto l'immagine\n                    print(f\"Predizioni vs Realtà per l'immagine {file_name}:\")\n                    for j, bbox in enumerate(bboxes_real):\n                        pred_label = predicted[i].item()\n                        # Stampa i valori predetti e reali\n                        print(f\"  Bounding Box {j + 1}: Predetto: {pred_label}, Reale: {labels[i].item()}\")\n\n    avg_test_loss = test_loss / len(test_loader.dataset)\n    test_accuracy = 100. * correct_test / total_test\n\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n\n    # Calcola e visualizza la matrice di confusione\n    cm = confusion_matrix(all_labels, all_predictions)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizza per riga\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:16:09.743225Z","iopub.execute_input":"2024-12-29T18:16:09.743535Z","iopub.status.idle":"2024-12-29T18:16:09.757098Z","shell.execute_reply.started":"2024-12-29T18:16:09.743507Z","shell.execute_reply":"2024-12-29T18:16:09.756312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.001\nepochs = 3\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\nnet = AlexNet(num_classes)\ndevice = torch.device(\"cuda\")\nnet = net.to(device)\n\ncriterion = criterion.to(device)\n\npath_min_loss = '/kaggle/working/AlexNet.pth'\n\ntrain_features = train_model(\n    net=net,\n    train_loader=TrainLoader,\n    val_loader=ValLoader,\n    criterion=criterion,\n    optimizer=optimizer,\n    device=device,\n    epochs=epochs,\n    path_min_loss=path_min_loss,\n    num_classes = num_classes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:37:19.459326Z","iopub.execute_input":"2024-12-29T03:37:19.459749Z","iopub.status.idle":"2024-12-29T03:43:23.969325Z","shell.execute_reply.started":"2024-12-29T03:37:19.459711Z","shell.execute_reply":"2024-12-29T03:43:23.968592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\n    net=net,\n    test_loader=TestLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=path_min_loss,\n    json_path = test_path,\n    original_img_path = img_fldr,\n    reference_json_path = in_new_coco_json_pth\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:46:41.540509Z","iopub.execute_input":"2024-12-29T03:46:41.540960Z","iopub.status.idle":"2024-12-29T03:47:08.624853Z","shell.execute_reply.started":"2024-12-29T03:46:41.540923Z","shell.execute_reply":"2024-12-29T03:47:08.623508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Numero di classi\nnum_classes = 12\n\n# Modello pre-addestrato ResNet50\nresnet50 = models.resnet50(pretrained=False)\n\n# Modifica l'ultimo layer completamente connesso per il numero di classi\nin_features = resnet50.fc.in_features  # Estrai le caratteristiche in input dell'ultimo layer\nresnet50.fc = nn.Linear(in_features, num_classes)  # Nuovo layer di classificazione\n\n# Passa il modello alla GPU (se disponibile)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresnet50 = resnet50.to(device)\n\n# Modifica il forward per restituire sia le feature che le predizioni finali\nclass ResNet50FeatureExtractor(nn.Module):\n    def __init__(self, model):\n        super(ResNet50FeatureExtractor, self).__init__()\n        self.resnet = model\n        # Rimuove l'ultimo layer\n        self.features = nn.Sequential(*list(self.resnet.children())[:-1])  # Rimuove il Fully Connected finale\n\n    def forward(self, x):\n        # Ottieni le feature intermedie e poi la predizione finale\n        features = self.features(x)\n        features = features.view(features.size(0), -1)  # Flattening per la classificazione\n        outputs = self.resnet.fc(features)\n        return features, outputs\n\n# Crea il nuovo modello con estrazione delle feature\nmodel = ResNet50FeatureExtractor(resnet50)\nmodel = model.to(device)\n\n# Funzione di perdita (loss) e ottimizzatore\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n\n# Numero di epoche\nepochs = 15\n\n# Funzione di training\ntrain_features = train_model(\n    net=model,\n    train_loader=TrainLoader,\n    val_loader=ValLoader,\n    criterion=criterion,\n    optimizer=optimizer,\n    device=device,\n    epochs=epochs,\n    path_min_loss=path_min_loss,\n    num_classes=num_classes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T20:40:30.683613Z","iopub.execute_input":"2024-12-29T20:40:30.683955Z","iopub.status.idle":"2024-12-29T22:01:30.627457Z","shell.execute_reply.started":"2024-12-29T20:40:30.683924Z","shell.execute_reply":"2024-12-29T22:01:30.626132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save(train_features_path, train_features)  # Salva in formato .npy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T22:01:30.665139Z","iopub.execute_input":"2024-12-29T22:01:30.665383Z","iopub.status.idle":"2024-12-29T22:01:37.093935Z","shell.execute_reply.started":"2024-12-29T22:01:30.665360Z","shell.execute_reply":"2024-12-29T22:01:37.092662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\n    net=model,\n    test_loader=TestLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=path_min_loss,\n    json_path = test_path,\n    original_img_path = img_fldr,\n    reference_json_path = in_new_coco_json_pth\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T22:01:37.096573Z","iopub.execute_input":"2024-12-29T22:01:37.096974Z","iopub.status.idle":"2024-12-29T22:02:03.227881Z","shell.execute_reply.started":"2024-12-29T22:01:37.096928Z","shell.execute_reply":"2024-12-29T22:02:03.226924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Box Regressor","metadata":{}},{"cell_type":"code","source":"def test_model_regressor(net, test_loader, criterion, device, path_min_loss, pred_json_path):\n    \"\"\"\n    Funzione per testare il modello, sostituire il category_id con quello predetto e visualizzare i risultati su immagini originali.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    :param json_path: Percorso al file JSON contenente i dettagli delle immagini del test set.\n    :param pred_json_path: Percorso al file JSON da modificare con i category_id predetti.\n    \"\"\"\n    # Carica il miglior modello salvato\n    net.load_state_dict(torch.load(path_min_loss, map_location=device))\n    net.eval()\n\n    all_labels = []\n    all_predictions = []\n\n    # Carica il JSON con le informazioni delle immagini da modificare\n    with open(pred_json_path, 'r') as f:\n        pred_data = json.load(f)\n\n    with torch.no_grad():\n        for idx, (images, labels) in enumerate(test_loader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            _, predicted = outputs.max(1)\n\n            # Salva tutte le etichette e predizioni\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n            # Modifica il JSON con le predizioni\n            for i in range(len(images)):\n                image_info = pred_data[idx * len(images) + i]  # Recupera info immagine dal JSON\n                image_id = image_info['image_id']\n\n                # Trova l'elemento nel pred_json e sostituisci il category_id con quello predetto\n                for entry in pred_data:\n                    if entry['image_id'] == image_id:\n                        entry['category_id'] = int(predicted[i].item())\n\n    # Salva il JSON modificato\n    with open(pred_json_path, 'w') as f:\n        json.dump(pred_data, f, indent=4)\n\n    # Calcola la test accuracy\n    test_accuracy = 100. * sum(np.array(all_labels) == np.array(all_predictions)) / len(all_labels)\n    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n\n    # Calcola e visualizza la matrice di confusione\n    cm = confusion_matrix(all_labels, all_predictions)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizza per riga\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:07:17.802390Z","iopub.execute_input":"2024-12-30T12:07:17.802763Z","iopub.status.idle":"2024-12-30T12:07:17.812545Z","shell.execute_reply.started":"2024-12-30T12:07:17.802733Z","shell.execute_reply":"2024-12-30T12:07:17.811679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carica i dati JSON per val e test\nval_json = load_json(val_path)\ntest_json = load_json(test_path)\nreg_json = val_json + test_json\n\n# Salva il JSON unito in un nuovo file\nwith open(reg_path, 'w') as f_merged:\n    json.dump(reg_json, f_merged, indent=4)\n    \nreg_ds = CustomDataset(reg_path)\nRegLoader = DataLoader(reg_ds, batch_size=32, shuffle=False)\n\n# Chiama la funzione di test\ntest_model_regressor(\n    net=model,\n    test_loader=RegLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=resnet_min_loss_path,\n    pred_json_path=reg_path\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-30T12:07:25.068006Z","iopub.execute_input":"2024-12-30T12:07:25.068380Z","iopub.status.idle":"2024-12-30T12:07:25.569564Z","shell.execute_reply.started":"2024-12-30T12:07:25.068351Z","shell.execute_reply":"2024-12-30T12:07:25.568437Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Box Regressor","metadata":{}},{"cell_type":"code","source":"class BoundingBoxRegressor:\n    def __init__(self, lambda_reg=1000):  \n        self.lambda_reg = lambda_reg\n        self.category_weights = defaultdict(dict)  # Salva pesi per ogni categoria\n\n    def parse_json(self, json_data):\n        \"\"\"\n        Estrae proposals, ground truths e category_id da un JSON strutturato,\n        ignorando le box con category_id = 0 per il training, ma mantenendole per il JSON finale.\n        \"\"\"\n        proposals = []\n        ground_truths = []\n        categories = []\n        filtered_data = []\n        background_data = []  # Dati per la categoria 0 (background)\n    \n        for item in json_data:\n            category_id = item[\"category_id\"]\n            \n            # Se la categoria è 0, salva come background\n            if category_id == 0:\n                background_data.append(item)\n                continue\n            \n            # Verifica validità di original_bbox e region_bbox\n            original_bbox = item.get(\"original_bbox\", None)\n            region_bbox = item.get(\"region_bbox\", None)\n            \n            if (\n                original_bbox and len(original_bbox) == 4 and \n                region_bbox and len(region_bbox) == 4\n            ):\n                # Converto ground truth (original_bbox) da (x_min, y_min, x_max, y_max) a (x, y, width, height)\n                x_min, y_min, x_max, y_max = original_bbox\n                width = x_max - x_min\n                height = y_max - y_min\n                ground_truths.append([x_min, y_min, width, height])\n\n                # Converto proposal (region_bbox) da (x_min, y_min, x_max, y_max) a (x, y, width, height)\n                x_min, y_min, x_max, y_max = region_bbox\n                width = x_max - x_min\n                height = y_max - y_min\n                proposals.append([x_min, y_min, width, height])\n            else:\n                continue\n            \n            categories.append(category_id)\n            filtered_data.append(item)\n\n        return np.array(proposals), np.array(ground_truths), np.array(categories), filtered_data, background_data\n        \n    def train(self, json_data):\n        \"\"\"\n        Addestra il regressore usando i dati JSON forniti.\n        \"\"\"\n        proposals, ground_truths, categories, _, _ = self.parse_json(json_data)\n        unique_categories = np.unique(categories)\n\n        for category in tqdm(unique_categories, desc=\"Training Categories\"):\n            # Filtra le proposte e i ground truth per la categoria\n            cat_indices = np.where(categories == category)[0]\n            cat_proposals = proposals[cat_indices]\n            cat_ground_truths = ground_truths[cat_indices]\n\n            if len(cat_proposals) == 0:\n                continue  # Salta se non ci sono dati per la categoria\n\n            N = len(cat_proposals)\n            targets = np.zeros((N, 4))  # tx, ty, tw, th\n\n            # Calcola i target di regressione\n            for i, (P, G) in enumerate(zip(cat_proposals, cat_ground_truths)):\n                Px, Py, Pw, Ph = P\n                Gx, Gy, Gw, Gh = G\n\n                targets[i, 0] = (Gx - Px) / Pw  # tx\n                targets[i, 1] = (Gy - Py) / Ph  # ty\n                targets[i, 2] = np.log(Gw / Pw)  # tw\n                targets[i, 3] = np.log(Gh / Ph)  # th\n\n            # Risoluzione dei pesi per ogni coordinata\n            for k, label in enumerate(['x', 'y', 'w', 'h']):\n                target_k = targets[:, k]\n                A = np.dot(cat_proposals.T, cat_proposals) + self.lambda_reg * np.eye(4)\n                b = np.dot(cat_proposals.T, target_k)\n                self.category_weights[category][label] = np.linalg.solve(A, b)\n\n    def predict(self, json_data, output_path):\n        \"\"\"\n        Predice i bounding box corretti per ogni categoria dato un insieme di dati JSON.\n        Salva il risultato in un nuovo file JSON.\n        \"\"\"\n        proposals, _, categories, filtered_data, background_data = self.parse_json(json_data)\n        unique_categories = np.unique(categories)\n\n        for category in tqdm(unique_categories, desc=\"Predicting Categories\"):\n            cat_indices = np.where(categories == category)[0]\n            cat_proposals = proposals[cat_indices]\n            \n            if len(cat_proposals) == 0:\n                continue  # Salta se non ci sono proposte per la categoria\n            \n            for rel_idx, abs_idx in enumerate(cat_indices):\n                Px, Py, Pw, Ph = cat_proposals[rel_idx]\n\n                dx = np.dot(cat_proposals[rel_idx], self.category_weights[category]['x'])\n                dy = np.dot(cat_proposals[rel_idx], self.category_weights[category]['y'])\n                dw = np.dot(cat_proposals[rel_idx], self.category_weights[category]['w'])\n                dh = np.dot(cat_proposals[rel_idx], self.category_weights[category]['h'])\n\n                # Predizione del bounding box corretto (nel formato x, y, width, height)\n                Gx_hat = Pw * dx + Px\n                Gy_hat = Ph * dy + Py\n                Gw_hat = Pw * np.exp(dw)\n                Gh_hat = Ph * np.exp(dh)\n\n                # Conversione delle coordinate predette in formato (x_min, y_min, x_max, y_max)\n                x_min = Gx_hat\n                y_min = Gy_hat\n                x_max = Gx_hat + Gw_hat\n                y_max = Gy_hat + Gh_hat\n\n                # Aggiorna il JSON con il nuovo bounding box nel formato (x_min, y_min, x_max, y_max)\n                filtered_data[abs_idx]['region_bbox'] = [x_min, y_min, x_max, y_max]\n\n        # Riaggiungi i dati della categoria 0 (background)\n        filtered_data.extend(background_data)\n\n        # Salva il risultato nel file JSON di output\n        with open(output_path, 'w') as outfile:\n            json.dump(filtered_data, outfile, indent=4)\n\n        print(f\"File salvato in: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:17:04.480721Z","iopub.execute_input":"2024-12-31T12:17:04.481098Z","iopub.status.idle":"2024-12-31T12:17:04.500823Z","shell.execute_reply.started":"2024-12-31T12:17:04.481062Z","shell.execute_reply":"2024-12-31T12:17:04.499496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training\ntrain_json = load_json(in_train_path)\nregressor = BoundingBoxRegressor()\nregressor.train(train_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:17:07.382700Z","iopub.execute_input":"2024-12-31T12:17:07.383712Z","iopub.status.idle":"2024-12-31T12:17:07.815530Z","shell.execute_reply.started":"2024-12-31T12:17:07.383638Z","shell.execute_reply":"2024-12-31T12:17:07.811499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predizione\nreg_json = load_json(in_reg_path)\nregressor.predict(reg_json, out_reg_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:17:09.859154Z","iopub.execute_input":"2024-12-31T12:17:09.859530Z","iopub.status.idle":"2024-12-31T12:17:10.082816Z","shell.execute_reply.started":"2024-12-31T12:17:09.859496Z","shell.execute_reply":"2024-12-31T12:17:10.081695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 12\n\nnew_reg_json = load_json(out_reg_path)\nmodel = resnet_min_loss_path\n\nreg_ds = CustomDataset(out_reg_path)\nRegLoader = DataLoader(reg_ds, batch_size=32, shuffle=False)\n\n# Modello pre-addestrato ResNet50\nresnet50 = models.resnet50(pretrained=False)\n\n# Modifica l'ultimo layer completamente connesso per il numero di classi\nin_features = resnet50.fc.in_features  # Estrai le caratteristiche in input dell'ultimo layer\nresnet50.fc = nn.Linear(in_features, num_classes)  # Nuovo layer di classificazione\n\n# Passa il modello alla GPU (se disponibile)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresnet50 = resnet50.to(device)\n\n# Modifica il forward per restituire sia le feature che le predizioni finali\nclass ResNet50FeatureExtractor(nn.Module):\n    def __init__(self, model):\n        super(ResNet50FeatureExtractor, self).__init__()\n        self.resnet = model\n        # Rimuove l'ultimo layer\n        self.features = nn.Sequential(*list(self.resnet.children())[:-1])  # Rimuove il Fully Connected finale\n\n    def forward(self, x):\n        # Ottieni le feature intermedie e poi la predizione finale\n        features = self.features(x)\n        features = features.view(features.size(0), -1)  # Flattening per la classificazione\n        outputs = self.resnet.fc(features)\n        return features, outputs\n\n# Crea il nuovo modello con estrazione delle feature\nmodel = ResNet50FeatureExtractor(resnet50)\nmodel = model.to(device)\n\n# Funzione di perdita (loss) e ottimizzatore\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n\n# Chiama la funzione di test\ntest_model_regressor(\n    net=model,\n    test_loader=RegLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=resnet_min_loss_path,\n    pred_json_path=out_reg_path\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:07:59.917647Z","iopub.execute_input":"2024-12-30T12:07:59.918026Z","iopub.status.idle":"2024-12-30T12:09:12.281425Z","shell.execute_reply.started":"2024-12-30T12:07:59.917996Z","shell.execute_reply":"2024-12-30T12:09:12.280583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_region_boxes(image_path, regions1, regions2, labels1, labels2, image_title, title1, title2):\n    \"\"\"\n    Visualizza un'immagine con due set di region box e relative etichette affiancati.\n\n    Parameters:\n        image_path (str): Il percorso dell'immagine.\n        regions1 (list): Lista di regioni per la prima immagine ([x_min, y_min, x_max, y_max]).\n        regions2 (list): Lista di regioni per la seconda immagine ([x_min, y_min, x_max, y_max]).\n        labels1 (list): Lista di category_id per la prima immagine.\n        labels2 (list): Lista di category_id per la seconda immagine.\n        image_title (str): Titolo generale per la coppia di immagini (nome dell'immagine).\n        title1 (str): Sottotitolo per la prima immagine.\n        title2 (str): Sottotitolo per la seconda immagine.\n    \"\"\"\n    # Apri l'immagine\n    img = Image.open(image_path)\n    \n    # Crea una figura con due subplot affiancati\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    fig.suptitle(image_title, fontsize=16)  # Titolo generale per la coppia\n    \n    # Mostra la prima immagine con i box e le etichette\n    axes[0].imshow(img)\n    for region, label in zip(regions1, labels1):\n        x_min, y_min, x_max, y_max = region\n        width = x_max - x_min\n        height = y_max - y_min\n        rect = plt.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        axes[0].add_patch(rect)\n        axes[0].text(x_min, y_min - 5, str(label), color='r', fontsize=8, ha='left', backgroundcolor='white')\n    axes[0].set_title(title1, fontsize=12)\n    axes[0].axis('off')\n    \n    # Mostra la seconda immagine con i box e le etichette\n    axes[1].imshow(img)\n    for region, label in zip(regions2, labels2):\n        x_min, y_min, x_max, y_max = region\n        width = x_max - x_min\n        height = y_max - y_min\n        rect = plt.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        axes[1].add_patch(rect)\n        axes[1].text(x_min, y_min - 5, str(label), color='r', fontsize=8, ha='left', backgroundcolor='white')\n    axes[1].set_title(title2, fontsize=12)\n    axes[1].axis('off')\n    \n    # Mostra la figura\n    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adatta layout lasciando spazio per il titolo generale\n    plt.show()\n\ndef visualize_boxes(reg_file, new_reg_file, inactproposals_file, img_folder, selected_files=None):\n    \"\"\"\n    Visualizza immagini affiancate con region box sovrapposti e relative etichette da due file JSON.\n\n    Parameters:\n        reg_file (str): Percorso del file reg.json.\n        new_reg_file (str): Percorso del file new_reg.json.\n        inactproposals_file (str): Percorso del file inactproposals_json.\n        img_folder (str): Percorso della cartella contenente le immagini.\n        selected_files (list, optional): Lista di nomi di file immagine da visualizzare. Se None, usa tutte le immagini.\n    \"\"\"\n    # Carica i dati dai file JSON\n    with open(reg_file, 'r') as f:\n        reg_data = json.load(f)\n\n    with open(new_reg_file, 'r') as f:\n        new_reg_data = json.load(f)\n\n    with open(inactproposals_file, 'r') as f:\n        inactproposals_data = json.load(f)\n\n    # Mappa i file_name agli image_id usando inactproposals_json\n    file_to_id = {item['file_name']: item['image_id'] for item in inactproposals_data}\n\n    # Filtra le immagini se `selected_files` è specificato\n    files_to_visualize = selected_files if selected_files else file_to_id.keys()\n\n    # Itera sulle immagini selezionate\n    for file_name in files_to_visualize:\n        if file_name not in file_to_id:\n            print(f\"File {file_name} non trovato in inactproposals_json.\")\n            continue\n\n        image_id = file_to_id[file_name]\n\n        # Trova tutti i region box e i category_id associati a questo image_id nei due file JSON\n        reg_bboxes = []\n        reg_labels = []\n        for item in reg_data:\n            if item['image_id'] == image_id:\n                reg_bboxes.append(item['region_bbox'])\n                reg_labels.append(item['category_id'])  # Usa il category_id come label\n\n        new_reg_bboxes = []\n        new_reg_labels = []\n        for item in new_reg_data:\n            if item['image_id'] == image_id:\n                new_reg_bboxes.append(item['region_bbox'])\n                new_reg_labels.append(item['category_id'])  # Usa il category_id come label\n\n        # Costruisci il percorso completo dell'immagine\n        image_path = os.path.join(img_folder, file_name)\n\n        if not os.path.exists(image_path):\n            print(f\"L'immagine {image_path} non esiste.\")\n            continue\n\n        # Visualizza l'immagine con i box affiancati e i category_id\n        plot_region_boxes(\n            image_path, \n            reg_bboxes, \n            new_reg_bboxes, \n            reg_labels, \n            new_reg_labels, \n            f\"Immagine: {file_name}\",  # Titolo generale\n            \"Boxes prima del Box Regressor\",  # Sottotitolo immagine 1\n            \"Boxes dopo il Box Regressor\"    # Sottotitolo immagine 2\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:17:14.466170Z","iopub.execute_input":"2024-12-31T12:17:14.466516Z","iopub.status.idle":"2024-12-31T12:17:14.782897Z","shell.execute_reply.started":"2024-12-31T12:17:14.466484Z","shell.execute_reply":"2024-12-31T12:17:14.781532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_boxes(in_reg_path, out_reg_path, in_actproposals_json, img_fldr, selected_files=[\"img_1896_0_2560.jpg\", \"img_2008_320_640.jpg\", \"img_1114_1280_960.jpg\", \"img_322_1280_2240.jpg\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:17:18.443202Z","iopub.execute_input":"2024-12-31T12:17:18.443640Z","iopub.status.idle":"2024-12-31T12:17:21.488924Z","shell.execute_reply.started":"2024-12-31T12:17:18.443597Z","shell.execute_reply":"2024-12-31T12:17:21.487810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_top_images(json_data, top_n=4):\n    # Un dizionario per contare il numero di bbox per ciascun image_id per categoria\n    category_image_bbox_count = defaultdict(lambda: defaultdict(int))\n\n    # Itera attraverso i dati JSON e conta i bbox per ciascun image_id e category_id\n    for entry in json_data:\n        image_id = entry['image_id']\n        category_id = entry['category_id']  # Supponiamo che la categoria sia in 'category_id'\n        category_image_bbox_count[category_id][image_id] += 1\n\n    # Un dizionario per memorizzare i risultati finali\n    top_images_by_category = {}\n\n    # Per ogni categoria, ordina le immagini in base al numero di bbox (in ordine decrescente)\n    for category_id, image_count in category_image_bbox_count.items():\n        sorted_images = sorted(image_count.items(), key=lambda x: x[1], reverse=True)\n        top_images_by_category[category_id] = sorted_images[:top_n]\n\n    return top_images_by_category\n\n# Carica il file JSON\nwith open(in_reg_path, 'r') as f:\n    data = json.load(f)\n\n# Chiamata alla funzione per trovare le immagini con più region\ntop_images = find_top_images(data)\n\n# Stampa i risultati per ogni categoria\nif top_images:\n    for category_id, top_images_list in top_images.items():\n        print(f\"\\nCategoria ID: {category_id}\")\n        for rank, (image_id, bbox_count) in enumerate(top_images_list, start=1):\n            print(f\"{rank}. Image ID: {image_id}, Numero di bbox: {bbox_count}\")\nelse:\n    print(\"Nessuna immagine trovata.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:00:25.214957Z","iopub.execute_input":"2024-12-31T12:00:25.215374Z","iopub.status.idle":"2024-12-31T12:00:25.263020Z","shell.execute_reply.started":"2024-12-31T12:00:25.215337Z","shell.execute_reply":"2024-12-31T12:00:25.261989Z"}},"outputs":[],"execution_count":null}]}