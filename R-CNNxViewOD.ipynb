{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10118002,"sourceType":"datasetVersion","datasetId":6242793},{"sourceId":10140069,"sourceType":"datasetVersion","datasetId":6258380}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:13:45.263025Z","iopub.execute_input":"2024-12-08T14:13:45.263502Z","iopub.status.idle":"2024-12-08T14:13:58.459655Z","shell.execute_reply.started":"2024-12-08T14:13:45.263466Z","shell.execute_reply":"2024-12-08T14:13:58.458246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per l'ottimizzazione e la gestione delle dipendenze\n#import selectivesearch\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nOUT_COCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_IMAGE_FLDR_NM = 'images'\nOUT_CFG_FLDR_NM = 'YOLO_cfg'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\ncfg_fldr_pth = Path(f'/kaggle/input/our-xview-dataset/{OUT_CFG_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / OUT_COCO_JSON_NM\ntrain_txt_pth = cfg_fldr_pth / 'train.txt'\nval_txt_pth = cfg_fldr_pth / 'val.txt'\ntest_txt_pth = cfg_fldr_pth / 'test.txt'\n\n# PROPOSALS\nOUT_PROPOSALS_FLDR_NM = 'proposals'\nprop_fldr = Path(f'/kaggle/working/{OUT_PROPOSALS_FLDR_NM}')\nPROP_COCO_JSON_NM = 'proposals.json'\nproposals_json = out_dataset_pth / PROP_COCO_JSON_NM\nACTPROP_COCO_JSON_NM ='active_regions.json'\nactproposals_json = out_dataset_pth / ACTPROP_COCO_JSON_NM\n\n# ACTIVE REGIONS\nact_reg_path = Path('/kaggle/input/activeregion-xviewdataset')\nact_reg_folder = Path('/kaggle/input/activeregion-xviewdataset/activeregion-xview-dataset/proposals')\n\n#DATASET\ntrain_path = '/kaggle/working/train.json'\ntest_path = '/kaggle/working/test.json'\nval_path = '/kaggle/working/val.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)\nclean_output(prop_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:14:04.128657Z","iopub.execute_input":"2024-12-08T14:14:04.128974Z","iopub.status.idle":"2024-12-08T14:14:04.156606Z","shell.execute_reply.started":"2024-12-08T14:14:04.128944Z","shell.execute_reply":"2024-12-08T14:14:04.155365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\n\n# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:14:04.159468Z","iopub.execute_input":"2024-12-08T14:14:04.159809Z","iopub.status.idle":"2024-12-08T14:14:04.168078Z","shell.execute_reply.started":"2024-12-08T14:14:04.159777Z","shell.execute_reply":"2024-12-08T14:14:04.167016Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"markdown","source":"## Region Proposals Generation","metadata":{}},{"cell_type":"code","source":"# Funzione per elaborare una singola immagine\ndef process_single_image(image_data, img_fldr):\n    img_id = image_data['id']\n    img_name = image_data['file_name']\n    img_path = os.path.join(img_fldr, img_name)\n\n    if not os.path.exists(img_path):\n        raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    # Carica l'immagine usando opencv (in modalità RGB)\n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n    original_height, original_width, _ = image.shape\n\n    # Ridimensiona l'immagine per velocizzare la Selective Search\n    resized_image = cv2.resize(image, (original_width // 2, original_height // 2), interpolation=cv2.INTER_AREA)\n\n    # Genera le region proposals sulla versione ridotta\n    processed_proposals = generate_and_process_proposals(resized_image, original_width // 2, original_height // 2)\n\n    # Riscalare le coordinate delle proposte alla dimensione originale\n    scaled_proposals = [[x * 2, y * 2, x_max * 2, y_max * 2] for x, y, x_max, y_max in processed_proposals]\n\n    image_data = {\n        \"image_id\": img_id,\n        \"file_name\": img_name,\n        \"original_size\": [original_width, original_height],\n        \"proposals\": []\n    }\n\n    for i, proposal in enumerate(scaled_proposals):\n        x_min, y_min, x_max, y_max = proposal\n        image_data[\"proposals\"].append({\n            \"proposal_id\": i,\n            \"coordinates\": [x_min, y_min, x_max, y_max]\n        })\n\n    return image_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:14:04.169358Z","iopub.execute_input":"2024-12-08T14:14:04.169678Z","iopub.status.idle":"2024-12-08T14:14:04.180175Z","shell.execute_reply.started":"2024-12-08T14:14:04.169647Z","shell.execute_reply":"2024-12-08T14:14:04.179124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funzione per generare le region proposals con Selective Search\ndef generate_and_process_proposals(image, img_width, img_height):\n    img_np = np.array(image, dtype=np.uint8)\n\n    # Esegui la selective search con parametri ottimizzati\n    _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.8, min_size=20)\n\n    if len(regions) == 0:\n        print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n    processed_proposals = []\n\n    # Pre-filtraggio delle regioni\n    for region in regions:\n        x, y, w, h = region['rect']\n        area = w * h\n        if w >= 10 and h >= 10 and 10 <= area <= 0.8 * (img_width * img_height):\n            x_max, y_max = x + w, y + h\n            processed_proposals.append([x, y, x_max, y_max])\n\n    return processed_proposals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:14:04.181475Z","iopub.execute_input":"2024-12-08T14:14:04.181773Z","iopub.status.idle":"2024-12-08T14:14:04.195200Z","shell.execute_reply.started":"2024-12-08T14:14:04.181744Z","shell.execute_reply":"2024-12-08T14:14:04.194229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funzione per gestire i batch\ndef batch(iterable, n=1):\n    it = iter(iterable)\n    while True:\n        chunk = list(islice(it, n))\n        if not chunk:\n            break\n        yield chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:14:04.196501Z","iopub.execute_input":"2024-12-08T14:14:04.196821Z","iopub.status.idle":"2024-12-08T14:14:04.208449Z","shell.execute_reply.started":"2024-12-08T14:14:04.196792Z","shell.execute_reply":"2024-12-08T14:14:04.207409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_dataset_proposals(coco_json, img_fldr, output_dir, output_json):\n    os.makedirs(output_dir, exist_ok=True)\n    all_image_data = []\n\n    # Carica il file JSON di COCO\n    with open(coco_json, 'r') as f:\n        coco_data = json.load(f)\n\n    # Prepara il mapping delle annotazioni per le immagini\n    image_annotations_map = {}\n    for annotation in coco_data['annotations']:\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_map:\n            image_annotations_map[image_id] = []\n        image_annotations_map[image_id].append(annotation)\n\n    images_with_annotations = [\n        image_data for image_data in coco_data['images']\n        if image_data['id'] in image_annotations_map and len(image_annotations_map[image_data['id']]) > 0\n    ]\n\n    # Parametri per parallelizzazione e batch processing\n    max_workers = os.cpu_count() - 1\n    batch_size = 500\n    total_batches = len(images_with_annotations) // batch_size + (len(images_with_annotations) % batch_size > 0)\n\n    # Processa le immagini in batch con tqdm per monitorare il progresso dei batch\n    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n        for image_batch in batch(images_with_annotations, batch_size):\n            with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n                results = list(executor.map(process_single_image, image_batch, [img_fldr] * len(image_batch)))\n            all_image_data.extend(results)\n            pbar.update(1)  # Aggiorna la barra di progresso per ogni batch completato\n\n    # Salva il risultato in formato JSON usando orjson\n    with open(output_json, 'wb') as json_file:\n        json_file.write(orjson.dumps(all_image_data, option=orjson.OPT_INDENT_2))\n\n    print(f\"Creato file JSON con le region proposals: {output_json}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:14:04.209942Z","iopub.execute_input":"2024-12-08T14:14:04.210394Z","iopub.status.idle":"2024-12-08T14:14:04.226349Z","shell.execute_reply.started":"2024-12-08T14:14:04.210348Z","shell.execute_reply":"2024-12-08T14:14:04.225219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_dataset_proposals(coco_json_pth, img_fldr, prop_fldr, proposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:14:04.227797Z","iopub.execute_input":"2024-12-08T14:14:04.228233Z","iopub.status.idle":"2024-12-08T14:52:18.783165Z","shell.execute_reply.started":"2024-12-08T14:14:04.228186Z","shell.execute_reply":"2024-12-08T14:52:18.781481Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Positive Region Proposals","metadata":{}},{"cell_type":"code","source":"ignored_count = 0  # Contatore globale per le regioni ignorate\n\ndef get_iou(bb1, bb2):\n    global ignored_count  # Accedi alla variabile globale del contatore\n\n    try:\n        # Assicurati che le dimensioni siano corrette\n        assert bb1['x1'] < bb1['x2']\n        assert bb1['y1'] < bb1['y2']\n        assert bb2['x1'] < bb2['x2']\n        assert bb2['y1'] < bb2['y2']\n    except AssertionError:\n        # Se si verifica un errore, incrementa il contatore delle regioni ignorate\n        ignored_count += 1\n        return 0.0  # Restituisci 0.0 per l'IoU in caso di errore (nessuna sovrapposizione)\n\n    # Calcola le dimensioni dell'area comune tra i due box\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    # Se non c'è sovrapposizione, restituisci 0 come area di intersezione\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calcola l'area di intersezione\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calcola le aree individuali dei due bounding box\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n    \n    # Calcola l'area dell'unione\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    # Verifica che l'IoU sia nel range corretto\n    assert iou >= 0.0\n    assert iou <= 1.0\n\n    return iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:52:18.785215Z","iopub.execute_input":"2024-12-08T14:52:18.786160Z","iopub.status.idle":"2024-12-08T14:52:18.796909Z","shell.execute_reply.started":"2024-12-08T14:52:18.786053Z","shell.execute_reply":"2024-12-08T14:52:18.795644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assign_and_save_regions(region_json_path, bbox_json_path, image_dir, output_dir, output_json_path, iou_threshold=0.5):\n    \"\"\"Associa le regioni proposte ai bounding boxes, salva le regioni positive come immagini e crea un nuovo JSON con informazioni attivate.\"\"\"\n    \n    # Carica i file JSON\n    with open(region_json_path, 'r') as f:\n        regions = json.load(f)\n\n    with open(bbox_json_path, 'r') as f:\n        bboxes = json.load(f)\n    \n    # Crea un dizionario per cercare annotations per image_id\n    annotations_by_image = {}\n    for annot in bboxes[\"annotations\"]:\n        img_id = annot[\"image_id\"]\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        # Converte il campo bbox da stringa a lista di float e poi in formato [x1, y1, x2, y2]\n        bbox = json.loads(annot[\"bbox\"])  # Converte la stringa in una lista di numeri\n        x, y, w, h = bbox\n        bbox_converted = [x, y, x + w, y + h]  # Converti in [x1, y1, x2, y2]\n        annotations_by_image[img_id].append((torch.tensor(bbox_converted, dtype=torch.float32), annot[\"category_id\"]))\n    \n    # Crea un dizionario per mappare category_id ai nomi delle categorie\n    category_mapping = {cat_id: name for cat_id, name in enumerate(bboxes[\"categories\"])}\n    \n    # Crea la directory di output se non esiste\n    os.makedirs(output_dir, exist_ok=True)\n\n    counter = 0  # Contatore delle immagini salvate\n    \n    active_region_data = []  # Lista per i dati delle regioni attive\n\n    # Avvolgi il ciclo principale per ogni immagine con tqdm (una barra di progresso generale)\n    for image in tqdm(regions, desc=\"Elaborazione immagini\", total=len(regions)):\n        image_id = image[\"image_id\"]\n        file_name = image[\"file_name\"]\n        proposals = image[\"proposals\"]\n        \n        # Ottieni bounding boxes ground-truth e categorie per l'immagine corrente\n        gt_data = annotations_by_image.get(image_id, [])\n        if not gt_data:\n            # Se non ci sono bounding boxes ground-truth, salta l'immagine\n            continue\n        \n        gt_bboxes = [item[0] for item in gt_data]  # Bounding box ground truth\n        gt_categories = [item[1] for item in gt_data]  # Categorie ground truth\n        \n        # Trasforma proposals in una lista di dizionari compatibili con get_iou\n        proposal_coords = [{'x1': p[\"coordinates\"][0], 'y1': p[\"coordinates\"][1], \n                            'x2': p[\"coordinates\"][2], 'y2': p[\"coordinates\"][3]} \n                           for p in proposals]\n        \n        # Calcola la matrice IoU usando la funzione get_iou\n        iou_matrix = []\n        for proposal in proposal_coords:\n            iou_row = []\n            for gt_bbox in gt_bboxes:\n                # Ogni gt_bbox deve essere un dizionario simile a proposal\n                gt_dict = {'x1': gt_bbox[0].item(), 'y1': gt_bbox[1].item(), \n                           'x2': gt_bbox[2].item(), 'y2': gt_bbox[3].item()}\n                iou = get_iou(proposal, gt_dict)\n                iou_row.append(iou)\n            iou_matrix.append(iou_row)\n\n        # Verifica se la matrice IoU è vuota (ad esempio, se non ci sono bounding box ground-truth per un'immagine, allora gt_bboxes è vuoto)\n        if not iou_matrix:\n            continue\n        \n        iou_matrix = torch.tensor(iou_matrix)\n\n        # Identifica le regioni positive (IoU >= soglia)\n        max_ious, indices = torch.max(iou_matrix, dim=1)\n        positive_indices = torch.nonzero(max_ious >= iou_threshold).squeeze(1)\n        \n        # Carica l'immagine originale\n        image_path = os.path.join(image_dir, file_name)\n        original_image = cv2.imread(image_path)\n        if original_image is None:\n            print(f\"Immagine non trovata: {image_path}\")\n            continue\n        \n        # Avvolgi il ciclo per ogni proposta positiva senza tqdm (non serve più una barra per ogni proposta)\n        for idx in positive_indices:\n            x_min, y_min, x_max, y_max = proposal_coords[idx].values()\n\n            # Calcola la larghezza e l'altezza per il formato COCO\n            width = x_max - x_min\n            height = y_max - y_min\n            \n            cropped = original_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n            \n            # Ridimensiona a 224x224\n            resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n            \n            # Ottieni l'etichetta della categoria dal bounding box assegnato\n            category_id = gt_categories[indices[idx].item()]\n            \n            # Salva l'immagine\n            output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n            cv2.imwrite(output_path, resized)\n            \n            # Aggiungi la proposta attivata al nuovo JSON in formato COCO\n            active_region_data.append({\n                \"image_id\": image_id,\n                \"file_name\": file_name,\n                \"category_id\": category_id,\n                \"proposal_id\": idx.item(),\n                \"region_bbox\": [x_min, y_min, width, height],  \n                \"saved_path\": output_path\n            })\n            \n            counter += 1\n    \n    # Salva il nuovo JSON con le regioni attive\n    with open(output_json_path, 'w') as json_file:\n        json.dump(active_region_data, json_file, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:52:18.798745Z","iopub.execute_input":"2024-12-08T14:52:18.799205Z","iopub.status.idle":"2024-12-08T14:52:18.818634Z","shell.execute_reply.started":"2024-12-08T14:52:18.799159Z","shell.execute_reply":"2024-12-08T14:52:18.817649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"assign_and_save_regions(proposals_json, coco_json_pth, img_fldr, prop_fldr, actproposals_json, iou_threshold=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:52:18.819971Z","iopub.execute_input":"2024-12-08T14:52:18.820413Z","iopub.status.idle":"2024-12-08T15:04:21.369780Z","shell.execute_reply.started":"2024-12-08T14:52:18.820365Z","shell.execute_reply":"2024-12-08T15:04:21.367617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ignored_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:04:21.375332Z","iopub.execute_input":"2024-12-08T15:04:21.375848Z","iopub.status.idle":"2024-12-08T15:04:21.384937Z","shell.execute_reply.started":"2024-12-08T15:04:21.375797Z","shell.execute_reply":"2024-12-08T15:04:21.383442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# controllo sulle regioni\nfile_path = '/kaggle/input/activeregion-xviewdataset/activeregion-xview-dataset/active_regions.json'\n\n#carica il file JSON\nwith open(file_path, 'r') as f:\n    data = json.load(f)\n\n#conta numero di regioni\nnum_regioni = len(data)\nprint(f\"Numero di regioni: {num_regioni}\")\n\n# occorrenze dei category_id\ncategory_ids = [entry['category_id'] for entry in data]\ncategory_counts = Counter(category_ids)\nprint(\"Occorrenze dei category_id:\", category_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:04:21.386392Z","iopub.execute_input":"2024-12-08T15:04:21.386824Z","iopub.status.idle":"2024-12-08T15:04:21.900306Z","shell.execute_reply.started":"2024-12-08T15:04:21.386776Z","shell.execute_reply":"2024-12-08T15:04:21.898791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nimport os\n\nworking_dir = \"/kaggle/working\"\n\n# Nome del file zip da creare\nzip_file_name = \"activeregion-xview-dataset.zip\"\n\n# Elenco di file e cartelle da includere nello zip\nitems_to_zip = [\n    \"active_regions.json\",\n    \"proposals\",\n    \"proposals.json\"\n]\n\n# Funzione per aggiungere file e cartelle allo zip\ndef zip_folder(zipf, folder_path, base_folder=\"\"):\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, base_folder)\n            zipf.write(file_path, arcname)\n\n# Creazione dello zip\nwith zipfile.ZipFile(zip_file_name, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.exists(item):  # Verifica che il file o la cartella esista\n            if os.path.isdir(item):  # Se è una cartella, aggiungi tutto il contenuto\n                zip_folder(zipf, item, working_dir)\n            else:  # Se è un file, aggiungilo direttamente\n                zipf.write(item)\n        else:\n            print(f\"Elemento non trovato: {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:04:21.901604Z","iopub.execute_input":"2024-12-08T15:04:21.901974Z","iopub.status.idle":"2024-12-08T15:06:44.057715Z","shell.execute_reply.started":"2024-12-08T15:04:21.901941Z","shell.execute_reply":"2024-12-08T15:06:44.056570Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"# Percorso del file JSON e del percorso base\ninput_json_path = \"/kaggle/input/activeregion-xviewdataset/activeregion-xview-dataset/active_regions.json\"\n\n# Carica il dataset dal JSON\nwith open(input_json_path, \"r\") as f:\n    data = json.load(f)\n\n# Converti in DataFrame per una gestione più comoda\ndf = pd.DataFrame(data)\n\n# Estrai il nome del file dal campo 'saved_path'\ndf[\"file_name\"] = df[\"saved_path\"].apply(lambda x: os.path.basename(x))\n\n# Aggiungi il percorso base al campo 'saved_path'\ndf[\"saved_path\"] = df[\"file_name\"].apply(lambda x: str(act_reg_folder / x))\n\n\n\n# Estrai i dati e le etichette\nX = df.index  # Indici delle righe\ny = df[\"category_id\"]  # Etichetta per stratificazione\n\n# Step 1: Train + Val/Test\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Step 2: Val/Test split\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n)\n\n# Creazione dei dataset finali\ntrain_data = df.loc[X_train]\nval_data = df.loc[X_val]\ntest_data = df.loc[X_test]\n\n# Salva i dataset in nuovi file JSON\ntrain_data.to_json(\"train.json\", orient=\"records\", lines=False)\nval_data.to_json(\"val.json\", orient=\"records\", lines=False)\ntest_data.to_json(\"test.json\", orient=\"records\", lines=False)\n\nprint(\"Splitting completato. File salvati: train.json, val.json, test.json.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:22:13.018625Z","iopub.execute_input":"2024-12-09T15:22:13.019070Z","iopub.status.idle":"2024-12-09T15:22:16.224016Z","shell.execute_reply.started":"2024-12-09T15:22:13.019036Z","shell.execute_reply":"2024-12-09T15:22:16.222802Z"}},"outputs":[{"name":"stdout","text":"Splitting completato. File salvati: train.json, val.json, test.json.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        \"\"\"\n        Inizializza il dataset.\n\n        :param json_file: Percorso del file JSON contenente le informazioni sulle regioni.\n        :param transform: Trasformazioni da applicare alle immagini. Se non fornito, vengono usate trasformazioni di default.\n        \"\"\"\n        with open(json_file, 'r') as f:\n            self.data = json.load(f)  # Carica il file JSON\n        \n        # Trasformazioni di default se non vengono fornite\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),         # Ridimensiona l'immagine a 224x224\n            transforms.ToTensor()                 # Converte l'immagine in un tensore\n            #transforms.Normalize(                  # Normalizzazione con valori di ImageNet\n            #    mean=[0.485, 0.456, 0.406], \n            #    std=[0.229, 0.224, 0.225]\n            #)\n        ])  \n\n    def __len__(self):\n        \"\"\"Restituisce il numero totale di immagini/proposte nel dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Restituisce un esempio (immagine e etichetta) per l'addestramento.\"\"\"\n        # Carica l'esempio dal file JSON\n        sample = self.data[idx]\n        \n        # Carica l'immagine\n        image = Image.open(sample[\"saved_path\"]).convert(\"RGB\")\n        \n        # Etichetta della categoria\n        label = sample[\"category_id\"]  # Categoria della proposta\n\n        # Applica le trasformazioni\n        image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:19:29.805751Z","iopub.execute_input":"2024-12-09T15:19:29.806154Z","iopub.status.idle":"2024-12-09T15:19:29.814305Z","shell.execute_reply.started":"2024-12-09T15:19:29.806118Z","shell.execute_reply":"2024-12-09T15:19:29.813039Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"test_ds = CustomDataset(test_path)\ntrain_ds = CustomDataset(train_path)\nval_ds = CustomDataset(val_path)\n\nTrainLoader = DataLoader(train_ds, batch_size=64, shuffle=True)\nValLoader = DataLoader(val_ds, batch_size=64, shuffle=False)\nTestLoader = DataLoader(test_ds, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:22:55.388301Z","iopub.execute_input":"2024-12-09T15:22:55.388673Z","iopub.status.idle":"2024-12-09T15:22:56.334204Z","shell.execute_reply.started":"2024-12-09T15:22:55.388634Z","shell.execute_reply":"2024-12-09T15:22:56.333203Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(AlexNet, self).__init__()\n        self._output_num = num_classes\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n     \n        self.drop8 = nn.Dropout()\n        self.fn8 = nn.Linear(256 * 6 * 6, 4096)\n        self.active8 = nn.ReLU(inplace=True)\n        \n        self.drop9 = nn.Dropout()\n        self.fn9 = nn.Linear(4096, 4096)\n        self.active9 = nn.ReLU(inplace=True)\n        \n        self.fn10 = nn.Linear(4096, self._output_num)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.drop8(x)\n        x = self.fn8(x)\n        x = self.active8(x)\n\n        x = self.drop9(x)\n        x = self.fn9(x)\n        \n        feature = self.active9(x)\n        final = func.sigmoid(self.fn10(feature))\n\n        return feature, final","metadata":{"execution":{"iopub.status.busy":"2024-12-09T15:23:31.759539Z","iopub.execute_input":"2024-12-09T15:23:31.759966Z","iopub.status.idle":"2024-12-09T15:23:31.770720Z","shell.execute_reply.started":"2024-12-09T15:23:31.759914Z","shell.execute_reply":"2024-12-09T15:23:31.769664Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"num_classes = 12 #11 classi + sfondo\nnet = AlexNet(num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:25:39.909434Z","iopub.execute_input":"2024-12-09T15:25:39.909839Z","iopub.status.idle":"2024-12-09T15:25:40.537199Z","shell.execute_reply.started":"2024-12-09T15:25:39.909807Z","shell.execute_reply":"2024-12-09T15:25:40.536074Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print(net)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:25:43.189673Z","iopub.execute_input":"2024-12-09T15:25:43.190110Z","iopub.status.idle":"2024-12-09T15:25:43.196592Z","shell.execute_reply.started":"2024-12-09T15:25:43.190075Z","shell.execute_reply":"2024-12-09T15:25:43.195121Z"}},"outputs":[{"name":"stdout","text":"AlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (drop8): Dropout(p=0.5, inplace=False)\n  (fn8): Linear(in_features=9216, out_features=4096, bias=True)\n  (active8): ReLU(inplace=True)\n  (drop9): Dropout(p=0.5, inplace=False)\n  (fn9): Linear(in_features=4096, out_features=4096, bias=True)\n  (active9): ReLU(inplace=True)\n  (fn10): Linear(in_features=4096, out_features=12, bias=True)\n)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def train_model(net, train_loader, val_loader, criterion, optimizer, device, epochs, path_min_loss):\n    \"\"\"\n    Funzione per addestrare il modello.\n\n    :param net: Modello da addestrare.\n    :param train_loader: DataLoader per il training set.\n    :param val_loader: DataLoader per il validation set.\n    :param criterion: Funzione di loss.\n    :param optimizer: Ottimizzatore.\n    :param device: Dispositivo (CPU o GPU).\n    :param epochs: Numero di epoche di training.\n    :param path_min_loss: Percorso per salvare il modello con la minore loss di validazione.\n    \"\"\"\n    min_val_loss = float('inf')\n\n    for epoch in range(epochs):\n        net.train()  # Modalità training\n        train_loss = 0.0\n        correct_train = 0\n        total_train = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Backward pass e aggiornamento pesi\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Statistiche\n            train_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_train += labels.size(0)\n            correct_train += predicted.eq(labels).sum().item()\n\n        avg_train_loss = train_loss / len(train_loader.dataset)\n        train_accuracy = 100. * correct_train / total_train\n\n        # Validazione\n        net.eval()  # Modalità validazione\n        val_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n\n                # Forward pass\n                _, outputs = net(images)\n\n                # Calcolo della loss\n                loss = criterion(outputs, labels)\n\n                # Statistiche\n                val_loss += loss.item() * images.size(0)\n                _, predicted = outputs.max(1)\n                total_val += labels.size(0)\n                correct_val += predicted.eq(labels).sum().item()\n\n        avg_val_loss = val_loss / len(val_loader.dataset)\n        val_accuracy = 100. * correct_val / total_val\n\n        # Salva il modello con la loss di validazione più bassa\n        if avg_val_loss < min_val_loss:\n            print(f\"Salvataggio del miglior modello: Val Loss migliorata da {min_val_loss:.4f} a {avg_val_loss:.4f}\")\n            min_val_loss = avg_val_loss\n            torch.save(net.state_dict(), path_min_loss)\n\n        # Stampa statistiche per epoca\n        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n    print(\"Training completato!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:36:43.456845Z","iopub.execute_input":"2024-12-09T15:36:43.457403Z","iopub.status.idle":"2024-12-09T15:36:43.471673Z","shell.execute_reply.started":"2024-12-09T15:36:43.457363Z","shell.execute_reply":"2024-12-09T15:36:43.470482Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def test_model(net, test_loader, criterion, device, path_min_loss):\n    \"\"\"\n    Funzione per testare il modello.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    \"\"\"\n    # Carica il miglior modello salvato\n    net.load_state_dict(torch.load(path_min_loss))\n    net.eval()\n\n    test_loss = 0.0\n    correct_test = 0\n    total_test = 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            test_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_test += labels.size(0)\n            correct_test += predicted.eq(labels).sum().item()\n\n    avg_test_loss = test_loss / len(test_loader.dataset)\n    test_accuracy = 100. * correct_test / total_test\n\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:36:46.976028Z","iopub.execute_input":"2024-12-09T15:36:46.976433Z","iopub.status.idle":"2024-12-09T15:36:46.985757Z","shell.execute_reply.started":"2024-12-09T15:36:46.976399Z","shell.execute_reply":"2024-12-09T15:36:46.984066Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.001\nepochs = 10\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\ndevice = torch.device(\"cuda\")\n\nnet = net.to(device)\ncriterion = criterion.to(device)\n\npath_min_loss = '/kaggle/working/AlexNet.pth'\n\ntrain_model(\n    net=net,\n    train_loader=TrainLoader,\n    val_loader=ValLoader,\n    criterion=criterion,\n    optimizer=optimizer,\n    device=device,\n    epochs=epochs,\n    path_min_loss=path_min_loss\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:40:43.908134Z","iopub.execute_input":"2024-12-09T15:40:43.908492Z","iopub.status.idle":"2024-12-09T15:40:44.212391Z","shell.execute_reply.started":"2024-12-09T15:40:43.908463Z","shell.execute_reply":"2024-12-09T15:40:44.210884Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m criterion \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m path_min_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/AlexNet.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m     )\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"],"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"test_model(\n    net=net,\n    test_loader=TestLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=path_min_loss\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}