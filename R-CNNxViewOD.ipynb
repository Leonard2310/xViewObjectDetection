{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10182328,"sourceType":"datasetVersion","datasetId":6242793},{"sourceId":10311668,"sourceType":"datasetVersion","datasetId":6295408},{"sourceId":10322992,"sourceType":"datasetVersion","datasetId":6258380},{"sourceId":10329194,"sourceType":"datasetVersion","datasetId":6395444},{"sourceId":213827,"sourceType":"modelInstanceVersion","modelInstanceId":182143,"modelId":204371}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:56.666679Z","iopub.execute_input":"2024-12-28T23:07:56.667003Z","iopub.status.idle":"2024-12-28T23:07:58.491938Z","shell.execute_reply.started":"2024-12-28T23:07:56.666976Z","shell.execute_reply":"2024-12-28T23:07:58.490903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Librerie per l'ottimizzazione e la gestione delle dipendenze\nimport selectivesearch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport zipfile\nimport torchvision\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport matplotlib.patches as patches\nimport torch.nn.utils as utils\nimport ast\nimport itertools","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:57:29.248818Z","iopub.execute_input":"2025-01-05T17:57:29.249135Z","iopub.status.idle":"2025-01-05T17:57:36.923398Z","shell.execute_reply.started":"2025-01-05T17:57:29.249100Z","shell.execute_reply":"2025-01-05T17:57:36.921902Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nCOCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_COCO_JSON_NM = 'mod_COCO_annotations.json'\nOUT_IMAGE_FLDR_NM = 'images'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nprop_dataset_pth = Path('/kaggle/input/proposals-xview-dataset-r-cnn')\nactreg_dataset_pth = Path('/kaggle/input/activeregion-xviewdataset')\nresnet_pth = Path('/kaggle/input/resnet-50-xview/pytorch/default')\nregbox_dataset_pth = Path('/kaggle/input/regbboxdataset-xview')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / COCO_JSON_NM\nnew_coco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\nin_new_coco_json_pth = prop_dataset_pth / OUT_COCO_JSON_NM\n\n# PROPOSALS\nPROP_COCO_JSON_NM = 'proposals.json'\nproposals_json = prop_dataset_pth / PROP_COCO_JSON_NM\nout_proposals_json = out_dataset_pth / PROP_COCO_JSON_NM\nPROPOSALS = 'proposals'\nprop_fldr = out_dataset_pth / PROPOSALS\n\n\n# ACTIVE REGIONS\nACTPROP_COCO_JSON_NM ='active_regions.json'\nactproposals_json = out_dataset_pth / ACTPROP_COCO_JSON_NM\nin_actproposals_json = actreg_dataset_pth / ACTPROP_COCO_JSON_NM\nout_act_reg_folder = out_dataset_pth / PROPOSALS\nact_reg_folder = actreg_dataset_pth / PROPOSALS\n\n#DATASET\ntrain_path = out_dataset_pth / 'train.json'\ntest_path = out_dataset_pth / 'test.json'\nval_path = out_dataset_pth / 'val.json'\nreg_path = out_dataset_pth / 'reg.json'\n\n# Percorso per salvare il modello\npath_min_loss = '/kaggle/working/ResNet50.pth'\n\ntrain_features_path = out_dataset_pth / 'train_features.npy'\n\nresnet_min_loss_path =  resnet_pth / '3/ResNet50.pth'\n\nin_train_path = regbox_dataset_pth / 'train.json'\nin_reg_path = regbox_dataset_pth / 'reg.json'\nin_train_features_path = regbox_dataset_pth / 'train_features.npy'\n\nout_reg_path = out_dataset_pth / 'new_reg.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:59:10.578474Z","iopub.execute_input":"2025-01-05T17:59:10.579222Z","iopub.status.idle":"2025-01-05T17:59:10.591093Z","shell.execute_reply.started":"2025-01-05T17:59:10.579183Z","shell.execute_reply":"2025-01-05T17:59:10.589961Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T00:02:20.793347Z","iopub.execute_input":"2024-12-30T00:02:20.794082Z","iopub.status.idle":"2024-12-30T00:02:20.800239Z","shell.execute_reply.started":"2024-12-30T00:02:20.794042Z","shell.execute_reply":"2024-12-30T00:02:20.799432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utility","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")\n\n# Sopprime i warning relativi al parametro 'pretrained' deprecato\nwarnings.filterwarnings(\"ignore\", \n    message=\"The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\")\n\n# Sopprime i warning relativi agli argomenti diversi da weight enum o None\nwarnings.filterwarnings(\"ignore\", \n    message=\"Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future.*\")\n\n# Sopprime i warning relativi al parametro 'verbose' deprecato\nwarnings.filterwarnings(\"ignore\", \n    message=\"The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\")\n\n# Sopprime i warning relativi a `torch.load` e `weights_only=False`\nwarnings.filterwarnings(\"ignore\", \n    message=\"You are using `torch.load` with `weights_only=False`.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:56:54.833303Z","iopub.execute_input":"2024-12-31T09:56:54.834430Z","iopub.status.idle":"2024-12-31T09:56:54.841255Z","shell.execute_reply.started":"2024-12-31T09:56:54.834383Z","shell.execute_reply":"2024-12-31T09:56:54.840160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_json(file_path):\n    \"\"\"\n    Carica un file JSON dal percorso specificato.\n\n    :param file_path: Percorso al file JSON da caricare.\n    :return: Dati contenuti nel file JSON (come dizionario o lista).\n    \"\"\"\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:58:55.546102Z","iopub.execute_input":"2025-01-05T17:58:55.547775Z","iopub.status.idle":"2025-01-05T17:58:55.554294Z","shell.execute_reply.started":"2025-01-05T17:58:55.547714Z","shell.execute_reply":"2025-01-05T17:58:55.552560Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Test Dataset","metadata":{}},{"cell_type":"code","source":"def display_images_with_bboxes(json_file, specific_images, images_folder):\n    \"\"\"\n    Visualizza le immagini specificate con tutti i bounding box sopra di esse.\n    \n    :param json_file: percorso del file JSON contenente le immagini, annotazioni e categorie\n    :param specific_images: lista di nomi delle immagini da visualizzare\n    :param images_folder: percorso della cartella che contiene le immagini\n    \"\"\"\n    # Carica il JSON\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    \n    # Estrai le immagini e le annotazioni\n    images = data[\"images\"]\n    annotations = data[\"annotations\"]\n    \n    # Crea un dizionario per mappare l'id delle immagini al nome del file\n    image_dict = {image[\"id\"]: image[\"file_name\"] for image in images}\n    \n    # Filtra le annotazioni per le immagini specifiche\n    specific_annotations = [ann for ann in annotations if image_dict[ann[\"image_id\"]] in specific_images]\n\n    # Creiamo un dizionario per raccogliere tutte le annotazioni per ciascuna immagine\n    image_bboxes = {}\n    for annotation in specific_annotations:\n        image_name = image_dict[annotation[\"image_id\"]]\n        if image_name not in image_bboxes:\n            image_bboxes[image_name] = []\n        bbox = ast.literal_eval(annotation[\"bbox\"])\n        image_bboxes[image_name].append(bbox)\n    \n    # Visualizza tutte le immagini con tutti i bounding box\n    for image_name, bboxes in image_bboxes.items():\n        # Carica l'immagine\n        image_path = f'{images_folder}/{image_name}'  # Usa il percorso corretto per le immagini\n        image = Image.open(image_path)\n\n        # Crea la figura per la visualizzazione\n        plt.figure(figsize=(8, 8))\n        plt.imshow(image)\n\n        # Aggiungi tutti i bounding box\n        for bbox in bboxes:\n            x, y, w, h = bbox\n            plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none'))\n\n        # Imposta il titolo e disattiva gli assi\n        plt.title(f\"Image: {image_name}\")\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:14:42.690436Z","iopub.execute_input":"2024-12-31T11:14:42.691201Z","iopub.status.idle":"2024-12-31T11:14:42.702312Z","shell.execute_reply.started":"2024-12-31T11:14:42.691158Z","shell.execute_reply":"2024-12-31T11:14:42.701134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"specific_images = ['img_418_2240_2880.jpg','img_418_2240_640.jpg']\n\ndisplay_images_with_bboxes(coco_json_pth, specific_images, img_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:45:50.230030Z","iopub.execute_input":"2024-12-31T11:45:50.230904Z","iopub.status.idle":"2024-12-31T11:45:53.474913Z","shell.execute_reply.started":"2024-12-31T11:45:50.230861Z","shell.execute_reply":"2024-12-31T11:45:53.473705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# COCO Preprocessing","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    data = load_json(input_path)\n\n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n \n    for category in tqdm(raw_categories, desc=\"Processing Categories\"):\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n \n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n \n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n \n    # Preprocessa le annotazioni in un dizionario per immagini\n    image_annotations_dict = {}\n    for annotation in tqdm(data.get('annotations', []), desc=\"Building Image Annotations Dictionary\"):\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_dict:\n            image_annotations_dict[image_id] = []\n        image_annotations_dict[image_id].append(annotation)\n \n    # Elenco di annotazioni da mantenere (solo quelle valide)\n    valid_annotations = []\n    annotations_to_remove = set()\n \n    # Controllo dei bounding box\n    for annotation in tqdm(data.get('annotations', []), desc=\"Processing Annotations\"):\n        if annotation['category_id'] == 0:  # Se è Aircraft\n            annotation['category_id'] = 11\n        \n        # Converte il formato del bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n        \n        x, y, width, height = annotation['bbox']\n        xmin, xmax = x, x + width\n        ymin, ymax = y, y + height\n        \n        # Verifica che xmin < xmax e ymin < ymax, e che la larghezza e altezza siano sufficienti\n        if xmin >= xmax or ymin >= ymax or width <= 10 or height <= 10:\n            annotations_to_remove.add(annotation['id'])\n        else:\n            annotation['bbox'] = [xmin, ymin, xmax, ymax]\n            valid_annotations.append(annotation)\n \n    # Rimuovi le annotazioni non valide\n    data['annotations'] = valid_annotations\n \n    # Verifica se ci sono immagini senza annotazioni (usando il dizionario delle annotazioni)\n    new_annotations = []\n    for image in tqdm(data.get('images', []), desc=\"Processing Images\"):\n        if image['id'] not in image_annotations_dict:  # Se l'immagine non ha annotazioni\n            # Aggiungi la categoria \"background\"\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, 0.0, image['width'], image['height']],  # Background con bbox che copre tutta l'immagine\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n \n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n\n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n\n    # **Seleziona casualmente il 50% delle immagini**\n    all_images = data.get('images', [])\n    selected_images = random.sample(all_images, len(all_images) // 2)\n    selected_image_ids = {img['id'] for img in selected_images}\n\n    # Filtra immagini e annotazioni\n    data['images'] = selected_images\n    data['annotations'] = [ann for ann in data['annotations'] if ann['image_id'] in selected_image_ids]\n\n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.550338Z","iopub.status.idle":"2024-12-28T23:07:58.550688Z","shell.execute_reply.started":"2024-12-28T23:07:58.550523Z","shell.execute_reply":"2024-12-28T23:07:58.550541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_custom_coco_json(coco_json_pth, new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.551597Z","iopub.status.idle":"2024-12-28T23:07:58.551943Z","shell.execute_reply.started":"2024-12-28T23:07:58.551766Z","shell.execute_reply":"2024-12-28T23:07:58.551783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Category Check","metadata":{}},{"cell_type":"code","source":"def count_bounding_boxes(json_path):\n    \"\"\"\n    Conta il numero di bounding box per ogni categoria in un file COCO JSON.\n\n    Args:\n        json_path (str): Percorso del file JSON.\n\n    Returns:\n        list: Elenco di tuple con ID categoria, nome categoria e numero di bounding box.\n    \"\"\"\n    # Carica il file JSON\n    coco_data = load_json(json_path)\n\n    # Estrarre i dati principali\n    annotations = coco_data.get(\"annotations\", [])\n    categories = coco_data.get(\"categories\", [])\n\n    # Mappare id di categoria ai nomi delle categorie\n    category_id_to_name = {category[\"id\"]: category[\"name\"] for category in categories}\n\n    # Contare i bounding box per categoria\n    bbox_counts = defaultdict(int)\n    for annotation in annotations:\n        category_id = annotation[\"category_id\"]\n        bbox_counts[category_id] += 1\n\n    # Creare un elenco dei risultati\n    results = [\n        (cat_id, category_id_to_name.get(cat_id, \"Unknown\"), count)\n        for cat_id, count in bbox_counts.items()\n    ]\n    \n    # Ordinare i risultati in ordine decrescente per numero di bounding box\n    results.sort(key=lambda x: x[2], reverse=True)\n    \n    # Stampare i risultati\n    for cat_id, category_name, count in results:\n        print(f\"Categoria ID {cat_id} ('{category_name}'): {count} bounding box\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.552859Z","iopub.status.idle":"2024-12-28T23:07:58.553187Z","shell.execute_reply.started":"2024-12-28T23:07:58.553029Z","shell.execute_reply":"2024-12-28T23:07:58.553046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count_bounding_boxes(new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.554746Z","iopub.status.idle":"2024-12-28T23:07:58.555045Z","shell.execute_reply.started":"2024-12-28T23:07:58.554894Z","shell.execute_reply":"2024-12-28T23:07:58.554909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Region Proposals Generation","metadata":{}},{"cell_type":"code","source":"# Funzione per elaborare una singola immagine\ndef process_single_image(image_data, img_fldr):\n    img_id = image_data['id']\n    img_name = image_data['file_name']\n    img_path = os.path.join(img_fldr, img_name)\n\n    if not os.path.exists(img_path):\n        raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    # Carica l'immagine usando opencv (in modalità RGB)\n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n    original_height, original_width, _ = image.shape\n\n    # Ridimensiona l'immagine per velocizzare la Selective Search\n    resized_image = cv2.resize(image, (original_width // 2, original_height // 2), interpolation=cv2.INTER_AREA)\n\n    # Genera le region proposals sulla versione ridotta\n    processed_proposals = generate_and_process_proposals(resized_image, original_width // 2, original_height // 2)\n\n    # Riscalare le coordinate delle proposte alla dimensione originale\n    scaled_proposals = [[x * 2, y * 2, x_max * 2, y_max * 2] for x, y, x_max, y_max in processed_proposals]\n\n    image_data = {\n        \"image_id\": img_id,\n        \"file_name\": img_name,\n        \"original_size\": [original_width, original_height],\n        \"proposals\": []\n    }\n\n    for i, proposal in enumerate(scaled_proposals):\n        x_min, y_min, x_max, y_max = proposal\n        image_data[\"proposals\"].append({\n            \"proposal_id\": i,\n            \"coordinates\": [x_min, y_min, x_max, y_max]\n        })\n\n    return image_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.556269Z","iopub.status.idle":"2024-12-28T23:07:58.556618Z","shell.execute_reply.started":"2024-12-28T23:07:58.556413Z","shell.execute_reply":"2024-12-28T23:07:58.556464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funzione per generare le region proposals con Selective Search\ndef generate_and_process_proposals(image, img_width, img_height):\n    img_np = np.array(image, dtype=np.uint8)\n\n    # Esegui la selective search con parametri ottimizzati\n    _, regions = selectivesearch.selective_search(img_np, scale=200, sigma=0.5, min_size=10)\n    if len(regions) == 0:\n        print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n    processed_proposals = []\n\n    # Pre-filtraggio delle regioni\n    for region in regions:\n        x, y, w, h = region['rect']\n        area = w * h\n        if w >= 10 and h >= 10 and 10 <= area <= 0.8 * (img_width * img_height):\n            x_max, y_max = x + w, y + h\n            processed_proposals.append([x, y, x_max, y_max])\n\n    return processed_proposals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.557612Z","iopub.status.idle":"2024-12-28T23:07:58.557927Z","shell.execute_reply.started":"2024-12-28T23:07:58.557770Z","shell.execute_reply":"2024-12-28T23:07:58.557787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funzione per gestire i batch\ndef batch(iterable, n=1):\n    it = iter(iterable)\n    while True:\n        chunk = list(islice(it, n))\n        if not chunk:\n            break\n        yield chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.560384Z","iopub.status.idle":"2024-12-28T23:07:58.560726Z","shell.execute_reply.started":"2024-12-28T23:07:58.560575Z","shell.execute_reply":"2024-12-28T23:07:58.560592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_dataset_proposals(coco_json, img_fldr, output_dir, output_json):\n    os.makedirs(output_dir, exist_ok=True)\n    all_image_data = []\n\n    # Carica il file JSON di COCO\n    coco_data = load_json(coco_json)\n\n    # Prepara il mapping delle annotazioni per le immagini\n    image_annotations_map = {}\n    for annotation in coco_data['annotations']:\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_map:\n            image_annotations_map[image_id] = []\n        image_annotations_map[image_id].append(annotation)\n\n    # Filtra le immagini che contengono annotazioni con category_id == 0 (sfondo)\n    images_with_annotations = [\n        image_data for image_data in coco_data['images']\n        if image_data['id'] in image_annotations_map and len(image_annotations_map[image_data['id']]) > 0\n        and not any(annot['category_id'] == 0 for annot in image_annotations_map[image_data['id']])  # Escludi sfondo\n    ]\n\n    # Parametri per parallelizzazione e batch processing\n    max_workers = os.cpu_count() - 1\n    batch_size = 500\n    total_batches = len(images_with_annotations) // batch_size + (len(images_with_annotations) % batch_size > 0)\n\n    # Processa le immagini in batch con tqdm per monitorare il progresso dei batch\n    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n        for image_batch in batch(images_with_annotations, batch_size):\n            with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n                results = list(executor.map(process_single_image, image_batch, [img_fldr] * len(image_batch)))\n            all_image_data.extend(results)\n            pbar.update(1)  # Aggiorna la barra di progresso per ogni batch completato\n\n    # Salva il risultato in formato JSON usando orjson\n    with open(output_json, 'wb') as json_file:\n        json_file.write(orjson.dumps(all_image_data, option=orjson.OPT_INDENT_2))\n\n    print(f\"Creato file JSON con le region proposals: {output_json}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.561941Z","iopub.status.idle":"2024-12-28T23:07:58.562272Z","shell.execute_reply.started":"2024-12-28T23:07:58.562113Z","shell.execute_reply":"2024-12-28T23:07:58.562130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_dataset_proposals(new_coco_json_pth, img_fldr, prop_fldr, out_proposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.563851Z","iopub.status.idle":"2024-12-28T23:07:58.564168Z","shell.execute_reply.started":"2024-12-28T23:07:58.564013Z","shell.execute_reply":"2024-12-28T23:07:58.564029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Nome del file zip da creare\nzip_file_name = \"proposals-dataset.zip\"\n\n# Elenco di file e cartelle da includere nello zip\nitems_to_zip = [\n    \"mod_COCO_annotations.json\",\n    \"proposals.json\",\n]\n\n# Funzione per aggiungere file e cartelle allo zip\ndef zip_folder(zipf, folder_path, base_folder=\"\"):\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, base_folder)\n            zipf.write(file_path, arcname)\n\n# Creazione dello zip\nwith zipfile.ZipFile(zip_file_name, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.exists(item):  # Verifica che il file o la cartella esista\n            if os.path.isdir(item):  # Se è una cartella, aggiungi tutto il contenuto\n                zip_folder(zipf, item, out_dataset_pth)\n            else:  # Se è un file, aggiungilo direttamente\n                zipf.write(item)\n        else:\n            print(f\"Elemento non trovato: {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.565515Z","iopub.status.idle":"2024-12-28T23:07:58.565849Z","shell.execute_reply.started":"2024-12-28T23:07:58.565691Z","shell.execute_reply":"2024-12-28T23:07:58.565709Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Positive Region Proposals with Undersampling","metadata":{}},{"cell_type":"code","source":"def visualize_image_with_bbox(image_path, bbox, title, ax):\n    \"\"\"\n    Visualizza un'immagine con sopra disegnato un bounding box usando Matplotlib in un subplot.\n    :param image_path: Percorso all'immagine.\n    :param bbox: Bounding box in formato [xmin, ymin, xmax, ymax].\n    :param title: Titolo da mostrare sopra l'immagine.\n    :param ax: Axes del subplot dove disegnare l'immagine.\n    \"\"\"\n    # Carica l'immagine\n    image = Image.open(image_path)\n    \n    # Estrai xmin, ymin, xmax, ymax\n    xmin, ymin, xmax, ymax = bbox\n    width = xmax - xmin\n    height = ymax - ymin\n\n    # Aggiungi l'immagine e il bounding box\n    ax.imshow(image)\n    rect = patches.Rectangle(\n        (xmin, ymin), width, height,\n        linewidth=2, edgecolor='red', facecolor='none'\n    )\n    ax.add_patch(rect)\n    \n    # Aggiungi il titolo\n    ax.set_title(title, fontsize=15)\n    ax.axis('off')  # Rimuovi gli assi\n\ndef calculate_bbox_areas_for_all_categories(json_file_path, images_dir):\n    \"\"\"\n    Calcola l'area minima, massima e media per ogni categoria e disegna i bounding box \n    corrispondenti sulle immagini per visualizzarle.\n\n    :param json_file_path: Percorso al file JSON contenente le annotazioni.\n    :param images_dir: Directory contenente le immagini originali.\n    \"\"\"\n    # Carica il file JSON\n    with open(json_file_path, 'r') as f:\n        data = json.load(f)\n    \n    # Crea dizionari per mappare ID\n    areas_per_category = defaultdict(list)\n    category_map = {category['id']: category['name'] for category in data['categories']}\n    images_map = {image['id']: image for image in data['images']}\n\n    # Raccogliere le aree\n    for annotation in data.get(\"annotations\", []):\n        category_id = annotation[\"category_id\"]\n        area = annotation[\"area\"]\n        areas_per_category[category_id].append((area, annotation))\n\n    # Itera per ogni categoria\n    for category_id, areas in areas_per_category.items():\n        if not areas:\n            continue\n\n        # Trova le annotazioni con area minima, massima e calcola l'area media\n        min_area_annotation = min(areas, key=lambda x: x[0])[1]\n        max_area_annotation = max(areas, key=lambda x: x[0])[1]\n        avg_area = sum(area for area, _ in areas) / len(areas)\n\n        # Stampa le informazioni sulle aree\n        print(f\"Categoria: {category_map[category_id]}\")\n        print(f\"  Area Minima: {min_area_annotation['area']}\")\n        print(f\"  Area Massima: {max_area_annotation['area']}\")\n        print(f\"  Area Media: {avg_area:.2f}\\n\")\n\n        # Crea un subplot per visualizzare le immagini con i bounding box\n        fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n\n        # Visualizza le immagini con bounding box per l'area minima e massima\n        for annotation, label, ax_idx in [(min_area_annotation, \"Minima\", 0), (max_area_annotation, \"Massima\", 1)]:\n            image_id = annotation['image_id']\n            image_info = images_map.get(image_id)\n            if not image_info:\n                print(f\"Immagine con ID {image_id} non trovata.\")\n                continue\n\n            # Percorso immagine\n            image_path = os.path.join(images_dir, image_info['file_name'])\n            if not os.path.exists(image_path):\n                print(f\"Immagine {image_path} non trovata.\")\n                continue\n            \n            # Visualizza immagine con bounding box\n            bbox = annotation['bbox']\n            title = f\"{category_map[category_id]} ({label})\"\n            visualize_image_with_bbox(image_path, bbox, title, ax[ax_idx])\n\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:41.537994Z","iopub.execute_input":"2024-12-29T02:22:41.538367Z","iopub.status.idle":"2024-12-29T02:22:41.550875Z","shell.execute_reply.started":"2024-12-29T02:22:41.538321Z","shell.execute_reply":"2024-12-29T02:22:41.549997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esempio di utilizzo\ncalculate_bbox_areas_for_all_categories(in_new_coco_json_pth, img_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:44.715977Z","iopub.execute_input":"2024-12-29T02:22:44.716850Z","iopub.status.idle":"2024-12-29T02:22:58.003089Z","shell.execute_reply.started":"2024-12-29T02:22:44.716807Z","shell.execute_reply":"2024-12-29T02:22:58.001856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ignored_count = 0  # Contatore globale per le regioni ignorate\n\ndef get_iou(bb1, bb2):\n    global ignored_count  # Accedi alla variabile globale del contatore\n\n    try:\n        # Assicurati che le dimensioni siano corrette\n        assert bb1['x1'] < bb1['x2']\n        assert bb1['y1'] < bb1['y2']\n        assert bb2['x1'] < bb2['x2']\n        assert bb2['y1'] < bb2['y2']\n    except AssertionError:\n        # Se si verifica un errore, incrementa il contatore delle regioni ignorate\n        ignored_count += 1\n        return 0.0  # Restituisci 0.0 per l'IoU in caso di errore (nessuna sovrapposizione)\n\n    # Calcola le dimensioni dell'area comune tra i due box\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    # Se non c'è sovrapposizione, restituisci 0 come area di intersezione\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calcola l'area di intersezione\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calcola le aree individuali dei due bounding box\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n    \n    # Calcola l'area dell'unione\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    # Verifica che l'IoU sia nel range corretto\n    assert iou >= 0.0\n    assert iou <= 1.0\n\n    return iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.005203Z","iopub.execute_input":"2024-12-29T02:22:58.005664Z","iopub.status.idle":"2024-12-29T02:22:58.016873Z","shell.execute_reply.started":"2024-12-29T02:22:58.005609Z","shell.execute_reply":"2024-12-29T02:22:58.015848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_adaptive_threshold(bbox):\n    \"\"\"Calcola un threshold IoU adattivo in base all'area del bounding box e alla categoria.\"\"\"\n    # bbox è un tensore o una lista con [x1, y1, x2, y2]\n    width = bbox[2] - bbox[0]  # Calcolo larghezza\n    height = bbox[3] - bbox[1]  # Calcolo altezza\n    \n    # Calcolo dell'area\n    area = width * height\n\n    counter = 0\n    \n    # Aree medie per categoria (escludendo background)\n    area_media = {\n        2: 651.14,   # Truck\n        11: 6075.12  # Aircraft\n    }\n    \n    # Recupero delle aree minima e massima per normalizzazione\n    min_area = min(area_media.values())\n    max_area = max(area_media.values())\n    \n    # Normalizziamo l'area rispetto all'intervallo di area definito\n    normalized_area = (area - min_area) / (max_area - min_area)  # Valore tra 0 e 1\n    \n    # Clipping per evitare valori fuori intervallo\n    normalized_area = max(0, min(1, normalized_area))\n    \n    # Mappiamo il valore normalizzato su un intervallo di soglie (0.3 a 0.7)\n    threshold = 0.3 + 0.4 * normalized_area  # Da 0.3 a 0.7\n    \n    return threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.018027Z","iopub.execute_input":"2024-12-29T02:22:58.018295Z","iopub.status.idle":"2024-12-29T02:22:58.033329Z","shell.execute_reply.started":"2024-12-29T02:22:58.018270Z","shell.execute_reply":"2024-12-29T02:22:58.032583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assign_and_save_regions(region_json_path, bbox_json_path, image_dir, output_dir, output_json_path):\n    \"\"\"Associa le regioni proposte ai bounding boxes, salva le regioni positive come immagini e crea un nuovo JSON con informazioni attivate.\"\"\"\n    \n    # Carica i file JSON\n    with open(region_json_path, 'r') as f:\n        regions = json.load(f)\n\n    with open(bbox_json_path, 'r') as f:\n        bboxes = json.load(f)\n    \n    # Crea un dizionario per cercare annotations per image_id\n    annotations_by_image = {}\n    for annot in bboxes[\"annotations\"]:\n        img_id = annot[\"image_id\"]\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        bbox = annot[\"bbox\"]\n        annotations_by_image[img_id].append((torch.tensor(bbox, dtype=torch.float32), annot[\"category_id\"]))\n    \n    # Crea un dizionario per mappare category_id ai nomi delle categorie\n    category_mapping = {cat_id: name for cat_id, name in enumerate(bboxes[\"categories\"])}\n    \n    # Crea la directory di output se non esiste\n    os.makedirs(output_dir, exist_ok=True)\n\n    counter = 0  # Contatore delle immagini salvate\n    \n    active_region_data = []  # Lista per i dati delle regioni attive\n\n    # Avvolgi il ciclo principale per ogni immagine con tqdm\n    for image in tqdm(regions, desc=\"Elaborazione immagini\", total=len(regions)):\n        image_id = image[\"image_id\"]\n        file_name = image[\"file_name\"]\n        proposals = image[\"proposals\"]\n        \n        # Ottieni bounding boxes ground-truth e categorie per l'immagine corrente\n        gt_data = annotations_by_image.get(image_id, [])\n        if not gt_data:\n            # Se non ci sono bounding boxes ground-truth, salta l'immagine\n            continue\n        \n        gt_bboxes = [item[0] for item in gt_data]  # Bounding box ground truth\n        gt_categories = [item[1] for item in gt_data]  # Categorie ground truth\n        \n        # Trasforma proposals in una lista di dizionari compatibili con get_iou\n        proposal_coords = [{'x1': p[\"coordinates\"][0], 'y1': p[\"coordinates\"][1], \n                            'x2': p[\"coordinates\"][2], 'y2': p[\"coordinates\"][3]} \n                           for p in proposals]\n        \n        # Esegui undersampling per le categorie 6 prima di procedere\n        filtered_proposals = []\n        filtered_gt_bboxes = []\n        filtered_gt_categories = []\n        \n        for i, (prop, gt_bbox, gt_category) in enumerate(zip(proposal_coords, gt_bboxes, gt_categories)):\n            filtered_proposals.append(prop)\n            filtered_gt_bboxes.append(gt_bbox)\n            filtered_gt_categories.append(gt_category)\n\n        # Trasformiamo di nuovo in coordinate per IoU\n        proposal_coords = filtered_proposals\n        gt_bboxes = filtered_gt_bboxes\n        gt_categories = filtered_gt_categories\n\n        # Calcola la matrice IoU usando la funzione get_iou\n        iou_matrix = []\n        for proposal in proposal_coords:\n            iou_row = []\n            for gt_bbox in gt_bboxes:\n                gt_dict = {'x1': gt_bbox[0].item(), 'y1': gt_bbox[1].item(), \n                           'x2': gt_bbox[2].item(), 'y2': gt_bbox[3].item()}\n                iou = get_iou(proposal, gt_dict)\n                iou_row.append(iou)\n            iou_matrix.append(iou_row)\n\n        # Verifica se la matrice IoU è vuota\n        if not iou_matrix:\n            continue\n        \n        iou_matrix = torch.tensor(iou_matrix)\n\n        # Identifica le regioni positive utilizzando la soglia adattiva\n        positive_indices = []\n        zero_iou_indices = []\n        for row_idx, row in enumerate(iou_matrix):\n            for col_idx, iou in enumerate(row):\n                adaptive_threshold = get_adaptive_threshold(gt_bboxes[col_idx])\n                if iou >= adaptive_threshold:\n                    positive_indices.append((row_idx, col_idx))\n                elif iou == 0:\n                    # Accumula le immagini con IoU = 0 per selezione successiva\n                    zero_iou_indices.append((row_idx, -1))  # Assegna categoria 0 con -1\n        \n        # Seleziona casualmente il 20% delle immagini con IoU = 0\n        selected_zero_iou = random.sample(zero_iou_indices, int(0.005 * len(zero_iou_indices)))\n        \n        # Aggiungi solo le selezionate al set positivo\n        positive_indices.extend(selected_zero_iou)\n\n        # Carica l'immagine originale\n        image_path = os.path.join(image_dir, file_name)\n        original_image = cv2.imread(image_path)\n        if original_image is None:\n            print(f\"Immagine non trovata: {image_path}\")\n            continue\n\n        # Avvolgi il ciclo per ogni proposta positiva\n        for row_idx, col_idx in positive_indices:\n            if col_idx == -1:\n                category_id = 0  \n            else:\n                category_id = gt_categories[col_idx]\n            \n            # Aggiungi la condizione per includere solo il 10% delle categorie 0 e 6\n            if category_id in [0, 6]:\n                if random.random() > 0.9:  # Mantieni il 10% delle categorie 0 e 6\n                    # Calcola le coordinate del bounding box\n                    x_min, y_min, x_max, y_max = proposal_coords[row_idx].values()\n                    \n                    cropped = original_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n                    \n                    # Ridimensiona a 224x224\n                    resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n                    \n                    # Salva l'immagine\n                    output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n                    cv2.imwrite(output_path, resized)\n                    \n                    # Aggiungi la proposta attivata al nuovo JSON in formato COCO\n                    active_region_data.append({\n                        \"image_id\": image_id,\n                        \"file_name\": file_name,\n                        \"category_id\": category_id,\n                        \"proposal_id\": row_idx,\n                        \"region_bbox\": [x_min, y_min, x_max, y_max],  # Usa xmin, ymin, xmax, ymax\n                        \"original_bbox\": gt_bboxes[col_idx].tolist() if col_idx != -1 else [],  # Aggiunge il bbox originale\n                        \"saved_path\": output_path\n                    })\n                    \n                    counter += 1\n            else:\n                # Se la categoria non è 0 e 6, includi sempre la proposta\n                # Calcola le coordinate del bounding box\n                x_min, y_min, x_max, y_max = proposal_coords[row_idx].values()\n                \n                cropped = original_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n                \n                # Ridimensiona a 224x224\n                resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n                \n                # Salva l'immagine\n                output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n                cv2.imwrite(output_path, resized)\n                \n                # Aggiungi la proposta attivata al nuovo JSON in formato COCO\n                active_region_data.append({\n                    \"image_id\": image_id,\n                    \"file_name\": file_name,\n                    \"category_id\": category_id,\n                    \"proposal_id\": row_idx,\n                    \"region_bbox\": [x_min, y_min, x_max, y_max],  # Usa xmin, ymin, xmax, ymax\n                    \"original_bbox\": gt_bboxes[col_idx].tolist(),  # Aggiunge il bbox originale\n                    \"saved_path\": output_path\n                })\n                \n                counter += 1\n\n    # Salva il nuovo JSON con le regioni attive\n    with open(output_json_path, 'w') as json_file:\n        json.dump(active_region_data, json_file, indent=2)\n\n    print(counter)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.035945Z","iopub.execute_input":"2024-12-29T02:22:58.036319Z","iopub.status.idle":"2024-12-29T02:22:58.059694Z","shell.execute_reply.started":"2024-12-29T02:22:58.036290Z","shell.execute_reply":"2024-12-29T02:22:58.058891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esegui l'assegnazione e ottieni i valori IoU\n\nassign_and_save_regions(proposals_json, in_new_coco_json_pth, img_fldr, prop_fldr, actproposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.060673Z","iopub.execute_input":"2024-12-29T02:22:58.060948Z","iopub.status.idle":"2024-12-29T02:52:13.802699Z","shell.execute_reply.started":"2024-12-29T02:22:58.060921Z","shell.execute_reply":"2024-12-29T02:52:13.801875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_regions(file_path):\n    \"\"\"\n    Analizza un file JSON per ottenere il numero di regioni e le occorrenze dei category_id.\n\n    :param file_path: Percorso al file JSON contenente le annotazioni.\n    :return: Tupla contenente il numero di regioni e un dizionario con le occorrenze dei category_id.\n    \"\"\"\n    # Carica il file JSON\n    data = load_json(file_path)\n\n    # Conta il numero di regioni\n    num_regioni = len(data)\n    \n    # Ottieni le occorrenze dei category_id\n    category_ids = [entry['category_id'] for entry in data]\n    category_counts = Counter(category_ids)\n\n    # Ordina le occorrenze per ID di categoria\n    sorted_category_counts = dict(sorted(category_counts.items()))\n\n    return num_regioni, sorted_category_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:52:13.810154Z","iopub.execute_input":"2024-12-29T02:52:13.810458Z","iopub.status.idle":"2024-12-29T02:52:13.826351Z","shell.execute_reply.started":"2024-12-29T02:52:13.810409Z","shell.execute_reply":"2024-12-29T02:52:13.825390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_regioni, category_counts = analyze_regions(actproposals_json)\nprint(f\"Numero di regioni: {num_regioni}\")\nprint(\"Occorrenze dei category_id:\", category_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:52:13.827942Z","iopub.execute_input":"2024-12-29T02:52:13.828212Z","iopub.status.idle":"2024-12-29T02:52:13.977345Z","shell.execute_reply.started":"2024-12-29T02:52:13.828169Z","shell.execute_reply":"2024-12-29T02:52:13.976493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Nome del file zip da creare\nzip_file_name = \"activeregion-xview-dataset.zip\"\n\n# Elenco di file e cartelle da includere nello zip\nitems_to_zip = [\n    \"active_regions.json\",\n    \"proposals\",\n]\n\n# Funzione per aggiungere file e cartelle allo zip\ndef zip_folder(zipf, folder_path, base_folder=\"\"):\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, base_folder)\n            zipf.write(file_path, arcname)\n\n# Creazione dello zip\nwith zipfile.ZipFile(zip_file_name, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.exists(item):  # Verifica che il file o la cartella esista\n            if os.path.isdir(item):  # Se è una cartella, aggiungi tutto il contenuto\n                zip_folder(zipf, item, out_dataset_pth)\n            else:  # Se è un file, aggiungilo direttamente\n                zipf.write(item)\n        else:\n            print(f\"Elemento non trovato: {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:52:13.978349Z","iopub.execute_input":"2024-12-29T02:52:13.978639Z","iopub.status.idle":"2024-12-29T02:52:32.004881Z","shell.execute_reply.started":"2024-12-29T02:52:13.978613Z","shell.execute_reply":"2024-12-29T02:52:32.004053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Category Distribution","metadata":{}},{"cell_type":"code","source":"data = load_json(in_actproposals_json)\n \n# Conta le occorrenze delle categorie\ncategory_counts = Counter(item['category_id'] for item in data)\n \n# Prepara i dati per il grafico\ncategories = list(category_counts.keys())\ncounts = list(category_counts.values())\n \n# Crea il grafico\nplt.figure(figsize=(10, 6))\nplt.bar(categories, counts, color='skyblue')\nplt.xlabel('Category ID')\nplt.ylabel('Frequency')\nplt.title('Distribuzione delle categorie nel dataset')\nplt.xticks(categories)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:59:24.069363Z","iopub.execute_input":"2025-01-05T17:59:24.069854Z","iopub.status.idle":"2025-01-05T17:59:24.623964Z","shell.execute_reply.started":"2025-01-05T17:59:24.069812Z","shell.execute_reply":"2025-01-05T17:59:24.622733Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"# Carica il dataset dal JSON\n\ndata = load_json(in_actproposals_json)\n\n# Converti in DataFrame per una gestione più comoda\ndf = pd.DataFrame(data)\n\n# Estrai il nome del file dal campo 'saved_path'\ndf[\"file_name\"] = df[\"saved_path\"].apply(lambda x: os.path.basename(x))\n\n# Aggiungi il percorso base al campo 'saved_path'\ndf[\"saved_path\"] = df[\"file_name\"].apply(lambda x: str(act_reg_folder / x))\n\n# Estrai i dati e le etichette\nX = df.index  # Indici delle righe\ny = df[\"category_id\"]  # Etichetta per stratificazione\n\n# Step 1: Train + Val/Test\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Step 2: Val/Test split\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n)\n\n# Creazione dei dataset finali\ntrain_data = df.loc[X_train]\nval_data = df.loc[X_val]\ntest_data = df.loc[X_test]\n\n# Salva i dataset in nuovi file JSON\ntrain_data.to_json(\"train.json\", orient=\"records\", lines=False)\nval_data.to_json(\"val.json\", orient=\"records\", lines=False)\ntest_data.to_json(\"test.json\", orient=\"records\", lines=False)\n\nprint(\"Splitting completato. File salvati: train.json, val.json, test.json.\")\n\n# Percentuale dei dati per ciascun set\ntotal_data = len(df)\nprint(\"\\nPercentuale dei dati per ciascun set:\")\nprint(f\"Train: {len(train_data) / total_data:.2%}\")\nprint(f\"Validation: {len(val_data) / total_data:.2%}\")\nprint(f\"Test: {len(test_data) / total_data:.2%}\")\n\n# Distribuzione delle classi per ciascun set\nprint(\"\\nDistribuzione delle classi:\")\ntrain_class_dist = train_data[\"category_id\"].value_counts(normalize=True) * 100\nval_class_dist = val_data[\"category_id\"].value_counts(normalize=True) * 100\ntest_class_dist = test_data[\"category_id\"].value_counts(normalize=True) * 100\n\nprint(\"Train set:\")\nprint(train_class_dist.sort_index())\n\nprint(\"\\nValidation set:\")\nprint(val_class_dist.sort_index())\n\nprint(\"\\nTest set:\")\nprint(test_class_dist.sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:15:45.653113Z","iopub.execute_input":"2024-12-29T18:15:45.654045Z","iopub.status.idle":"2024-12-29T18:15:46.446675Z","shell.execute_reply.started":"2024-12-29T18:15:45.653985Z","shell.execute_reply":"2024-12-29T18:15:46.445717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = load_json(\"train.json\")\n\n# Visualizza 3 immagini a caso dal train set\nfig, axes = plt.subplots(1, 3, figsize=(20, 10))\nfor i, ax in enumerate(axes):\n    sample_entry = random.choice(train_data)\n    image_path = sample_entry[\"saved_path\"]\n    title = f\"Category ID: {sample_entry['category_id']}\"\n    visualize_image_with_bbox(image_path, [0,0,0,0], title, ax)\n\nplt.tight_layout()\nplt.show()\n\nval_data = load_json(\"val.json\")\n\n# Visualizza 3 immagini a caso dal train set\nfig, axes = plt.subplots(1, 3, figsize=(20, 10))\nfor i, ax in enumerate(axes):\n    sample_entry = random.choice(val_data)\n    image_path = sample_entry[\"saved_path\"]\n    title = f\"Category ID: {sample_entry['category_id']}\"\n    visualize_image_with_bbox(image_path, [0,0,0,0], title, ax)\n\nplt.tight_layout()\nplt.show()\n\ntest_data = load_json(\"test.json\")\n\n# Visualizza 3 immagini a caso dal train set\nfig, axes = plt.subplots(1, 3, figsize=(20, 10))\nfor i, ax in enumerate(axes):\n    sample_entry = random.choice(test_data)\n    image_path = sample_entry[\"saved_path\"]\n    title = f\"Category ID: {sample_entry['category_id']}\"\n    visualize_image_with_bbox(image_path, [0,0,0,0], title, ax)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:09:14.036666Z","iopub.execute_input":"2024-12-29T03:09:14.037010Z","iopub.status.idle":"2024-12-29T03:09:16.612852Z","shell.execute_reply.started":"2024-12-29T03:09:14.036979Z","shell.execute_reply":"2024-12-29T03:09:16.611983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        \"\"\"\n        Inizializza il dataset.\n\n        :param json_file: Percorso del file JSON contenente le informazioni sulle regioni.\n        :param transform: Trasformazioni da applicare alle immagini. Se non fornito, vengono usate trasformazioni di default.\n        \"\"\"\n        # Carica il file JSON\n        self.data = load_json(json_file)\n        \n        # Trasformazioni di default se non vengono fornite\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),         # Ridimensiona l'immagine a 224x224\n            transforms.ToTensor(),                  # Converte l'immagine in un tensore\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizzazione per i modelli pre-addestrati\n        ])  \n\n    def __len__(self):\n        \"\"\"Restituisce il numero totale di immagini/proposte nel dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Restituisce un esempio (immagine e etichetta) per l'addestramento.\"\"\"\n        # Carica l'esempio dal file JSON\n        sample = self.data[idx]\n        \n        # Carica l'immagine\n        image = Image.open(sample[\"saved_path\"]).convert(\"RGB\")\n        \n        # Etichetta della categoria\n        label = sample[\"category_id\"]  # Categoria della proposta\n\n        # Applica le trasformazioni\n        image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:07:02.634207Z","iopub.execute_input":"2024-12-30T12:07:02.635055Z","iopub.status.idle":"2024-12-30T12:07:02.641343Z","shell.execute_reply.started":"2024-12-30T12:07:02.635020Z","shell.execute_reply":"2024-12-30T12:07:02.640428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ds = CustomDataset(test_path)\ntrain_ds = CustomDataset(train_path)\nval_ds = CustomDataset(val_path)\n\nTrainLoader = DataLoader(train_ds, batch_size=32, shuffle=True)\nValLoader = DataLoader(val_ds, batch_size=32, shuffle=False)\nTestLoader = DataLoader(test_ds, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:15:54.436480Z","iopub.execute_input":"2024-12-29T18:15:54.436855Z","iopub.status.idle":"2024-12-29T18:15:54.553912Z","shell.execute_reply.started":"2024-12-29T18:15:54.436823Z","shell.execute_reply":"2024-12-29T18:15:54.553207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(AlexNet, self).__init__()\n        self._output_num = num_classes\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n     \n        self.drop8 = nn.Dropout()\n        self.fn8 = nn.Linear(256 * 6 * 6, 4096)\n        self.active8 = nn.ReLU(inplace=True)\n        \n        self.drop9 = nn.Dropout()\n        self.fn9 = nn.Linear(4096, 4096)\n        self.active9 = nn.ReLU(inplace=True)\n        \n        self.fn10 = nn.Linear(4096, self._output_num)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.drop8(x)\n        x = self.fn8(x)\n        x = self.active8(x)\n\n        x = self.drop9(x)\n        x = self.fn9(x)\n        \n        feature = self.active9(x)  \n        final = self.fn10(feature)\n\n        return feature, final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T10:04:33.478950Z","iopub.execute_input":"2024-12-29T10:04:33.479307Z","iopub.status.idle":"2024-12-29T10:04:33.492348Z","shell.execute_reply.started":"2024-12-29T10:04:33.479276Z","shell.execute_reply":"2024-12-29T10:04:33.491384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 12 #11 classi + sfondo\nnet = AlexNet(num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T10:04:35.581384Z","iopub.execute_input":"2024-12-29T10:04:35.582026Z","iopub.status.idle":"2024-12-29T10:04:36.063292Z","shell.execute_reply.started":"2024-12-29T10:04:35.581974Z","shell.execute_reply":"2024-12-29T10:04:36.062323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(net)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:34:37.601695Z","iopub.execute_input":"2024-12-29T03:34:37.602591Z","iopub.status.idle":"2024-12-29T03:34:37.608048Z","shell.execute_reply.started":"2024-12-29T03:34:37.602528Z","shell.execute_reply":"2024-12-29T03:34:37.606853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_loss(train_losses, val_losses):\n    \"\"\"\n    Funzione per fare il plot della funzione di loss durante il training e la validazione.\n\n    :param train_losses: Lista delle perdite durante il training.\n    :param val_losses: Lista delle perdite durante la validazione.\n    \"\"\"\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", linestyle='-', marker='o')\n    plt.plot(epochs, val_losses, label=\"Validation Loss\", color=\"red\", linestyle='-', marker='x')\n    \n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T19:39:23.382361Z","iopub.execute_input":"2024-12-29T19:39:23.382720Z","iopub.status.idle":"2024-12-29T19:39:23.388496Z","shell.execute_reply.started":"2024-12-29T19:39:23.382688Z","shell.execute_reply":"2024-12-29T19:39:23.387748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(net, train_loader, val_loader, criterion, optimizer, device, epochs, path_min_loss, num_classes, clip_value=1.0):\n    min_val_loss = float('inf')\n    \n    # Liste per registrare le perdite durante il training e la validazione\n    train_losses = []\n    val_losses = []\n    \n    # Dizionario per salvare le feature\n    train_features = {}  # Chiave: epoch, Valore: lista di feature\n\n    # Aggiungi un learning rate scheduler per ridurre il learning rate quando la loss di validazione non migliora\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n\n    for epoch in range(epochs):\n        net.train()  # Modalità training\n        train_loss = 0.0\n        correct_train = 0\n        total_train = 0\n        epoch_features = []  # Per registrare le feature di ogni batch\n        class_probs = torch.zeros((len(train_loader.dataset), num_classes)).to(device)  # Probabilità per classe\n\n        # Barra di avanzamento per il training\n        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", leave=False)\n\n        for i, (images, labels) in enumerate(train_progress):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            features, outputs = net(images)\n\n            # Salva le feature per il batch corrente\n            epoch_features.append(features.detach().cpu().numpy())\n\n            # Calcolo della loss pesata\n            loss = criterion(outputs, labels)\n\n            # Predizione della classe con la probabilità massima\n            _, predicted = outputs.max(1)\n\n            # Calcola le probabilità per ciascuna classe (softmax)\n            probs = torch.softmax(outputs, dim=1)\n            class_probs[i * images.size(0):(i + 1) * images.size(0)] = probs\n\n            # Monitoraggio dei gradienti\n            optimizer.zero_grad()\n            loss.backward()\n\n            # Clipping dei gradienti\n            utils.clip_grad_norm_(net.parameters(), clip_value)\n\n            optimizer.step()\n\n            # Statistiche\n            train_loss += loss.item() * images.size(0)\n            total_train += labels.size(0)\n            correct_train += predicted.eq(labels).sum().item()\n\n            # Aggiorna la barra di avanzamento con la loss corrente\n            train_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_train / total_train)\n\n        # Salva le feature per l'epoca corrente\n        train_features[epoch] = epoch_features\n\n        avg_train_loss = train_loss / len(train_loader.dataset)\n        train_accuracy = 100. * correct_train / total_train\n        train_losses.append(avg_train_loss)\n\n        # Calcola la probabilità media per ogni classe\n        avg_class_probs = class_probs.mean(dim=0)\n        print(f\"Probabilità media per classe in epoca {epoch+1}: {avg_class_probs}\")\n\n        # Validazione\n        net.eval()  # Modalità validazione\n        val_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        # Barra di avanzamento per la validazione\n        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\", leave=False)\n\n        with torch.no_grad():\n            for images, labels in val_progress:\n                images, labels = images.to(device), labels.to(device)\n\n                # Forward pass\n                _, outputs = net(images)\n                \n                # Predizione della classe con la probabilità massima\n                _, predicted = outputs.max(1)\n\n                # Calcolo della loss\n                loss = criterion(outputs, labels)\n\n                # Statistiche\n                val_loss += loss.item() * images.size(0)\n                total_val += labels.size(0)\n                correct_val += predicted.eq(labels).sum().item()\n\n                # Aggiorna la barra di avanzamento con la loss e accuracy\n                val_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_val / total_val)\n\n        avg_val_loss = val_loss / len(val_loader.dataset)\n        val_accuracy = 100. * correct_val / total_val\n        val_losses.append(avg_val_loss)\n\n        # Salva il modello con la loss di validazione più bassa\n        if avg_val_loss < min_val_loss:\n            print(f\"Salvataggio del miglior modello: Val Loss migliorata da {min_val_loss:.4f} a {avg_val_loss:.4f}\")\n            min_val_loss = avg_val_loss\n            torch.save(net.state_dict(), path_min_loss)\n\n        # Aggiorna il learning rate in base alla loss di validazione\n        scheduler.step(avg_val_loss)\n\n        # Stampa statistiche per epoca\n        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n    print(\"Training completato!\")\n\n    # Chiamata alla funzione per tracciare il grafico\n    plot_loss(train_losses, val_losses)\n\n    return train_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:16:07.024988Z","iopub.execute_input":"2024-12-29T18:16:07.025307Z","iopub.status.idle":"2024-12-29T18:16:07.039041Z","shell.execute_reply.started":"2024-12-29T18:16:07.025281Z","shell.execute_reply":"2024-12-29T18:16:07.038159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(net, test_loader, criterion, device, path_min_loss, json_path, original_img_path, reference_json_path):\n    \"\"\"\n    Funzione per testare il modello e visualizzare i risultati su immagini originali.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    :param json_path: Percorso al file JSON contenente i dettagli delle immagini del test set.\n    :param original_img_path: Percorso al folder delle immagini originali.\n    :param reference_json_path: Percorso al file JSON contenente le informazioni delle immagini originali.\n    \"\"\"\n    # Carica il miglior modello salvato (con il parametro weights_only=True per evitare il warning)\n    net.load_state_dict(torch.load(path_min_loss, map_location=device))\n    net.eval()\n\n    test_loss = 0.0\n    correct_test = 0\n    total_test = 0\n\n    all_labels = []\n    all_predictions = []\n\n    # Carica i file JSON\n    with open(json_path, 'r') as f:\n        test_data = json.load(f)\n\n    with open(reference_json_path, 'r') as f:\n        reference_data = json.load(f)\n\n    # Crea un dizionario per una ricerca rapida delle immagini originali\n    id_to_filename = {img['id']: img['file_name'] for img in reference_data['images']}\n\n    # Crea un dizionario per raggruppare i bounding box per image_id\n    image_id_to_bboxes = {}\n    for image_info in test_data:\n        image_id = image_info['image_id']\n        region_bbox = image_info['region_bbox']\n        if image_id not in image_id_to_bboxes:\n            image_id_to_bboxes[image_id] = []\n        image_id_to_bboxes[image_id].append(region_bbox)\n\n    with torch.no_grad():\n        for idx, (images, labels) in enumerate(test_loader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            test_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_test += labels.size(0)\n            correct_test += predicted.eq(labels).sum().item()\n\n            # Salva tutte le etichette e predizioni\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n            # Mostra alcuni esempi\n            if idx < 5:  # Mostra i primi 5 batch\n                for i in range(min(len(images), 3)):  # Mostra fino a 3 immagini per batch\n                    image_info = test_data[idx * len(images) + i]  # Recupera info immagine dal JSON\n                    image_id = image_info['image_id']\n\n                    # Trova il file_name usando l'image_id\n                    file_name = id_to_filename.get(image_id)\n                    if not file_name:\n                        print(f\"Immagine con ID {image_id} non trovata nel reference JSON.\")\n                        continue\n\n                    # Percorso dell'immagine originale\n                    img_path = os.path.join(original_img_path, file_name)\n\n                    # Carica l'immagine originale\n                    img = cv2.imread(img_path)\n                    if img is None:\n                        print(f\"Immagine non trovata: {img_path}\")\n                        continue\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n                    # Disegna i bounding box per le etichette reali (blu)\n                    bboxes_real = image_id_to_bboxes.get(image_id, [])\n                    for bbox in bboxes_real:\n                        xmin, ymin, xmax, ymax = bbox\n                        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)  # Blu per reale\n\n                    # Mostra l'immagine con i bounding box (senza le etichette scritte nell'immagine)\n                    plt.figure(figsize=(6, 6))\n                    plt.imshow(img)\n                    plt.axis('off')\n                    plt.show()\n\n                    # Ora stampa le etichette predette e reali sotto l'immagine\n                    print(f\"Predizioni vs Realtà per l'immagine {file_name}:\")\n                    for j, bbox in enumerate(bboxes_real):\n                        pred_label = predicted[i].item()\n                        # Stampa i valori predetti e reali\n                        print(f\"  Bounding Box {j + 1}: Predetto: {pred_label}, Reale: {labels[i].item()}\")\n\n    avg_test_loss = test_loss / len(test_loader.dataset)\n    test_accuracy = 100. * correct_test / total_test\n\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n\n    # Calcola e visualizza la matrice di confusione\n    cm = confusion_matrix(all_labels, all_predictions)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizza per riga\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:16:09.743225Z","iopub.execute_input":"2024-12-29T18:16:09.743535Z","iopub.status.idle":"2024-12-29T18:16:09.757098Z","shell.execute_reply.started":"2024-12-29T18:16:09.743507Z","shell.execute_reply":"2024-12-29T18:16:09.756312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.001\nepochs = 3\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\nnet = AlexNet(num_classes)\ndevice = torch.device(\"cuda\")\nnet = net.to(device)\n\ncriterion = criterion.to(device)\n\npath_min_loss = '/kaggle/working/AlexNet.pth'\n\ntrain_features = train_model(\n    net=net,\n    train_loader=TrainLoader,\n    val_loader=ValLoader,\n    criterion=criterion,\n    optimizer=optimizer,\n    device=device,\n    epochs=epochs,\n    path_min_loss=path_min_loss,\n    num_classes = num_classes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:37:19.459326Z","iopub.execute_input":"2024-12-29T03:37:19.459749Z","iopub.status.idle":"2024-12-29T03:43:23.969325Z","shell.execute_reply.started":"2024-12-29T03:37:19.459711Z","shell.execute_reply":"2024-12-29T03:43:23.968592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\n    net=net,\n    test_loader=TestLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=path_min_loss,\n    json_path = test_path,\n    original_img_path = img_fldr,\n    reference_json_path = in_new_coco_json_pth\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:46:41.540509Z","iopub.execute_input":"2024-12-29T03:46:41.540960Z","iopub.status.idle":"2024-12-29T03:47:08.624853Z","shell.execute_reply.started":"2024-12-29T03:46:41.540923Z","shell.execute_reply":"2024-12-29T03:47:08.623508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Numero di classi\nnum_classes = 12\n\n# Modello pre-addestrato ResNet50\nresnet50 = models.resnet50(pretrained=False)\n\n# Modifica l'ultimo layer completamente connesso per il numero di classi\nin_features = resnet50.fc.in_features  # Estrai le caratteristiche in input dell'ultimo layer\nresnet50.fc = nn.Linear(in_features, num_classes)  # Nuovo layer di classificazione\n\n# Passa il modello alla GPU (se disponibile)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresnet50 = resnet50.to(device)\n\n# Modifica il forward per restituire sia le feature che le predizioni finali\nclass ResNet50FeatureExtractor(nn.Module):\n    def __init__(self, model):\n        super(ResNet50FeatureExtractor, self).__init__()\n        self.resnet = model\n        # Rimuove l'ultimo layer\n        self.features = nn.Sequential(*list(self.resnet.children())[:-1])  # Rimuove il Fully Connected finale\n\n    def forward(self, x):\n        # Ottieni le feature intermedie e poi la predizione finale\n        features = self.features(x)\n        features = features.view(features.size(0), -1)  # Flattening per la classificazione\n        outputs = self.resnet.fc(features)\n        return features, outputs\n\n# Crea il nuovo modello con estrazione delle feature\nmodel = ResNet50FeatureExtractor(resnet50)\nmodel = model.to(device)\n\n# Funzione di perdita (loss) e ottimizzatore\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n\n# Numero di epoche\nepochs = 15\n\n# Funzione di training\ntrain_features = train_model(\n    net=model,\n    train_loader=TrainLoader,\n    val_loader=ValLoader,\n    criterion=criterion,\n    optimizer=optimizer,\n    device=device,\n    epochs=epochs,\n    path_min_loss=path_min_loss,\n    num_classes=num_classes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T20:40:30.683613Z","iopub.execute_input":"2024-12-29T20:40:30.683955Z","iopub.status.idle":"2024-12-29T22:01:30.627457Z","shell.execute_reply.started":"2024-12-29T20:40:30.683924Z","shell.execute_reply":"2024-12-29T22:01:30.626132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save(train_features_path, train_features)  # Salva in formato .npy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T22:01:30.665139Z","iopub.execute_input":"2024-12-29T22:01:30.665383Z","iopub.status.idle":"2024-12-29T22:01:37.093935Z","shell.execute_reply.started":"2024-12-29T22:01:30.665360Z","shell.execute_reply":"2024-12-29T22:01:37.092662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\n    net=model,\n    test_loader=TestLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=path_min_loss,\n    json_path = test_path,\n    original_img_path = img_fldr,\n    reference_json_path = in_new_coco_json_pth\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T22:01:37.096573Z","iopub.execute_input":"2024-12-29T22:01:37.096974Z","iopub.status.idle":"2024-12-29T22:02:03.227881Z","shell.execute_reply.started":"2024-12-29T22:01:37.096928Z","shell.execute_reply":"2024-12-29T22:02:03.226924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Box Regressor","metadata":{}},{"cell_type":"code","source":"def test_model_regressor(net, test_loader, criterion, device, path_min_loss, pred_json_path):\n    \"\"\"\n    Funzione per testare il modello, sostituire il category_id con quello predetto e visualizzare i risultati su immagini originali.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    :param json_path: Percorso al file JSON contenente i dettagli delle immagini del test set.\n    :param pred_json_path: Percorso al file JSON da modificare con i category_id predetti.\n    \"\"\"\n    # Carica il miglior modello salvato\n    net.load_state_dict(torch.load(path_min_loss, map_location=device))\n    net.eval()\n\n    all_labels = []\n    all_predictions = []\n\n    # Carica il JSON con le informazioni delle immagini da modificare\n    with open(pred_json_path, 'r') as f:\n        pred_data = json.load(f)\n\n    with torch.no_grad():\n        for idx, (images, labels) in enumerate(test_loader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            _, predicted = outputs.max(1)\n\n            # Salva tutte le etichette e predizioni\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n            # Modifica il JSON con le predizioni\n            for i in range(len(images)):\n                image_info = pred_data[idx * len(images) + i]  # Recupera info immagine dal JSON\n                image_id = image_info['image_id']\n\n                # Trova l'elemento nel pred_json e sostituisci il category_id con quello predetto\n                for entry in pred_data:\n                    if entry['image_id'] == image_id:\n                        entry['category_id'] = int(predicted[i].item())\n\n    # Salva il JSON modificato\n    with open(pred_json_path, 'w') as f:\n        json.dump(pred_data, f, indent=4)\n\n    # Calcola la test accuracy\n    test_accuracy = 100. * sum(np.array(all_labels) == np.array(all_predictions)) / len(all_labels)\n    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n\n    # Calcola e visualizza la matrice di confusione\n    cm = confusion_matrix(all_labels, all_predictions)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizza per riga\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:07:17.802390Z","iopub.execute_input":"2024-12-30T12:07:17.802763Z","iopub.status.idle":"2024-12-30T12:07:17.812545Z","shell.execute_reply.started":"2024-12-30T12:07:17.802733Z","shell.execute_reply":"2024-12-30T12:07:17.811679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carica i dati JSON per val e test\nval_json = load_json(val_path)\ntest_json = load_json(test_path)\nreg_json = val_json + test_json\n\n# Salva il JSON unito in un nuovo file\nwith open(reg_path, 'w') as f_merged:\n    json.dump(reg_json, f_merged, indent=4)\n    \nreg_ds = CustomDataset(reg_path)\nRegLoader = DataLoader(reg_ds, batch_size=32, shuffle=False)\n\n# Chiama la funzione di test\ntest_model_regressor(\n    net=model,\n    test_loader=RegLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=resnet_min_loss_path,\n    pred_json_path=reg_path\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-30T12:07:25.068006Z","iopub.execute_input":"2024-12-30T12:07:25.068380Z","iopub.status.idle":"2024-12-30T12:07:25.569564Z","shell.execute_reply.started":"2024-12-30T12:07:25.068351Z","shell.execute_reply":"2024-12-30T12:07:25.568437Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Box Regressor","metadata":{}},{"cell_type":"code","source":"class BoundingBoxRegressor:\n    def __init__(self, lambda_reg=1000):  \n        self.lambda_reg = lambda_reg\n        self.category_weights = defaultdict(dict)  # Salva pesi per ogni categoria\n\n    def parse_json(self, json_data):\n        \"\"\"\n        Estrae proposals, ground truths e category_id da un JSON strutturato,\n        ignorando le box con category_id = 0 per il training, ma mantenendole per il JSON finale.\n        \"\"\"\n        proposals = []\n        ground_truths = []\n        categories = []\n        filtered_data = []\n        background_data = []  # Dati per la categoria 0 (background)\n    \n        for item in json_data:\n            category_id = item[\"category_id\"]\n            \n            # Se la categoria è 0, salva come background\n            if category_id == 0:\n                background_data.append(item)\n                continue\n            \n            # Verifica validità di original_bbox e region_bbox\n            original_bbox = item.get(\"original_bbox\", None)\n            region_bbox = item.get(\"region_bbox\", None)\n            \n            if (\n                original_bbox and len(original_bbox) == 4 and \n                region_bbox and len(region_bbox) == 4\n            ):\n                # Converto ground truth (original_bbox) da (x_min, y_min, x_max, y_max) a (x, y, width, height)\n                x_min, y_min, x_max, y_max = original_bbox\n                width = x_max - x_min\n                height = y_max - y_min\n                ground_truths.append([x_min, y_min, width, height])\n\n                # Converto proposal (region_bbox) da (x_min, y_min, x_max, y_max) a (x, y, width, height)\n                x_min, y_min, x_max, y_max = region_bbox\n                width = x_max - x_min\n                height = y_max - y_min\n                proposals.append([x_min, y_min, width, height])\n            else:\n                continue\n            \n            categories.append(category_id)\n            filtered_data.append(item)\n\n        return np.array(proposals), np.array(ground_truths), np.array(categories), filtered_data, background_data\n        \n    def train(self, json_data):\n        \"\"\"\n        Addestra il regressore usando i dati JSON forniti.\n        \"\"\"\n        proposals, ground_truths, categories, _, _ = self.parse_json(json_data)\n        unique_categories = np.unique(categories)\n\n        for category in tqdm(unique_categories, desc=\"Training Categories\"):\n            # Filtra le proposte e i ground truth per la categoria\n            cat_indices = np.where(categories == category)[0]\n            cat_proposals = proposals[cat_indices]\n            cat_ground_truths = ground_truths[cat_indices]\n\n            if len(cat_proposals) == 0:\n                continue  # Salta se non ci sono dati per la categoria\n\n            N = len(cat_proposals)\n            targets = np.zeros((N, 4))  # tx, ty, tw, th\n\n            # Calcola i target di regressione\n            for i, (P, G) in enumerate(zip(cat_proposals, cat_ground_truths)):\n                Px, Py, Pw, Ph = P\n                Gx, Gy, Gw, Gh = G\n\n                targets[i, 0] = (Gx - Px) / Pw  # tx\n                targets[i, 1] = (Gy - Py) / Ph  # ty\n                targets[i, 2] = np.log(Gw / Pw)  # tw\n                targets[i, 3] = np.log(Gh / Ph)  # th\n\n            # Risoluzione dei pesi per ogni coordinata\n            for k, label in enumerate(['x', 'y', 'w', 'h']):\n                target_k = targets[:, k]\n                A = np.dot(cat_proposals.T, cat_proposals) + self.lambda_reg * np.eye(4)\n                b = np.dot(cat_proposals.T, target_k)\n                self.category_weights[category][label] = np.linalg.solve(A, b)\n\n    def predict(self, json_data, output_path):\n        \"\"\"\n        Predice i bounding box corretti per ogni categoria dato un insieme di dati JSON.\n        Salva il risultato in un nuovo file JSON.\n        \"\"\"\n        proposals, _, categories, filtered_data, background_data = self.parse_json(json_data)\n        unique_categories = np.unique(categories)\n\n        for category in tqdm(unique_categories, desc=\"Predicting Categories\"):\n            cat_indices = np.where(categories == category)[0]\n            cat_proposals = proposals[cat_indices]\n            \n            if len(cat_proposals) == 0:\n                continue  # Salta se non ci sono proposte per la categoria\n            \n            for rel_idx, abs_idx in enumerate(cat_indices):\n                Px, Py, Pw, Ph = cat_proposals[rel_idx]\n\n                dx = np.dot(cat_proposals[rel_idx], self.category_weights[category]['x'])\n                dy = np.dot(cat_proposals[rel_idx], self.category_weights[category]['y'])\n                dw = np.dot(cat_proposals[rel_idx], self.category_weights[category]['w'])\n                dh = np.dot(cat_proposals[rel_idx], self.category_weights[category]['h'])\n\n                # Predizione del bounding box corretto (nel formato x, y, width, height)\n                Gx_hat = Pw * dx + Px\n                Gy_hat = Ph * dy + Py\n                Gw_hat = Pw * np.exp(dw)\n                Gh_hat = Ph * np.exp(dh)\n\n                # Conversione delle coordinate predette in formato (x_min, y_min, x_max, y_max)\n                x_min = Gx_hat\n                y_min = Gy_hat\n                x_max = Gx_hat + Gw_hat\n                y_max = Gy_hat + Gh_hat\n\n                # Aggiorna il JSON con il nuovo bounding box nel formato (x_min, y_min, x_max, y_max)\n                filtered_data[abs_idx]['region_bbox'] = [x_min, y_min, x_max, y_max]\n\n        # Riaggiungi i dati della categoria 0 (background)\n        filtered_data.extend(background_data)\n\n        # Salva il risultato nel file JSON di output\n        with open(output_path, 'w') as outfile:\n            json.dump(filtered_data, outfile, indent=4)\n\n        print(f\"File salvato in: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:17:04.480721Z","iopub.execute_input":"2024-12-31T12:17:04.481098Z","iopub.status.idle":"2024-12-31T12:17:04.500823Z","shell.execute_reply.started":"2024-12-31T12:17:04.481062Z","shell.execute_reply":"2024-12-31T12:17:04.499496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training\ntrain_json = load_json(in_train_path)\nregressor = BoundingBoxRegressor()\nregressor.train(train_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:17:07.382700Z","iopub.execute_input":"2024-12-31T12:17:07.383712Z","iopub.status.idle":"2024-12-31T12:17:07.815530Z","shell.execute_reply.started":"2024-12-31T12:17:07.383638Z","shell.execute_reply":"2024-12-31T12:17:07.811499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predizione\nreg_json = load_json(in_reg_path)\nregressor.predict(reg_json, out_reg_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:17:09.859154Z","iopub.execute_input":"2024-12-31T12:17:09.859530Z","iopub.status.idle":"2024-12-31T12:17:10.082816Z","shell.execute_reply.started":"2024-12-31T12:17:09.859496Z","shell.execute_reply":"2024-12-31T12:17:10.081695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 12\n\nnew_reg_json = load_json(out_reg_path)\nmodel = resnet_min_loss_path\n\nreg_ds = CustomDataset(out_reg_path)\nRegLoader = DataLoader(reg_ds, batch_size=32, shuffle=False)\n\n# Modello pre-addestrato ResNet50\nresnet50 = models.resnet50(pretrained=False)\n\n# Modifica l'ultimo layer completamente connesso per il numero di classi\nin_features = resnet50.fc.in_features  # Estrai le caratteristiche in input dell'ultimo layer\nresnet50.fc = nn.Linear(in_features, num_classes)  # Nuovo layer di classificazione\n\n# Passa il modello alla GPU (se disponibile)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresnet50 = resnet50.to(device)\n\n# Modifica il forward per restituire sia le feature che le predizioni finali\nclass ResNet50FeatureExtractor(nn.Module):\n    def __init__(self, model):\n        super(ResNet50FeatureExtractor, self).__init__()\n        self.resnet = model\n        # Rimuove l'ultimo layer\n        self.features = nn.Sequential(*list(self.resnet.children())[:-1])  # Rimuove il Fully Connected finale\n\n    def forward(self, x):\n        # Ottieni le feature intermedie e poi la predizione finale\n        features = self.features(x)\n        features = features.view(features.size(0), -1)  # Flattening per la classificazione\n        outputs = self.resnet.fc(features)\n        return features, outputs\n\n# Crea il nuovo modello con estrazione delle feature\nmodel = ResNet50FeatureExtractor(resnet50)\nmodel = model.to(device)\n\n# Funzione di perdita (loss) e ottimizzatore\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n\n# Chiama la funzione di test\ntest_model_regressor(\n    net=model,\n    test_loader=RegLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=resnet_min_loss_path,\n    pred_json_path=out_reg_path\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:07:59.917647Z","iopub.execute_input":"2024-12-30T12:07:59.918026Z","iopub.status.idle":"2024-12-30T12:09:12.281425Z","shell.execute_reply.started":"2024-12-30T12:07:59.917996Z","shell.execute_reply":"2024-12-30T12:09:12.280583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_region_boxes(image_path, regions1, regions2, labels1, labels2, image_title, title1, title2):\n    \"\"\"\n    Visualizza un'immagine con due set di region box e relative etichette affiancati.\n\n    Parameters:\n        image_path (str): Il percorso dell'immagine.\n        regions1 (list): Lista di regioni per la prima immagine ([x_min, y_min, x_max, y_max]).\n        regions2 (list): Lista di regioni per la seconda immagine ([x_min, y_min, x_max, y_max]).\n        labels1 (list): Lista di category_id per la prima immagine.\n        labels2 (list): Lista di category_id per la seconda immagine.\n        image_title (str): Titolo generale per la coppia di immagini (nome dell'immagine).\n        title1 (str): Sottotitolo per la prima immagine.\n        title2 (str): Sottotitolo per la seconda immagine.\n    \"\"\"\n    # Apri l'immagine\n    img = Image.open(image_path)\n    \n    # Crea una figura con due subplot affiancati\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    fig.suptitle(image_title, fontsize=16)  # Titolo generale per la coppia\n    \n    # Mostra la prima immagine con i box e le etichette\n    axes[0].imshow(img)\n    for region, label in zip(regions1, labels1):\n        x_min, y_min, x_max, y_max = region\n        width = x_max - x_min\n        height = y_max - y_min\n        rect = plt.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        axes[0].add_patch(rect)\n        axes[0].text(x_min, y_min - 5, str(label), color='r', fontsize=8, ha='left', backgroundcolor='white')\n    axes[0].set_title(title1, fontsize=12)\n    axes[0].axis('off')\n    \n    # Mostra la seconda immagine con i box e le etichette\n    axes[1].imshow(img)\n    for region, label in zip(regions2, labels2):\n        x_min, y_min, x_max, y_max = region\n        width = x_max - x_min\n        height = y_max - y_min\n        rect = plt.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        axes[1].add_patch(rect)\n        axes[1].text(x_min, y_min - 5, str(label), color='r', fontsize=8, ha='left', backgroundcolor='white')\n    axes[1].set_title(title2, fontsize=12)\n    axes[1].axis('off')\n    \n    # Mostra la figura\n    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adatta layout lasciando spazio per il titolo generale\n    plt.show()\n\ndef visualize_boxes(reg_file, new_reg_file, inactproposals_file, img_folder, selected_files=None):\n    \"\"\"\n    Visualizza immagini affiancate con region box sovrapposti e relative etichette da due file JSON.\n\n    Parameters:\n        reg_file (str): Percorso del file reg.json.\n        new_reg_file (str): Percorso del file new_reg.json.\n        inactproposals_file (str): Percorso del file inactproposals_json.\n        img_folder (str): Percorso della cartella contenente le immagini.\n        selected_files (list, optional): Lista di nomi di file immagine da visualizzare. Se None, usa tutte le immagini.\n    \"\"\"\n    # Carica i dati dai file JSON\n    with open(reg_file, 'r') as f:\n        reg_data = json.load(f)\n\n    with open(new_reg_file, 'r') as f:\n        new_reg_data = json.load(f)\n\n    with open(inactproposals_file, 'r') as f:\n        inactproposals_data = json.load(f)\n\n    # Mappa i file_name agli image_id usando inactproposals_json\n    file_to_id = {item['file_name']: item['image_id'] for item in inactproposals_data}\n\n    # Filtra le immagini se `selected_files` è specificato\n    files_to_visualize = selected_files if selected_files else file_to_id.keys()\n\n    # Itera sulle immagini selezionate\n    for file_name in files_to_visualize:\n        if file_name not in file_to_id:\n            print(f\"File {file_name} non trovato in inactproposals_json.\")\n            continue\n\n        image_id = file_to_id[file_name]\n\n        # Trova tutti i region box e i category_id associati a questo image_id nei due file JSON\n        reg_bboxes = []\n        reg_labels = []\n        for item in reg_data:\n            if item['image_id'] == image_id:\n                reg_bboxes.append(item['region_bbox'])\n                reg_labels.append(item['category_id'])  # Usa il category_id come label\n\n        new_reg_bboxes = []\n        new_reg_labels = []\n        for item in new_reg_data:\n            if item['image_id'] == image_id:\n                new_reg_bboxes.append(item['region_bbox'])\n                new_reg_labels.append(item['category_id'])  # Usa il category_id come label\n\n        # Costruisci il percorso completo dell'immagine\n        image_path = os.path.join(img_folder, file_name)\n\n        if not os.path.exists(image_path):\n            print(f\"L'immagine {image_path} non esiste.\")\n            continue\n\n        # Visualizza l'immagine con i box affiancati e i category_id\n        plot_region_boxes(\n            image_path, \n            reg_bboxes, \n            new_reg_bboxes, \n            reg_labels, \n            new_reg_labels, \n            f\"Immagine: {file_name}\",  # Titolo generale\n            \"Boxes prima del Box Regressor\",  # Sottotitolo immagine 1\n            \"Boxes dopo il Box Regressor\"    # Sottotitolo immagine 2\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:17:14.466170Z","iopub.execute_input":"2024-12-31T12:17:14.466516Z","iopub.status.idle":"2024-12-31T12:17:14.782897Z","shell.execute_reply.started":"2024-12-31T12:17:14.466484Z","shell.execute_reply":"2024-12-31T12:17:14.781532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_boxes(in_reg_path, out_reg_path, in_actproposals_json, img_fldr, selected_files=[\"img_1896_0_2560.jpg\", \"img_2008_320_640.jpg\", \"img_1114_1280_960.jpg\", \"img_322_1280_2240.jpg\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:17:18.443202Z","iopub.execute_input":"2024-12-31T12:17:18.443640Z","iopub.status.idle":"2024-12-31T12:17:21.488924Z","shell.execute_reply.started":"2024-12-31T12:17:18.443597Z","shell.execute_reply":"2024-12-31T12:17:21.487810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_top_images(json_data, top_n=4):\n    # Un dizionario per contare il numero di bbox per ciascun image_id per categoria\n    category_image_bbox_count = defaultdict(lambda: defaultdict(int))\n\n    # Itera attraverso i dati JSON e conta i bbox per ciascun image_id e category_id\n    for entry in json_data:\n        image_id = entry['image_id']\n        category_id = entry['category_id']  # Supponiamo che la categoria sia in 'category_id'\n        category_image_bbox_count[category_id][image_id] += 1\n\n    # Un dizionario per memorizzare i risultati finali\n    top_images_by_category = {}\n\n    # Per ogni categoria, ordina le immagini in base al numero di bbox (in ordine decrescente)\n    for category_id, image_count in category_image_bbox_count.items():\n        sorted_images = sorted(image_count.items(), key=lambda x: x[1], reverse=True)\n        top_images_by_category[category_id] = sorted_images[:top_n]\n\n    return top_images_by_category\n\n# Carica il file JSON\nwith open(in_reg_path, 'r') as f:\n    data = json.load(f)\n\n# Chiamata alla funzione per trovare le immagini con più region\ntop_images = find_top_images(data)\n\n# Stampa i risultati per ogni categoria\nif top_images:\n    for category_id, top_images_list in top_images.items():\n        print(f\"\\nCategoria ID: {category_id}\")\n        for rank, (image_id, bbox_count) in enumerate(top_images_list, start=1):\n            print(f\"{rank}. Image ID: {image_id}, Numero di bbox: {bbox_count}\")\nelse:\n    print(\"Nessuna immagine trovata.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:00:25.214957Z","iopub.execute_input":"2024-12-31T12:00:25.215374Z","iopub.status.idle":"2024-12-31T12:00:25.263020Z","shell.execute_reply.started":"2024-12-31T12:00:25.215337Z","shell.execute_reply":"2024-12-31T12:00:25.261989Z"}},"outputs":[],"execution_count":null}]}