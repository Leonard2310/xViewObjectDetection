{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10182328,"sourceType":"datasetVersion","datasetId":6242793},{"sourceId":10329194,"sourceType":"datasetVersion","datasetId":6395444}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"pip install selectivesearch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:56.666679Z","iopub.execute_input":"2024-12-28T23:07:56.667003Z","iopub.status.idle":"2024-12-28T23:07:58.491938Z","shell.execute_reply.started":"2024-12-28T23:07:56.666976Z","shell.execute_reply":"2024-12-28T23:07:58.490903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Librerie per l'ottimizzazione e la gestione delle dipendenze\nimport selectivesearch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Librerie standard\nimport os\nimport random\nimport time\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom itertools import islice\n\n# Librerie per il trattamento delle immagini\nimport cv2\nimport imageio.v3 as imageio\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom torchvision.transforms import functional as TF\n\n# Librerie per il machine learning e deep learning\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as func\nimport torchvision.models as models\nfrom sklearn.svm import SVC\n\n# Librerie per la gestione dei dati\nimport pandas as pd\nimport json\nimport orjson\nimport shutil \n\n# Librerie per il parallelismo e il multiprocessing\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Librerie per il progresso e il monitoraggio\nfrom tqdm import tqdm\n\n# Librerie per la gestione dei dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# Librerie per modelli e trasformazioni in PyTorch\nfrom torchvision import transforms\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport zipfile\nimport torchvision\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport matplotlib.patches as patches\nimport torch.nn.utils as utils\nimport ast\nimport itertools","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:16:40.190776Z","iopub.execute_input":"2024-12-30T11:16:40.191686Z","iopub.status.idle":"2024-12-30T11:16:40.205195Z","shell.execute_reply.started":"2024-12-30T11:16:40.191632Z","shell.execute_reply":"2024-12-30T11:16:40.203658Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#Output folders and file names\nCOCO_JSON_NM = 'COCO_annotations_new.json'\nOUT_COCO_JSON_NM = 'mod_COCO_annotations.json'\nOUT_IMAGE_FLDR_NM = 'images'\nRANDOM_SEED = 2023\n\nin_dataset_pth = Path('/kaggle/input/our-xview-dataset')\nprop_dataset_pth = Path('/kaggle/input/proposals-xview-dataset-r-cnn')\nactreg_dataset_pth = Path('/kaggle/input/activeregion-xviewdataset')\nresnet_pth = Path('/kaggle/input/resnet-50-xview/pytorch/default')\nregbox_dataset_pth = Path('/kaggle/input/regbboxdataset-xview')\nout_dataset_pth = Path('/kaggle/working/')\nimg_fldr = Path(f'/kaggle/input/our-xview-dataset/{OUT_IMAGE_FLDR_NM}')\n\ncoco_json_pth = in_dataset_pth / COCO_JSON_NM\nnew_coco_json_pth = out_dataset_pth / OUT_COCO_JSON_NM\nin_new_coco_json_pth = prop_dataset_pth / OUT_COCO_JSON_NM\n\n# PROPOSALS\nPROP_COCO_JSON_NM = 'proposals.json'\nproposals_json = prop_dataset_pth / PROP_COCO_JSON_NM\nout_proposals_json = out_dataset_pth / PROP_COCO_JSON_NM\nPROPOSALS = 'proposals'\nprop_fldr = out_dataset_pth / PROPOSALS\n\n\n# ACTIVE REGIONS\nACTPROP_COCO_JSON_NM ='active_regions.json'\nactproposals_json = out_dataset_pth / ACTPROP_COCO_JSON_NM\nin_actproposals_json = actreg_dataset_pth / ACTPROP_COCO_JSON_NM\nout_act_reg_folder = out_dataset_pth / PROPOSALS\nact_reg_folder = actreg_dataset_pth / PROPOSALS\n\n#DATASET\ntrain_path = out_dataset_pth / 'train.json'\ntest_path = out_dataset_pth / 'test.json'\nval_path = out_dataset_pth / 'val.json'\nreg_path = out_dataset_pth / 'reg.json'\n\n# Percorso per salvare il modello\npath_min_loss = '/kaggle/working/ResNet50.pth'\n\ntrain_features_path = out_dataset_pth / 'train_features.npy'\n\nresnet_min_loss_path =  resnet_pth / '3/ResNet50.pth'\n\nin_train_path = regbox_dataset_pth / 'train.json'\nin_reg_path = regbox_dataset_pth / 'reg.json'\nin_train_features_path = regbox_dataset_pth / 'train_features.npy'\n\nout_reg_path = out_dataset_pth / 'new_reg.json'\n\nrandom.seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:16:42.676471Z","iopub.execute_input":"2024-12-30T11:16:42.676875Z","iopub.status.idle":"2024-12-30T11:16:42.688039Z","shell.execute_reply.started":"2024-12-30T11:16:42.676839Z","shell.execute_reply":"2024-12-30T11:16:42.686447Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Pulizia dell'output per cartelle specifiche\ndef clean_output(output_dir):\n    if output_dir.exists() and output_dir.is_dir():\n        for item in output_dir.iterdir():\n            if item.is_dir():\n                shutil.rmtree(item)  # Rimuove la sotto-cartella\n            else:\n                item.unlink()  # Rimuove il file\n        print(f\"Cartella {output_dir} pulita.\")\n    else:\n        print(f\"Cartella {output_dir} non trovata. Nessuna azione necessaria.\")\n\n# Pulisce la cartella di output prima di avviare il processo\nclean_output(out_dataset_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T00:02:20.793347Z","iopub.execute_input":"2024-12-30T00:02:20.794082Z","iopub.status.idle":"2024-12-30T00:02:20.800239Z","shell.execute_reply.started":"2024-12-30T00:02:20.794042Z","shell.execute_reply":"2024-12-30T00:02:20.799432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utility","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# Sopprime i warning specifici del modulo skimage\nwarnings.filterwarnings(\"ignore\", \n    message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results.*\")\n\n# Sopprime i warning relativi al parametro 'pretrained' deprecato\nwarnings.filterwarnings(\"ignore\", \n    message=\"The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\")\n\n# Sopprime i warning relativi agli argomenti diversi da weight enum o None\nwarnings.filterwarnings(\"ignore\", \n    message=\"Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future.*\")\n\n# Sopprime i warning relativi al parametro 'verbose' deprecato\nwarnings.filterwarnings(\"ignore\", \n    message=\"The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\")\n\n# Sopprime i warning relativi a `torch.load` e `weights_only=False`\nwarnings.filterwarnings(\"ignore\", \n    message=\"You are using `torch.load` with `weights_only=False`.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:16:46.108734Z","iopub.execute_input":"2024-12-30T11:16:46.109780Z","iopub.status.idle":"2024-12-30T11:16:46.117624Z","shell.execute_reply.started":"2024-12-30T11:16:46.109724Z","shell.execute_reply":"2024-12-30T11:16:46.115822Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def load_json(file_path):\n    \"\"\"\n    Carica un file JSON dal percorso specificato.\n\n    :param file_path: Percorso al file JSON da caricare.\n    :return: Dati contenuti nel file JSON (come dizionario o lista).\n    \"\"\"\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:16:47.750815Z","iopub.execute_input":"2024-12-30T11:16:47.751323Z","iopub.status.idle":"2024-12-30T11:16:47.758821Z","shell.execute_reply.started":"2024-12-30T11:16:47.751283Z","shell.execute_reply":"2024-12-30T11:16:47.757233Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Test Dataset","metadata":{}},{"cell_type":"code","source":"def display_images_with_bboxes(json_file, specific_images, images_folder):\n    \"\"\"\n    Visualizza le immagini specificate con tutti i bounding box sopra di esse.\n    \n    :param json_file: percorso del file JSON contenente le immagini, annotazioni e categorie\n    :param specific_images: lista di nomi delle immagini da visualizzare\n    :param images_folder: percorso della cartella che contiene le immagini\n    \"\"\"\n    # Carica il JSON\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    \n    # Estrai le immagini e le annotazioni\n    images = data[\"images\"]\n    annotations = data[\"annotations\"]\n    \n    # Crea un dizionario per mappare l'id delle immagini al nome del file\n    image_dict = {image[\"id\"]: image[\"file_name\"] for image in images}\n    \n    # Filtra le annotazioni per le immagini specifiche\n    specific_annotations = [ann for ann in annotations if image_dict[ann[\"image_id\"]] in specific_images]\n\n    # Creiamo un dizionario per raccogliere tutte le annotazioni per ciascuna immagine\n    image_bboxes = {}\n    for annotation in specific_annotations:\n        image_name = image_dict[annotation[\"image_id\"]]\n        if image_name not in image_bboxes:\n            image_bboxes[image_name] = []\n        bbox = ast.literal_eval(annotation[\"bbox\"])\n        image_bboxes[image_name].append(bbox)\n    \n    # Visualizza tutte le immagini con tutti i bounding box\n    for image_name, bboxes in image_bboxes.items():\n        # Carica l'immagine\n        image_path = f'{images_folder}/{image_name}'  # Usa il percorso corretto per le immagini\n        image = Image.open(image_path)\n\n        # Crea la figura per la visualizzazione\n        plt.figure(figsize=(8, 8))\n        plt.imshow(image)\n\n        # Aggiungi tutti i bounding box\n        for bbox in bboxes:\n            x, y, w, h = bbox\n            plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none'))\n\n        # Imposta il titolo e disattiva gli assi\n        plt.title(f\"Image: {image_name}\")\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:14:55.234849Z","iopub.execute_input":"2024-12-29T18:14:55.235243Z","iopub.status.idle":"2024-12-29T18:14:55.249067Z","shell.execute_reply.started":"2024-12-29T18:14:55.235195Z","shell.execute_reply":"2024-12-29T18:14:55.247875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"specific_images = ['img_1831_0_2240.jpg', 'img_1964_0_1920.jpg', 'img_91_1920_0.jpg', 'img_1284_640_1920.jpg', 'img_128_640_960.jpg']\n\ndisplay_images_with_bboxes(coco_json_pth, specific_images, img_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:14:57.463262Z","iopub.execute_input":"2024-12-29T18:14:57.464086Z","iopub.status.idle":"2024-12-29T18:15:01.377339Z","shell.execute_reply.started":"2024-12-29T18:14:57.464044Z","shell.execute_reply":"2024-12-29T18:15:01.376397Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# COCO Preprocessing","metadata":{}},{"cell_type":"code","source":"def process_custom_coco_json(input_path, output_path):\n    \"\"\"\n    Funzione per processare un JSON COCO in formato personalizzato.\n    \"\"\"\n    # Leggi il JSON dal file di input\n    data = load_json(input_path)\n\n    # Ottieni e correggi il formato delle categorie\n    raw_categories = data.get('categories', [])\n    categories = []\n \n    for category in tqdm(raw_categories, desc=\"Processing Categories\"):\n        for id_str, name in category.items():\n            try:\n                categories.append({\"id\": int(id_str), \"name\": name})\n            except ValueError:\n                print(f\"Errore nel parsing della categoria: {category}\")\n \n    # Trova la categoria \"Aircraft\" con ID 0\n    aircraft_category = next((cat for cat in categories if cat['id'] == 0 and cat['name'] == \"Aircraft\"), None)\n    if aircraft_category:\n        aircraft_category['id'] = 11  # Cambia l'ID della categoria \"Aircraft\" a 11\n \n    # Aggiungi la categoria \"background\" con ID 0 se non esiste\n    if not any(cat['id'] == 0 for cat in categories):\n        categories.append({\"id\": 0, \"name\": \"background\"})\n \n    # Preprocessa le annotazioni in un dizionario per immagini\n    image_annotations_dict = {}\n    for annotation in tqdm(data.get('annotations', []), desc=\"Building Image Annotations Dictionary\"):\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_dict:\n            image_annotations_dict[image_id] = []\n        image_annotations_dict[image_id].append(annotation)\n \n    # Elenco di annotazioni da mantenere (solo quelle valide)\n    valid_annotations = []\n    annotations_to_remove = set()\n \n    # Controllo dei bounding box\n    for annotation in tqdm(data.get('annotations', []), desc=\"Processing Annotations\"):\n        if annotation['category_id'] == 0:  # Se è Aircraft\n            annotation['category_id'] = 11\n        \n        # Converte il formato del bbox\n        if isinstance(annotation['bbox'], str):\n            annotation['bbox'] = json.loads(annotation['bbox'])\n        \n        x, y, width, height = annotation['bbox']\n        xmin, xmax = x, x + width\n        ymin, ymax = y, y + height\n        \n        # Verifica che xmin < xmax e ymin < ymax, e che la larghezza e altezza siano sufficienti\n        if xmin >= xmax or ymin >= ymax or width <= 10 or height <= 10:\n            annotations_to_remove.add(annotation['id'])\n        else:\n            annotation['bbox'] = [xmin, ymin, xmax, ymax]\n            valid_annotations.append(annotation)\n \n    # Rimuovi le annotazioni non valide\n    data['annotations'] = valid_annotations\n \n    # Verifica se ci sono immagini senza annotazioni (usando il dizionario delle annotazioni)\n    new_annotations = []\n    for image in tqdm(data.get('images', []), desc=\"Processing Images\"):\n        if image['id'] not in image_annotations_dict:  # Se l'immagine non ha annotazioni\n            # Aggiungi la categoria \"background\"\n            new_annotation = {\n                'id': len(data['annotations']) + len(new_annotations),\n                'image_id': image['id'],\n                'category_id': 0,  # Categoria background con ID 0\n                'area': image['width'] * image['height'],\n                'bbox': [0.0, 0.0, image['width'], image['height']],  # Background con bbox che copre tutta l'immagine\n                'iscrowd': 0\n            }\n            new_annotations.append(new_annotation)\n \n    # Aggiungi le nuove annotazioni al JSON originale\n    data['annotations'].extend(new_annotations)\n \n    # Aggiorna le categorie nel JSON\n    data['categories'] = categories\n \n    # Scrivi il JSON modificato nel file di output\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.550338Z","iopub.status.idle":"2024-12-28T23:07:58.550688Z","shell.execute_reply.started":"2024-12-28T23:07:58.550523Z","shell.execute_reply":"2024-12-28T23:07:58.550541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_custom_coco_json(coco_json_pth, new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.551597Z","iopub.status.idle":"2024-12-28T23:07:58.551943Z","shell.execute_reply.started":"2024-12-28T23:07:58.551766Z","shell.execute_reply":"2024-12-28T23:07:58.551783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Category Check","metadata":{}},{"cell_type":"code","source":"def count_bounding_boxes(json_path):\n    \"\"\"\n    Conta il numero di bounding box per ogni categoria in un file COCO JSON.\n\n    Args:\n        json_path (str): Percorso del file JSON.\n\n    Returns:\n        list: Elenco di tuple con ID categoria, nome categoria e numero di bounding box.\n    \"\"\"\n    # Carica il file JSON\n    coco_data = load_json(json_path)\n\n    # Estrarre i dati principali\n    annotations = coco_data.get(\"annotations\", [])\n    categories = coco_data.get(\"categories\", [])\n\n    # Mappare id di categoria ai nomi delle categorie\n    category_id_to_name = {category[\"id\"]: category[\"name\"] for category in categories}\n\n    # Contare i bounding box per categoria\n    bbox_counts = defaultdict(int)\n    for annotation in annotations:\n        category_id = annotation[\"category_id\"]\n        bbox_counts[category_id] += 1\n\n    # Creare un elenco dei risultati\n    results = [\n        (cat_id, category_id_to_name.get(cat_id, \"Unknown\"), count)\n        for cat_id, count in bbox_counts.items()\n    ]\n    \n    # Ordinare i risultati in ordine decrescente per numero di bounding box\n    results.sort(key=lambda x: x[2], reverse=True)\n    \n    # Stampare i risultati\n    for cat_id, category_name, count in results:\n        print(f\"Categoria ID {cat_id} ('{category_name}'): {count} bounding box\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.552859Z","iopub.status.idle":"2024-12-28T23:07:58.553187Z","shell.execute_reply.started":"2024-12-28T23:07:58.553029Z","shell.execute_reply":"2024-12-28T23:07:58.553046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count_bounding_boxes(new_coco_json_pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.554746Z","iopub.status.idle":"2024-12-28T23:07:58.555045Z","shell.execute_reply.started":"2024-12-28T23:07:58.554894Z","shell.execute_reply":"2024-12-28T23:07:58.554909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Region Proposals Generation","metadata":{}},{"cell_type":"code","source":"# Funzione per elaborare una singola immagine\ndef process_single_image(image_data, img_fldr):\n    img_id = image_data['id']\n    img_name = image_data['file_name']\n    img_path = os.path.join(img_fldr, img_name)\n\n    if not os.path.exists(img_path):\n        raise ValueError(f\"Immagine non trovata nel percorso: {img_path}\")\n\n    # Carica l'immagine usando opencv (in modalità RGB)\n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n    original_height, original_width, _ = image.shape\n\n    # Ridimensiona l'immagine per velocizzare la Selective Search\n    resized_image = cv2.resize(image, (original_width // 2, original_height // 2), interpolation=cv2.INTER_AREA)\n\n    # Genera le region proposals sulla versione ridotta\n    processed_proposals = generate_and_process_proposals(resized_image, original_width // 2, original_height // 2)\n\n    # Riscalare le coordinate delle proposte alla dimensione originale\n    scaled_proposals = [[x * 2, y * 2, x_max * 2, y_max * 2] for x, y, x_max, y_max in processed_proposals]\n\n    image_data = {\n        \"image_id\": img_id,\n        \"file_name\": img_name,\n        \"original_size\": [original_width, original_height],\n        \"proposals\": []\n    }\n\n    for i, proposal in enumerate(scaled_proposals):\n        x_min, y_min, x_max, y_max = proposal\n        image_data[\"proposals\"].append({\n            \"proposal_id\": i,\n            \"coordinates\": [x_min, y_min, x_max, y_max]\n        })\n\n    return image_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.556269Z","iopub.status.idle":"2024-12-28T23:07:58.556618Z","shell.execute_reply.started":"2024-12-28T23:07:58.556413Z","shell.execute_reply":"2024-12-28T23:07:58.556464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funzione per generare le region proposals con Selective Search\ndef generate_and_process_proposals(image, img_width, img_height):\n    img_np = np.array(image, dtype=np.uint8)\n\n    # Esegui la selective search con parametri ottimizzati\n    '''\n    _, regions = selectivesearch.selective_search(img_np, scale=300, sigma=0.8, min_size=20)\n    '''\n    _, regions = selectivesearch.selective_search(img_np, scale=200, sigma=0.5, min_size=10)\n    if len(regions) == 0:\n        print(f\"Warning: Nessuna regione proposta generata per immagine con forma {img_np.shape}.\")\n\n    processed_proposals = []\n\n    # Pre-filtraggio delle regioni\n    for region in regions:\n        x, y, w, h = region['rect']\n        area = w * h\n        if w >= 10 and h >= 10 and 10 <= area <= 0.8 * (img_width * img_height):\n            x_max, y_max = x + w, y + h\n            processed_proposals.append([x, y, x_max, y_max])\n\n    return processed_proposals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.557612Z","iopub.status.idle":"2024-12-28T23:07:58.557927Z","shell.execute_reply.started":"2024-12-28T23:07:58.557770Z","shell.execute_reply":"2024-12-28T23:07:58.557787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funzione per gestire i batch\ndef batch(iterable, n=1):\n    it = iter(iterable)\n    while True:\n        chunk = list(islice(it, n))\n        if not chunk:\n            break\n        yield chunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.560384Z","iopub.status.idle":"2024-12-28T23:07:58.560726Z","shell.execute_reply.started":"2024-12-28T23:07:58.560575Z","shell.execute_reply":"2024-12-28T23:07:58.560592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_dataset_proposals(coco_json, img_fldr, output_dir, output_json):\n    os.makedirs(output_dir, exist_ok=True)\n    all_image_data = []\n\n    # Carica il file JSON di COCO\n    coco_data = load_json(coco_json)\n\n    # Prepara il mapping delle annotazioni per le immagini\n    image_annotations_map = {}\n    for annotation in coco_data['annotations']:\n        image_id = annotation['image_id']\n        if image_id not in image_annotations_map:\n            image_annotations_map[image_id] = []\n        image_annotations_map[image_id].append(annotation)\n\n    # Filtra le immagini che contengono annotazioni con category_id == 0 (sfondo)\n    images_with_annotations = [\n        image_data for image_data in coco_data['images']\n        if image_data['id'] in image_annotations_map and len(image_annotations_map[image_data['id']]) > 0\n        and not any(annot['category_id'] == 0 for annot in image_annotations_map[image_data['id']])  # Escludi sfondo\n    ]\n\n    # Parametri per parallelizzazione e batch processing\n    max_workers = os.cpu_count() - 1\n    batch_size = 500\n    total_batches = len(images_with_annotations) // batch_size + (len(images_with_annotations) % batch_size > 0)\n\n    # Processa le immagini in batch con tqdm per monitorare il progresso dei batch\n    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n        for image_batch in batch(images_with_annotations, batch_size):\n            with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n                results = list(executor.map(process_single_image, image_batch, [img_fldr] * len(image_batch)))\n            all_image_data.extend(results)\n            pbar.update(1)  # Aggiorna la barra di progresso per ogni batch completato\n\n    # Salva il risultato in formato JSON usando orjson\n    with open(output_json, 'wb') as json_file:\n        json_file.write(orjson.dumps(all_image_data, option=orjson.OPT_INDENT_2))\n\n    print(f\"Creato file JSON con le region proposals: {output_json}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.561941Z","iopub.status.idle":"2024-12-28T23:07:58.562272Z","shell.execute_reply.started":"2024-12-28T23:07:58.562113Z","shell.execute_reply":"2024-12-28T23:07:58.562130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_dataset_proposals(new_coco_json_pth, img_fldr, prop_fldr, out_proposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.563851Z","iopub.status.idle":"2024-12-28T23:07:58.564168Z","shell.execute_reply.started":"2024-12-28T23:07:58.564013Z","shell.execute_reply":"2024-12-28T23:07:58.564029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Nome del file zip da creare\nzip_file_name = \"proposals-dataset.zip\"\n\n# Elenco di file e cartelle da includere nello zip\nitems_to_zip = [\n    \"mod_COCO_annotations.json\",\n    \"proposals.json\",\n]\n\n# Funzione per aggiungere file e cartelle allo zip\ndef zip_folder(zipf, folder_path, base_folder=\"\"):\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, base_folder)\n            zipf.write(file_path, arcname)\n\n# Creazione dello zip\nwith zipfile.ZipFile(zip_file_name, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.exists(item):  # Verifica che il file o la cartella esista\n            if os.path.isdir(item):  # Se è una cartella, aggiungi tutto il contenuto\n                zip_folder(zipf, item, out_dataset_pth)\n            else:  # Se è un file, aggiungilo direttamente\n                zipf.write(item)\n        else:\n            print(f\"Elemento non trovato: {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T23:07:58.565515Z","iopub.status.idle":"2024-12-28T23:07:58.565849Z","shell.execute_reply.started":"2024-12-28T23:07:58.565691Z","shell.execute_reply":"2024-12-28T23:07:58.565709Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Positive Region Proposals with Undersampling","metadata":{}},{"cell_type":"code","source":"def visualize_image_with_bbox(image_path, bbox, title, ax):\n    \"\"\"\n    Visualizza un'immagine con sopra disegnato un bounding box usando Matplotlib in un subplot.\n    :param image_path: Percorso all'immagine.\n    :param bbox: Bounding box in formato [xmin, ymin, xmax, ymax].\n    :param title: Titolo da mostrare sopra l'immagine.\n    :param ax: Axes del subplot dove disegnare l'immagine.\n    \"\"\"\n    # Carica l'immagine\n    image = Image.open(image_path)\n    \n    # Estrai xmin, ymin, xmax, ymax\n    xmin, ymin, xmax, ymax = bbox\n    width = xmax - xmin\n    height = ymax - ymin\n\n    # Aggiungi l'immagine e il bounding box\n    ax.imshow(image)\n    rect = patches.Rectangle(\n        (xmin, ymin), width, height,\n        linewidth=2, edgecolor='red', facecolor='none'\n    )\n    ax.add_patch(rect)\n    \n    # Aggiungi il titolo\n    ax.set_title(title, fontsize=15)\n    ax.axis('off')  # Rimuovi gli assi\n\ndef calculate_bbox_areas_for_all_categories(json_file_path, images_dir):\n    \"\"\"\n    Calcola l'area minima, massima e media per ogni categoria e disegna i bounding box \n    corrispondenti sulle immagini per visualizzarle.\n\n    :param json_file_path: Percorso al file JSON contenente le annotazioni.\n    :param images_dir: Directory contenente le immagini originali.\n    \"\"\"\n    # Carica il file JSON\n    with open(json_file_path, 'r') as f:\n        data = json.load(f)\n    \n    # Crea dizionari per mappare ID\n    areas_per_category = defaultdict(list)\n    category_map = {category['id']: category['name'] for category in data['categories']}\n    images_map = {image['id']: image for image in data['images']}\n\n    # Raccogliere le aree\n    for annotation in data.get(\"annotations\", []):\n        category_id = annotation[\"category_id\"]\n        area = annotation[\"area\"]\n        areas_per_category[category_id].append((area, annotation))\n\n    # Itera per ogni categoria\n    for category_id, areas in areas_per_category.items():\n        if not areas:\n            continue\n\n        # Trova le annotazioni con area minima, massima e calcola l'area media\n        min_area_annotation = min(areas, key=lambda x: x[0])[1]\n        max_area_annotation = max(areas, key=lambda x: x[0])[1]\n        avg_area = sum(area for area, _ in areas) / len(areas)\n\n        # Stampa le informazioni sulle aree\n        print(f\"Categoria: {category_map[category_id]}\")\n        print(f\"  Area Minima: {min_area_annotation['area']}\")\n        print(f\"  Area Massima: {max_area_annotation['area']}\")\n        print(f\"  Area Media: {avg_area:.2f}\\n\")\n\n        # Crea un subplot per visualizzare le immagini con i bounding box\n        fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n\n        # Visualizza le immagini con bounding box per l'area minima e massima\n        for annotation, label, ax_idx in [(min_area_annotation, \"Minima\", 0), (max_area_annotation, \"Massima\", 1)]:\n            image_id = annotation['image_id']\n            image_info = images_map.get(image_id)\n            if not image_info:\n                print(f\"Immagine con ID {image_id} non trovata.\")\n                continue\n\n            # Percorso immagine\n            image_path = os.path.join(images_dir, image_info['file_name'])\n            if not os.path.exists(image_path):\n                print(f\"Immagine {image_path} non trovata.\")\n                continue\n            \n            # Visualizza immagine con bounding box\n            bbox = annotation['bbox']\n            title = f\"{category_map[category_id]} ({label})\"\n            visualize_image_with_bbox(image_path, bbox, title, ax[ax_idx])\n\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:41.537994Z","iopub.execute_input":"2024-12-29T02:22:41.538367Z","iopub.status.idle":"2024-12-29T02:22:41.550875Z","shell.execute_reply.started":"2024-12-29T02:22:41.538321Z","shell.execute_reply":"2024-12-29T02:22:41.549997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esempio di utilizzo\ncalculate_bbox_areas_for_all_categories(in_new_coco_json_pth, img_fldr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:44.715977Z","iopub.execute_input":"2024-12-29T02:22:44.716850Z","iopub.status.idle":"2024-12-29T02:22:58.003089Z","shell.execute_reply.started":"2024-12-29T02:22:44.716807Z","shell.execute_reply":"2024-12-29T02:22:58.001856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ignored_count = 0  # Contatore globale per le regioni ignorate\n\ndef get_iou(bb1, bb2):\n    global ignored_count  # Accedi alla variabile globale del contatore\n\n    try:\n        # Assicurati che le dimensioni siano corrette\n        assert bb1['x1'] < bb1['x2']\n        assert bb1['y1'] < bb1['y2']\n        assert bb2['x1'] < bb2['x2']\n        assert bb2['y1'] < bb2['y2']\n    except AssertionError:\n        # Se si verifica un errore, incrementa il contatore delle regioni ignorate\n        ignored_count += 1\n        return 0.0  # Restituisci 0.0 per l'IoU in caso di errore (nessuna sovrapposizione)\n\n    # Calcola le dimensioni dell'area comune tra i due box\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    # Se non c'è sovrapposizione, restituisci 0 come area di intersezione\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calcola l'area di intersezione\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    # Calcola le aree individuali dei due bounding box\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n    \n    # Calcola l'area dell'unione\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    # Verifica che l'IoU sia nel range corretto\n    assert iou >= 0.0\n    assert iou <= 1.0\n\n    return iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.005203Z","iopub.execute_input":"2024-12-29T02:22:58.005664Z","iopub.status.idle":"2024-12-29T02:22:58.016873Z","shell.execute_reply.started":"2024-12-29T02:22:58.005609Z","shell.execute_reply":"2024-12-29T02:22:58.015848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_adaptive_threshold(bbox):\n    \"\"\"Calcola un threshold IoU adattivo in base all'area del bounding box e alla categoria.\"\"\"\n    # bbox è un tensore o una lista con [x1, y1, x2, y2]\n    width = bbox[2] - bbox[0]  # Calcolo larghezza\n    height = bbox[3] - bbox[1]  # Calcolo altezza\n    \n    # Calcolo dell'area\n    area = width * height\n\n    counter = 0\n    \n    # Aree medie per categoria (escludendo background)\n    area_media = {\n        2: 651.14,   # Truck\n        11: 6075.12  # Aircraft\n    }\n    \n    # Recupero delle aree minima e massima per normalizzazione\n    min_area = min(area_media.values())\n    max_area = max(area_media.values())\n    \n    # Normalizziamo l'area rispetto all'intervallo di area definito\n    normalized_area = (area - min_area) / (max_area - min_area)  # Valore tra 0 e 1\n    \n    # Clipping per evitare valori fuori intervallo\n    normalized_area = max(0, min(1, normalized_area))\n    \n    # Mappiamo il valore normalizzato su un intervallo di soglie (0.3 a 0.7)\n    threshold = 0.3 + 0.4 * normalized_area  # Da 0.3 a 0.7\n    \n    return threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.018027Z","iopub.execute_input":"2024-12-29T02:22:58.018295Z","iopub.status.idle":"2024-12-29T02:22:58.033329Z","shell.execute_reply.started":"2024-12-29T02:22:58.018270Z","shell.execute_reply":"2024-12-29T02:22:58.032583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assign_and_save_regions(region_json_path, bbox_json_path, image_dir, output_dir, output_json_path):\n    \"\"\"Associa le regioni proposte ai bounding boxes, salva le regioni positive come immagini e crea un nuovo JSON con informazioni attivate.\"\"\"\n    \n    # Carica i file JSON\n    with open(region_json_path, 'r') as f:\n        regions = json.load(f)\n\n    with open(bbox_json_path, 'r') as f:\n        bboxes = json.load(f)\n    \n    # Crea un dizionario per cercare annotations per image_id\n    annotations_by_image = {}\n    for annot in bboxes[\"annotations\"]:\n        img_id = annot[\"image_id\"]\n        if img_id not in annotations_by_image:\n            annotations_by_image[img_id] = []\n        bbox = annot[\"bbox\"]\n        annotations_by_image[img_id].append((torch.tensor(bbox, dtype=torch.float32), annot[\"category_id\"]))\n    \n    # Crea un dizionario per mappare category_id ai nomi delle categorie\n    category_mapping = {cat_id: name for cat_id, name in enumerate(bboxes[\"categories\"])}\n    \n    # Crea la directory di output se non esiste\n    os.makedirs(output_dir, exist_ok=True)\n\n    counter = 0  # Contatore delle immagini salvate\n    \n    active_region_data = []  # Lista per i dati delle regioni attive\n\n    # Avvolgi il ciclo principale per ogni immagine con tqdm\n    for image in tqdm(regions, desc=\"Elaborazione immagini\", total=len(regions)):\n        image_id = image[\"image_id\"]\n        file_name = image[\"file_name\"]\n        proposals = image[\"proposals\"]\n        \n        # Ottieni bounding boxes ground-truth e categorie per l'immagine corrente\n        gt_data = annotations_by_image.get(image_id, [])\n        if not gt_data:\n            # Se non ci sono bounding boxes ground-truth, salta l'immagine\n            continue\n        \n        gt_bboxes = [item[0] for item in gt_data]  # Bounding box ground truth\n        gt_categories = [item[1] for item in gt_data]  # Categorie ground truth\n        \n        # Trasforma proposals in una lista di dizionari compatibili con get_iou\n        proposal_coords = [{'x1': p[\"coordinates\"][0], 'y1': p[\"coordinates\"][1], \n                            'x2': p[\"coordinates\"][2], 'y2': p[\"coordinates\"][3]} \n                           for p in proposals]\n        \n        # Esegui undersampling per le categorie 6 prima di procedere\n        filtered_proposals = []\n        filtered_gt_bboxes = []\n        filtered_gt_categories = []\n        \n        for i, (prop, gt_bbox, gt_category) in enumerate(zip(proposal_coords, gt_bboxes, gt_categories)):\n            filtered_proposals.append(prop)\n            filtered_gt_bboxes.append(gt_bbox)\n            filtered_gt_categories.append(gt_category)\n\n        # Trasformiamo di nuovo in coordinate per IoU\n        proposal_coords = filtered_proposals\n        gt_bboxes = filtered_gt_bboxes\n        gt_categories = filtered_gt_categories\n\n        # Calcola la matrice IoU usando la funzione get_iou\n        iou_matrix = []\n        for proposal in proposal_coords:\n            iou_row = []\n            for gt_bbox in gt_bboxes:\n                gt_dict = {'x1': gt_bbox[0].item(), 'y1': gt_bbox[1].item(), \n                           'x2': gt_bbox[2].item(), 'y2': gt_bbox[3].item()}\n                iou = get_iou(proposal, gt_dict)\n                iou_row.append(iou)\n            iou_matrix.append(iou_row)\n\n        # Verifica se la matrice IoU è vuota\n        if not iou_matrix:\n            continue\n        \n        iou_matrix = torch.tensor(iou_matrix)\n\n        # Identifica le regioni positive utilizzando la soglia adattiva\n        positive_indices = []\n        zero_iou_indices = []\n        for row_idx, row in enumerate(iou_matrix):\n            for col_idx, iou in enumerate(row):\n                adaptive_threshold = get_adaptive_threshold(gt_bboxes[col_idx])\n                if iou >= adaptive_threshold:\n                    positive_indices.append((row_idx, col_idx))\n                elif iou == 0:\n                    # Accumula le immagini con IoU = 0 per selezione successiva\n                    zero_iou_indices.append((row_idx, -1))  # Assegna categoria 0 con -1\n        \n        # Seleziona casualmente il 20% delle immagini con IoU = 0\n        selected_zero_iou = random.sample(zero_iou_indices, int(0.005 * len(zero_iou_indices)))\n        \n        # Aggiungi solo le selezionate al set positivo\n        positive_indices.extend(selected_zero_iou)\n\n        # Carica l'immagine originale\n        image_path = os.path.join(image_dir, file_name)\n        original_image = cv2.imread(image_path)\n        if original_image is None:\n            print(f\"Immagine non trovata: {image_path}\")\n            continue\n\n        # Avvolgi il ciclo per ogni proposta positiva\n        for row_idx, col_idx in positive_indices:\n            if col_idx == -1:\n                category_id = 0  \n            else:\n                category_id = gt_categories[col_idx]\n            \n            # Aggiungi la condizione per includere solo il 10% delle categorie 0 e 6\n            if category_id in [0, 6]:\n                if random.random() > 0.9:  # Mantieni il 10% delle categorie 0 e 6\n                    # Calcola le coordinate del bounding box\n                    x_min, y_min, x_max, y_max = proposal_coords[row_idx].values()\n                    \n                    cropped = original_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n                    \n                    # Ridimensiona a 224x224\n                    resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n                    \n                    # Salva l'immagine\n                    output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n                    cv2.imwrite(output_path, resized)\n                    \n                    # Aggiungi la proposta attivata al nuovo JSON in formato COCO\n                    active_region_data.append({\n                        \"image_id\": image_id,\n                        \"file_name\": file_name,\n                        \"category_id\": category_id,\n                        \"proposal_id\": row_idx,\n                        \"region_bbox\": [x_min, y_min, x_max, y_max],  # Usa xmin, ymin, xmax, ymax\n                        \"original_bbox\": gt_bboxes[col_idx].tolist() if col_idx != -1 else [],  # Aggiunge il bbox originale\n                        \"saved_path\": output_path\n                    })\n                    \n                    counter += 1\n            else:\n                # Se la categoria non è 0 e 6, includi sempre la proposta\n                # Calcola le coordinate del bounding box\n                x_min, y_min, x_max, y_max = proposal_coords[row_idx].values()\n                \n                cropped = original_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n                \n                # Ridimensiona a 224x224\n                resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n                \n                # Salva l'immagine\n                output_path = os.path.join(output_dir, f\"image_{counter:06d}.jpg\")\n                cv2.imwrite(output_path, resized)\n                \n                # Aggiungi la proposta attivata al nuovo JSON in formato COCO\n                active_region_data.append({\n                    \"image_id\": image_id,\n                    \"file_name\": file_name,\n                    \"category_id\": category_id,\n                    \"proposal_id\": row_idx,\n                    \"region_bbox\": [x_min, y_min, x_max, y_max],  # Usa xmin, ymin, xmax, ymax\n                    \"original_bbox\": gt_bboxes[col_idx].tolist(),  # Aggiunge il bbox originale\n                    \"saved_path\": output_path\n                })\n                \n                counter += 1\n\n    # Salva il nuovo JSON con le regioni attive\n    with open(output_json_path, 'w') as json_file:\n        json.dump(active_region_data, json_file, indent=2)\n\n    print(counter)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.035945Z","iopub.execute_input":"2024-12-29T02:22:58.036319Z","iopub.status.idle":"2024-12-29T02:22:58.059694Z","shell.execute_reply.started":"2024-12-29T02:22:58.036290Z","shell.execute_reply":"2024-12-29T02:22:58.058891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Esegui l'assegnazione e ottieni i valori IoU\n\nassign_and_save_regions(proposals_json, in_new_coco_json_pth, img_fldr, prop_fldr, actproposals_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:22:58.060673Z","iopub.execute_input":"2024-12-29T02:22:58.060948Z","iopub.status.idle":"2024-12-29T02:52:13.802699Z","shell.execute_reply.started":"2024-12-29T02:22:58.060921Z","shell.execute_reply":"2024-12-29T02:52:13.801875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_regions(file_path):\n    \"\"\"\n    Analizza un file JSON per ottenere il numero di regioni e le occorrenze dei category_id.\n\n    :param file_path: Percorso al file JSON contenente le annotazioni.\n    :return: Tupla contenente il numero di regioni e un dizionario con le occorrenze dei category_id.\n    \"\"\"\n    # Carica il file JSON\n    data = load_json(file_path)\n\n    # Conta il numero di regioni\n    num_regioni = len(data)\n    \n    # Ottieni le occorrenze dei category_id\n    category_ids = [entry['category_id'] for entry in data]\n    category_counts = Counter(category_ids)\n\n    # Ordina le occorrenze per ID di categoria\n    sorted_category_counts = dict(sorted(category_counts.items()))\n\n    return num_regioni, sorted_category_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:52:13.810154Z","iopub.execute_input":"2024-12-29T02:52:13.810458Z","iopub.status.idle":"2024-12-29T02:52:13.826351Z","shell.execute_reply.started":"2024-12-29T02:52:13.810409Z","shell.execute_reply":"2024-12-29T02:52:13.825390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_regioni, category_counts = analyze_regions(actproposals_json)\nprint(f\"Numero di regioni: {num_regioni}\")\nprint(\"Occorrenze dei category_id:\", category_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:52:13.827942Z","iopub.execute_input":"2024-12-29T02:52:13.828212Z","iopub.status.idle":"2024-12-29T02:52:13.977345Z","shell.execute_reply.started":"2024-12-29T02:52:13.828169Z","shell.execute_reply":"2024-12-29T02:52:13.976493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Nome del file zip da creare\nzip_file_name = \"activeregion-xview-dataset.zip\"\n\n# Elenco di file e cartelle da includere nello zip\nitems_to_zip = [\n    \"active_regions.json\",\n    \"proposals\",\n]\n\n# Funzione per aggiungere file e cartelle allo zip\ndef zip_folder(zipf, folder_path, base_folder=\"\"):\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, base_folder)\n            zipf.write(file_path, arcname)\n\n# Creazione dello zip\nwith zipfile.ZipFile(zip_file_name, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.exists(item):  # Verifica che il file o la cartella esista\n            if os.path.isdir(item):  # Se è una cartella, aggiungi tutto il contenuto\n                zip_folder(zipf, item, out_dataset_pth)\n            else:  # Se è un file, aggiungilo direttamente\n                zipf.write(item)\n        else:\n            print(f\"Elemento non trovato: {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T02:52:13.978349Z","iopub.execute_input":"2024-12-29T02:52:13.978639Z","iopub.status.idle":"2024-12-29T02:52:32.004881Z","shell.execute_reply.started":"2024-12-29T02:52:13.978613Z","shell.execute_reply":"2024-12-29T02:52:32.004053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting","metadata":{}},{"cell_type":"code","source":"# Carica il dataset dal JSON\n\ndata = load_json(in_actproposals_json)\n\n# Converti in DataFrame per una gestione più comoda\ndf = pd.DataFrame(data)\n\n# Estrai il nome del file dal campo 'saved_path'\ndf[\"file_name\"] = df[\"saved_path\"].apply(lambda x: os.path.basename(x))\n\n# Aggiungi il percorso base al campo 'saved_path'\ndf[\"saved_path\"] = df[\"file_name\"].apply(lambda x: str(act_reg_folder / x))\n\n# Estrai i dati e le etichette\nX = df.index  # Indici delle righe\ny = df[\"category_id\"]  # Etichetta per stratificazione\n\n# Step 1: Train + Val/Test\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Step 2: Val/Test split\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n)\n\n# Creazione dei dataset finali\ntrain_data = df.loc[X_train]\nval_data = df.loc[X_val]\ntest_data = df.loc[X_test]\n\n# Salva i dataset in nuovi file JSON\ntrain_data.to_json(\"train.json\", orient=\"records\", lines=False)\nval_data.to_json(\"val.json\", orient=\"records\", lines=False)\ntest_data.to_json(\"test.json\", orient=\"records\", lines=False)\n\nprint(\"Splitting completato. File salvati: train.json, val.json, test.json.\")\n\n# Percentuale dei dati per ciascun set\ntotal_data = len(df)\nprint(\"\\nPercentuale dei dati per ciascun set:\")\nprint(f\"Train: {len(train_data) / total_data:.2%}\")\nprint(f\"Validation: {len(val_data) / total_data:.2%}\")\nprint(f\"Test: {len(test_data) / total_data:.2%}\")\n\n# Distribuzione delle classi per ciascun set\nprint(\"\\nDistribuzione delle classi:\")\ntrain_class_dist = train_data[\"category_id\"].value_counts(normalize=True) * 100\nval_class_dist = val_data[\"category_id\"].value_counts(normalize=True) * 100\ntest_class_dist = test_data[\"category_id\"].value_counts(normalize=True) * 100\n\nprint(\"Train set:\")\nprint(train_class_dist.sort_index())\n\nprint(\"\\nValidation set:\")\nprint(val_class_dist.sort_index())\n\nprint(\"\\nTest set:\")\nprint(test_class_dist.sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:15:45.653113Z","iopub.execute_input":"2024-12-29T18:15:45.654045Z","iopub.status.idle":"2024-12-29T18:15:46.446675Z","shell.execute_reply.started":"2024-12-29T18:15:45.653985Z","shell.execute_reply":"2024-12-29T18:15:46.445717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = load_json(\"train.json\")\n\n# Visualizza 3 immagini a caso dal train set\nfig, axes = plt.subplots(1, 3, figsize=(20, 10))\nfor i, ax in enumerate(axes):\n    sample_entry = random.choice(train_data)\n    image_path = sample_entry[\"saved_path\"]\n    title = f\"Category ID: {sample_entry['category_id']}\"\n    visualize_image_with_bbox(image_path, [0,0,0,0], title, ax)\n\nplt.tight_layout()\nplt.show()\n\nval_data = load_json(\"val.json\")\n\n# Visualizza 3 immagini a caso dal train set\nfig, axes = plt.subplots(1, 3, figsize=(20, 10))\nfor i, ax in enumerate(axes):\n    sample_entry = random.choice(val_data)\n    image_path = sample_entry[\"saved_path\"]\n    title = f\"Category ID: {sample_entry['category_id']}\"\n    visualize_image_with_bbox(image_path, [0,0,0,0], title, ax)\n\nplt.tight_layout()\nplt.show()\n\ntest_data = load_json(\"test.json\")\n\n# Visualizza 3 immagini a caso dal train set\nfig, axes = plt.subplots(1, 3, figsize=(20, 10))\nfor i, ax in enumerate(axes):\n    sample_entry = random.choice(test_data)\n    image_path = sample_entry[\"saved_path\"]\n    title = f\"Category ID: {sample_entry['category_id']}\"\n    visualize_image_with_bbox(image_path, [0,0,0,0], title, ax)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:09:14.036666Z","iopub.execute_input":"2024-12-29T03:09:14.037010Z","iopub.status.idle":"2024-12-29T03:09:16.612852Z","shell.execute_reply.started":"2024-12-29T03:09:14.036979Z","shell.execute_reply":"2024-12-29T03:09:16.611983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        \"\"\"\n        Inizializza il dataset.\n\n        :param json_file: Percorso del file JSON contenente le informazioni sulle regioni.\n        :param transform: Trasformazioni da applicare alle immagini. Se non fornito, vengono usate trasformazioni di default.\n        \"\"\"\n        # Carica il file JSON\n        self.data = load_json(json_file)\n        \n        # Trasformazioni di default se non vengono fornite\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),         # Ridimensiona l'immagine a 224x224\n            transforms.ToTensor(),                  # Converte l'immagine in un tensore\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizzazione per i modelli pre-addestrati\n        ])  \n\n    def __len__(self):\n        \"\"\"Restituisce il numero totale di immagini/proposte nel dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Restituisce un esempio (immagine e etichetta) per l'addestramento.\"\"\"\n        # Carica l'esempio dal file JSON\n        sample = self.data[idx]\n        \n        # Carica l'immagine\n        image = Image.open(sample[\"saved_path\"]).convert(\"RGB\")\n        \n        # Etichetta della categoria\n        label = sample[\"category_id\"]  # Categoria della proposta\n\n        # Applica le trasformazioni\n        image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:15:50.274388Z","iopub.execute_input":"2024-12-29T18:15:50.274755Z","iopub.status.idle":"2024-12-29T18:15:50.282472Z","shell.execute_reply.started":"2024-12-29T18:15:50.274724Z","shell.execute_reply":"2024-12-29T18:15:50.281302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ds = CustomDataset(test_path)\ntrain_ds = CustomDataset(train_path)\nval_ds = CustomDataset(val_path)\n\nTrainLoader = DataLoader(train_ds, batch_size=32, shuffle=True)\nValLoader = DataLoader(val_ds, batch_size=32, shuffle=False)\nTestLoader = DataLoader(test_ds, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:15:54.436480Z","iopub.execute_input":"2024-12-29T18:15:54.436855Z","iopub.status.idle":"2024-12-29T18:15:54.553912Z","shell.execute_reply.started":"2024-12-29T18:15:54.436823Z","shell.execute_reply":"2024-12-29T18:15:54.553207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(AlexNet, self).__init__()\n        self._output_num = num_classes\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n     \n        self.drop8 = nn.Dropout()\n        self.fn8 = nn.Linear(256 * 6 * 6, 4096)\n        self.active8 = nn.ReLU(inplace=True)\n        \n        self.drop9 = nn.Dropout()\n        self.fn9 = nn.Linear(4096, 4096)\n        self.active9 = nn.ReLU(inplace=True)\n        \n        self.fn10 = nn.Linear(4096, self._output_num)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.drop8(x)\n        x = self.fn8(x)\n        x = self.active8(x)\n\n        x = self.drop9(x)\n        x = self.fn9(x)\n        \n        feature = self.active9(x)  \n        final = self.fn10(feature)\n\n        return feature, final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T10:04:33.478950Z","iopub.execute_input":"2024-12-29T10:04:33.479307Z","iopub.status.idle":"2024-12-29T10:04:33.492348Z","shell.execute_reply.started":"2024-12-29T10:04:33.479276Z","shell.execute_reply":"2024-12-29T10:04:33.491384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 12 #11 classi + sfondo\nnet = AlexNet(num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T10:04:35.581384Z","iopub.execute_input":"2024-12-29T10:04:35.582026Z","iopub.status.idle":"2024-12-29T10:04:36.063292Z","shell.execute_reply.started":"2024-12-29T10:04:35.581974Z","shell.execute_reply":"2024-12-29T10:04:36.062323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(net)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:34:37.601695Z","iopub.execute_input":"2024-12-29T03:34:37.602591Z","iopub.status.idle":"2024-12-29T03:34:37.608048Z","shell.execute_reply.started":"2024-12-29T03:34:37.602528Z","shell.execute_reply":"2024-12-29T03:34:37.606853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_loss(train_losses, val_losses):\n    \"\"\"\n    Funzione per fare il plot della funzione di loss durante il training e la validazione.\n\n    :param train_losses: Lista delle perdite durante il training.\n    :param val_losses: Lista delle perdite durante la validazione.\n    \"\"\"\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", linestyle='-', marker='o')\n    plt.plot(epochs, val_losses, label=\"Validation Loss\", color=\"red\", linestyle='-', marker='x')\n    \n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T19:39:23.382361Z","iopub.execute_input":"2024-12-29T19:39:23.382720Z","iopub.status.idle":"2024-12-29T19:39:23.388496Z","shell.execute_reply.started":"2024-12-29T19:39:23.382688Z","shell.execute_reply":"2024-12-29T19:39:23.387748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(net, train_loader, val_loader, criterion, optimizer, device, epochs, path_min_loss, num_classes, clip_value=1.0):\n    min_val_loss = float('inf')\n    \n    # Liste per registrare le perdite durante il training e la validazione\n    train_losses = []\n    val_losses = []\n    \n    # Dizionario per salvare le feature\n    train_features = {}  # Chiave: epoch, Valore: lista di feature\n\n    # Aggiungi un learning rate scheduler per ridurre il learning rate quando la loss di validazione non migliora\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n\n    for epoch in range(epochs):\n        net.train()  # Modalità training\n        train_loss = 0.0\n        correct_train = 0\n        total_train = 0\n        epoch_features = []  # Per registrare le feature di ogni batch\n        class_probs = torch.zeros((len(train_loader.dataset), num_classes)).to(device)  # Probabilità per classe\n\n        # Barra di avanzamento per il training\n        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", leave=False)\n\n        for i, (images, labels) in enumerate(train_progress):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            features, outputs = net(images)\n\n            # Salva le feature per il batch corrente\n            epoch_features.append(features.detach().cpu().numpy())\n\n            # Calcolo della loss pesata\n            loss = criterion(outputs, labels)\n\n            # Predizione della classe con la probabilità massima\n            _, predicted = outputs.max(1)\n\n            # Calcola le probabilità per ciascuna classe (softmax)\n            probs = torch.softmax(outputs, dim=1)\n            class_probs[i * images.size(0):(i + 1) * images.size(0)] = probs\n\n            # Monitoraggio dei gradienti\n            optimizer.zero_grad()\n            loss.backward()\n\n            # Clipping dei gradienti\n            utils.clip_grad_norm_(net.parameters(), clip_value)\n\n            optimizer.step()\n\n            # Statistiche\n            train_loss += loss.item() * images.size(0)\n            total_train += labels.size(0)\n            correct_train += predicted.eq(labels).sum().item()\n\n            # Aggiorna la barra di avanzamento con la loss corrente\n            train_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_train / total_train)\n\n        # Salva le feature per l'epoca corrente\n        train_features[epoch] = epoch_features\n\n        avg_train_loss = train_loss / len(train_loader.dataset)\n        train_accuracy = 100. * correct_train / total_train\n        train_losses.append(avg_train_loss)\n\n        # Calcola la probabilità media per ogni classe\n        avg_class_probs = class_probs.mean(dim=0)\n        print(f\"Probabilità media per classe in epoca {epoch+1}: {avg_class_probs}\")\n\n        # Validazione\n        net.eval()  # Modalità validazione\n        val_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        # Barra di avanzamento per la validazione\n        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\", leave=False)\n\n        with torch.no_grad():\n            for images, labels in val_progress:\n                images, labels = images.to(device), labels.to(device)\n\n                # Forward pass\n                _, outputs = net(images)\n                \n                # Predizione della classe con la probabilità massima\n                _, predicted = outputs.max(1)\n\n                # Calcolo della loss\n                loss = criterion(outputs, labels)\n\n                # Statistiche\n                val_loss += loss.item() * images.size(0)\n                total_val += labels.size(0)\n                correct_val += predicted.eq(labels).sum().item()\n\n                # Aggiorna la barra di avanzamento con la loss e accuracy\n                val_progress.set_postfix(loss=loss.item(), accuracy=100. * correct_val / total_val)\n\n        avg_val_loss = val_loss / len(val_loader.dataset)\n        val_accuracy = 100. * correct_val / total_val\n        val_losses.append(avg_val_loss)\n\n        # Salva il modello con la loss di validazione più bassa\n        if avg_val_loss < min_val_loss:\n            print(f\"Salvataggio del miglior modello: Val Loss migliorata da {min_val_loss:.4f} a {avg_val_loss:.4f}\")\n            min_val_loss = avg_val_loss\n            torch.save(net.state_dict(), path_min_loss)\n\n        # Aggiorna il learning rate in base alla loss di validazione\n        scheduler.step(avg_val_loss)\n\n        # Stampa statistiche per epoca\n        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n    print(\"Training completato!\")\n\n    # Chiamata alla funzione per tracciare il grafico\n    plot_loss(train_losses, val_losses)\n\n    return train_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:16:07.024988Z","iopub.execute_input":"2024-12-29T18:16:07.025307Z","iopub.status.idle":"2024-12-29T18:16:07.039041Z","shell.execute_reply.started":"2024-12-29T18:16:07.025281Z","shell.execute_reply":"2024-12-29T18:16:07.038159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(net, test_loader, criterion, device, path_min_loss, json_path, original_img_path, reference_json_path):\n    \"\"\"\n    Funzione per testare il modello e visualizzare i risultati su immagini originali.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    :param json_path: Percorso al file JSON contenente i dettagli delle immagini del test set.\n    :param original_img_path: Percorso al folder delle immagini originali.\n    :param reference_json_path: Percorso al file JSON contenente le informazioni delle immagini originali.\n    \"\"\"\n    # Carica il miglior modello salvato (con il parametro weights_only=True per evitare il warning)\n    net.load_state_dict(torch.load(path_min_loss, map_location=device))\n    net.eval()\n\n    test_loss = 0.0\n    correct_test = 0\n    total_test = 0\n\n    all_labels = []\n    all_predictions = []\n\n    # Carica i file JSON\n    with open(json_path, 'r') as f:\n        test_data = json.load(f)\n\n    with open(reference_json_path, 'r') as f:\n        reference_data = json.load(f)\n\n    # Crea un dizionario per una ricerca rapida delle immagini originali\n    id_to_filename = {img['id']: img['file_name'] for img in reference_data['images']}\n\n    # Crea un dizionario per raggruppare i bounding box per image_id\n    image_id_to_bboxes = {}\n    for image_info in test_data:\n        image_id = image_info['image_id']\n        region_bbox = image_info['region_bbox']\n        if image_id not in image_id_to_bboxes:\n            image_id_to_bboxes[image_id] = []\n        image_id_to_bboxes[image_id].append(region_bbox)\n\n    with torch.no_grad():\n        for idx, (images, labels) in enumerate(test_loader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            test_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total_test += labels.size(0)\n            correct_test += predicted.eq(labels).sum().item()\n\n            # Salva tutte le etichette e predizioni\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n            # Mostra alcuni esempi\n            if idx < 5:  # Mostra i primi 5 batch\n                for i in range(min(len(images), 3)):  # Mostra fino a 3 immagini per batch\n                    image_info = test_data[idx * len(images) + i]  # Recupera info immagine dal JSON\n                    image_id = image_info['image_id']\n\n                    # Trova il file_name usando l'image_id\n                    file_name = id_to_filename.get(image_id)\n                    if not file_name:\n                        print(f\"Immagine con ID {image_id} non trovata nel reference JSON.\")\n                        continue\n\n                    # Percorso dell'immagine originale\n                    img_path = os.path.join(original_img_path, file_name)\n\n                    # Carica l'immagine originale\n                    img = cv2.imread(img_path)\n                    if img is None:\n                        print(f\"Immagine non trovata: {img_path}\")\n                        continue\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n                    # Disegna i bounding box per le etichette reali (blu)\n                    bboxes_real = image_id_to_bboxes.get(image_id, [])\n                    for bbox in bboxes_real:\n                        xmin, ymin, xmax, ymax = bbox\n                        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)  # Blu per reale\n\n                    # Mostra l'immagine con i bounding box (senza le etichette scritte nell'immagine)\n                    plt.figure(figsize=(6, 6))\n                    plt.imshow(img)\n                    plt.axis('off')\n                    plt.show()\n\n                    # Ora stampa le etichette predette e reali sotto l'immagine\n                    print(f\"Predizioni vs Realtà per l'immagine {file_name}:\")\n                    for j, bbox in enumerate(bboxes_real):\n                        pred_label = predicted[i].item()\n                        # Stampa i valori predetti e reali\n                        print(f\"  Bounding Box {j + 1}: Predetto: {pred_label}, Reale: {labels[i].item()}\")\n\n    avg_test_loss = test_loss / len(test_loader.dataset)\n    test_accuracy = 100. * correct_test / total_test\n\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n\n    # Calcola e visualizza la matrice di confusione\n    cm = confusion_matrix(all_labels, all_predictions)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizza per riga\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:16:09.743225Z","iopub.execute_input":"2024-12-29T18:16:09.743535Z","iopub.status.idle":"2024-12-29T18:16:09.757098Z","shell.execute_reply.started":"2024-12-29T18:16:09.743507Z","shell.execute_reply":"2024-12-29T18:16:09.756312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.001\nepochs = 3\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\nnet = AlexNet(num_classes)\ndevice = torch.device(\"cuda\")\nnet = net.to(device)\n\ncriterion = criterion.to(device)\n\npath_min_loss = '/kaggle/working/AlexNet.pth'\n\ntrain_features = train_model(\n    net=net,\n    train_loader=TrainLoader,\n    val_loader=ValLoader,\n    criterion=criterion,\n    optimizer=optimizer,\n    device=device,\n    epochs=epochs,\n    path_min_loss=path_min_loss,\n    num_classes = num_classes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:37:19.459326Z","iopub.execute_input":"2024-12-29T03:37:19.459749Z","iopub.status.idle":"2024-12-29T03:43:23.969325Z","shell.execute_reply.started":"2024-12-29T03:37:19.459711Z","shell.execute_reply":"2024-12-29T03:43:23.968592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\n    net=net,\n    test_loader=TestLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=path_min_loss,\n    json_path = test_path,\n    original_img_path = img_fldr,\n    reference_json_path = in_new_coco_json_pth\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:46:41.540509Z","iopub.execute_input":"2024-12-29T03:46:41.540960Z","iopub.status.idle":"2024-12-29T03:47:08.624853Z","shell.execute_reply.started":"2024-12-29T03:46:41.540923Z","shell.execute_reply":"2024-12-29T03:47:08.623508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Numero di classi\nnum_classes = 12\n\n# Modello pre-addestrato ResNet50\nresnet50 = models.resnet50(pretrained=False)\n\n# Modifica l'ultimo layer completamente connesso per il numero di classi\nin_features = resnet50.fc.in_features  # Estrai le caratteristiche in input dell'ultimo layer\nresnet50.fc = nn.Linear(in_features, num_classes)  # Nuovo layer di classificazione\n\n# Passa il modello alla GPU (se disponibile)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresnet50 = resnet50.to(device)\n\n# Modifica il forward per restituire sia le feature che le predizioni finali\nclass ResNet50FeatureExtractor(nn.Module):\n    def __init__(self, model):\n        super(ResNet50FeatureExtractor, self).__init__()\n        self.resnet = model\n        # Rimuove l'ultimo layer\n        self.features = nn.Sequential(*list(self.resnet.children())[:-1])  # Rimuove il Fully Connected finale\n\n    def forward(self, x):\n        # Ottieni le feature intermedie e poi la predizione finale\n        features = self.features(x)\n        features = features.view(features.size(0), -1)  # Flattening per la classificazione\n        outputs = self.resnet.fc(features)\n        return features, outputs\n\n# Crea il nuovo modello con estrazione delle feature\nmodel = ResNet50FeatureExtractor(resnet50)\nmodel = model.to(device)\n\n# Funzione di perdita (loss) e ottimizzatore\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n\n# Numero di epoche\nepochs = 15\n\n# Funzione di training\ntrain_features = train_model(\n    net=model,\n    train_loader=TrainLoader,\n    val_loader=ValLoader,\n    criterion=criterion,\n    optimizer=optimizer,\n    device=device,\n    epochs=epochs,\n    path_min_loss=path_min_loss,\n    num_classes=num_classes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T20:40:30.683613Z","iopub.execute_input":"2024-12-29T20:40:30.683955Z","iopub.status.idle":"2024-12-29T22:01:30.627457Z","shell.execute_reply.started":"2024-12-29T20:40:30.683924Z","shell.execute_reply":"2024-12-29T22:01:30.626132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save(train_features_path, train_features)  # Salva in formato .npy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T22:01:30.665139Z","iopub.execute_input":"2024-12-29T22:01:30.665383Z","iopub.status.idle":"2024-12-29T22:01:37.093935Z","shell.execute_reply.started":"2024-12-29T22:01:30.665360Z","shell.execute_reply":"2024-12-29T22:01:37.092662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\n    net=model,\n    test_loader=TestLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=path_min_loss,\n    json_path = test_path,\n    original_img_path = img_fldr,\n    reference_json_path = in_new_coco_json_pth\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T22:01:37.096573Z","iopub.execute_input":"2024-12-29T22:01:37.096974Z","iopub.status.idle":"2024-12-29T22:02:03.227881Z","shell.execute_reply.started":"2024-12-29T22:01:37.096928Z","shell.execute_reply":"2024-12-29T22:02:03.226924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Box Regressor","metadata":{}},{"cell_type":"code","source":"def test_model_regressor(net, test_loader, criterion, device, path_min_loss, pred_json_path):\n    \"\"\"\n    Funzione per testare il modello, sostituire il category_id con quello predetto e visualizzare i risultati su immagini originali.\n\n    :param net: Modello da testare.\n    :param test_loader: DataLoader per il test set.\n    :param criterion: Funzione di loss.\n    :param device: Dispositivo (CPU o GPU).\n    :param path_min_loss: Percorso del modello salvato.\n    :param json_path: Percorso al file JSON contenente i dettagli delle immagini del test set.\n    :param pred_json_path: Percorso al file JSON da modificare con i category_id predetti.\n    \"\"\"\n    # Carica il miglior modello salvato\n    net.load_state_dict(torch.load(path_min_loss, map_location=device))\n    net.eval()\n\n    all_labels = []\n    all_predictions = []\n\n    # Carica il JSON con le informazioni delle immagini da modificare\n    with open(pred_json_path, 'r') as f:\n        pred_data = json.load(f)\n\n    with torch.no_grad():\n        for idx, (images, labels) in enumerate(test_loader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            _, outputs = net(images)\n\n            # Calcolo della loss\n            loss = criterion(outputs, labels)\n\n            # Statistiche\n            _, predicted = outputs.max(1)\n\n            # Salva tutte le etichette e predizioni\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n            # Modifica il JSON con le predizioni\n            for i in range(len(images)):\n                image_info = pred_data[idx * len(images) + i]  # Recupera info immagine dal JSON\n                image_id = image_info['image_id']\n\n                # Trova l'elemento nel pred_json e sostituisci il category_id con quello predetto\n                for entry in pred_data:\n                    if entry['image_id'] == image_id:\n                        entry['category_id'] = int(predicted[i].item())\n\n    # Salva il JSON modificato\n    with open(pred_json_path, 'w') as f:\n        json.dump(pred_data, f, indent=4)\n\n    # Calcola la test accuracy\n    test_accuracy = 100. * sum(np.array(all_labels) == np.array(all_predictions)) / len(all_labels)\n    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n\n    # Calcola e visualizza la matrice di confusione\n    cm = confusion_matrix(all_labels, all_predictions)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizza per riga\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T22:05:58.311824Z","iopub.execute_input":"2024-12-29T22:05:58.312275Z","iopub.status.idle":"2024-12-29T22:05:58.323321Z","shell.execute_reply.started":"2024-12-29T22:05:58.312246Z","shell.execute_reply":"2024-12-29T22:05:58.322457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carica i dati JSON per val e test\nval_json = load_json(val_path)\ntest_json = load_json(test_path)\nreg_json = val_json + test_json\n\n# Salva il JSON unito in un nuovo file\nwith open(reg_path, 'w') as f_merged:\n    json.dump(reg_json, f_merged, indent=4)\n    \nreg_ds = CustomDataset(reg_path)\nRegLoader = DataLoader(reg_ds, batch_size=32, shuffle=False)\n\n# Chiama la funzione di test\ntest_model_regressor(\n    net=model,\n    test_loader=RegLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=resnet_min_loss_path,\n    pred_json_path=reg_path\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-29T22:06:01.220188Z","iopub.execute_input":"2024-12-29T22:06:01.221015Z","iopub.status.idle":"2024-12-29T22:06:35.393888Z","shell.execute_reply.started":"2024-12-29T22:06:01.220982Z","shell.execute_reply":"2024-12-29T22:06:35.393008Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Box Regressor","metadata":{}},{"cell_type":"code","source":"class BoundingBoxRegressor:\n    def __init__(self, lambda_reg=1000):\n        self.lambda_reg = lambda_reg\n        self.category_weights = defaultdict(dict)  # Salva pesi per ogni categoria\n\n    def parse_json(self, json_data):\n        \"\"\"\n        Estrae proposals, ground truths e category_id da un JSON strutturato,\n        ignorando le box con category_id = 0 per il training, ma mantenendole per il JSON finale.\n        \"\"\"\n        proposals = []\n        ground_truths = []\n        categories = []\n        filtered_data = []\n        background_data = []  # Dati per la categoria 0 (background)\n\n        for item in json_data:\n            if item[\"category_id\"] == 0:\n                background_data.append(item)  # Salva i dati con category_id = 0\n            else:\n                proposals.append(item[\"region_bbox\"])\n                ground_truths.append(item[\"original_bbox\"])\n                categories.append(item[\"category_id\"])\n                filtered_data.append(item)  # Aggiungi solo dati validi\n\n        return np.array(proposals), np.array(ground_truths), np.array(categories), filtered_data, background_data\n\n    def train(self, json_data):\n        # Parsing dei dati dal JSON (esclusi i dati per la categoria 0)\n        proposals, ground_truths, categories, _, background_data = self.parse_json(json_data)\n        unique_categories = np.unique(categories)\n\n        # Dimensione del batch\n        batch_size = 32\n\n        # Progress bar per le categorie\n        for category in tqdm(unique_categories, desc=\"Training Categories\"):\n            # Filtra per categoria\n            cat_indices = np.where(categories == category)[0]\n            cat_proposals = proposals[cat_indices]\n            cat_ground_truths = ground_truths[cat_indices]\n\n            # Preparazione matrici\n            N = len(cat_proposals)\n            targets = np.zeros((min(batch_size, N), 4))  # tx, ty, tw, th\n\n            # Progress bar per i batch\n            for i in tqdm(range(0, len(cat_proposals), batch_size), desc=f\"Category {category} Batches\"):\n                batch_proposals = cat_proposals[i:i + batch_size]\n                batch_ground_truths = cat_ground_truths[i:i + batch_size]\n                batch_targets = targets[:len(batch_proposals)]\n\n                for j, (P, G) in enumerate(zip(batch_proposals, batch_ground_truths)):\n                    Px, Py, Pw, Ph = P\n                    Gx, Gy, Gw, Gh = G\n\n                    # Calcola i target di regressione\n                    batch_targets[j, 0] = (Gx - Px) / Pw  # tx\n                    batch_targets[j, 1] = (Gy - Py) / Ph  # ty\n                    batch_targets[j, 2] = np.log(Gw / Pw)  # tw\n                    batch_targets[j, 3] = np.log(Gh / Ph)  # th\n\n                # Risoluzione dei pesi per ogni coordinata\n                for k, label in enumerate(['x', 'y', 'w', 'h']):\n                    target_k = batch_targets[:, k]\n                    A = np.dot(batch_proposals.T, batch_proposals) + self.lambda_reg * np.eye(4)\n                    b = np.dot(batch_proposals.T, target_k)\n\n                    self.category_weights[category][label] = np.linalg.solve(A, b)\n\n    def predict(self, json_data, output_path):\n        \"\"\"\n        Predice i bounding box corretti per ogni categoria dato un insieme di dati JSON.\n        Salva il risultato in un nuovo file JSON.\n        \"\"\"\n        proposals, _, categories, filtered_data, background_data = self.parse_json(json_data)\n        N = proposals.shape[0]\n    \n        # Progress bar per le proposte\n        for i in tqdm(range(N), desc=\"Predicting Bounding Boxes\", total=N):\n            category = categories[i]\n            Px, Py, Pw, Ph = proposals[i]\n    \n            # Calcola le correzioni per ogni coordinata (tx, ty, tw, th) usando i pesi\n            dx = np.dot(proposals[i], self.category_weights[category]['x'])\n            dy = np.dot(proposals[i], self.category_weights[category]['y'])\n            dw = np.dot(proposals[i], self.category_weights[category]['w'])\n            dh = np.dot(proposals[i], self.category_weights[category]['h'])\n    \n            # Predizione del bounding box corretto\n            Gx_hat = Pw * dx + Px\n            Gy_hat = Ph * dy + Py\n            Gw_hat = Pw * np.exp(dw)\n            Gh_hat = Ph * np.exp(dh)\n    \n            # Aggiorna il JSON con il nuovo bounding box\n            filtered_data[i]['region_bbox'] = [Gx_hat, Gy_hat, Gw_hat, Gh_hat]\n    \n        # Riaggiungi i dati della categoria 0 (background)\n        filtered_data.extend(background_data)\n    \n        # Salva il risultato nel file JSON di output\n        with open(output_path, 'w') as outfile:\n            json.dump(filtered_data, outfile, indent=4)\n    \n        print(f\"File salvato in: {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:28:05.456551Z","iopub.execute_input":"2024-12-30T11:28:05.457259Z","iopub.status.idle":"2024-12-30T11:28:05.479716Z","shell.execute_reply.started":"2024-12-30T11:28:05.457198Z","shell.execute_reply":"2024-12-30T11:28:05.478098Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train_json = load_json(in_train_path)\nreg_json = load_json(in_reg_path)\ntrain_features_dict = np.load(in_train_features_path, allow_pickle=True).item()\n    \ntrain_features_resized = []\nfor key, value in train_features_dict.items():\n    aligned = []\n    for feat in value:\n        arr = np.array(feat)\n        # Esempio: slice a 24\n        if arr.shape[0] > 24:\n            arr = arr[:24]\n        elif arr.shape[0] < 24:\n            pad_width = 24 - arr.shape[0]\n            arr = np.pad(arr, (0, pad_width))\n        aligned.append(arr)\n    stacked = np.stack(aligned, axis=0)\n    train_features_resized.append(stacked)\n\ntrain_features = np.concatenate(train_features_resized, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:03:28.379149Z","iopub.execute_input":"2024-12-30T11:03:28.380715Z","iopub.status.idle":"2024-12-30T11:03:51.250740Z","shell.execute_reply.started":"2024-12-30T11:03:28.380644Z","shell.execute_reply":"2024-12-30T11:03:51.248621Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(arr.dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:59:33.872531Z","iopub.execute_input":"2024-12-30T10:59:33.873087Z","iopub.status.idle":"2024-12-30T10:59:33.881154Z","shell.execute_reply.started":"2024-12-30T10:59:33.873025Z","shell.execute_reply":"2024-12-30T10:59:33.879465Z"}},"outputs":[{"name":"stdout","text":"float32\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(train_features.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:05:13.367220Z","iopub.execute_input":"2024-12-30T11:05:13.367728Z","iopub.status.idle":"2024-12-30T11:05:13.374439Z","shell.execute_reply.started":"2024-12-30T11:05:13.367690Z","shell.execute_reply":"2024-12-30T11:05:13.373171Z"}},"outputs":[{"name":"stdout","text":"(10920, 24, 2048)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Training\ntrain_json = load_json(in_train_path)\nregressor = BoundingBoxRegressor()\nregressor.train(train_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:19:34.828637Z","iopub.execute_input":"2024-12-30T11:19:34.829187Z","iopub.status.idle":"2024-12-30T11:19:35.627734Z","shell.execute_reply.started":"2024-12-30T11:19:34.829140Z","shell.execute_reply":"2024-12-30T11:19:35.626101Z"}},"outputs":[{"name":"stderr","text":"Training Categories:   0%|          | 0/11 [00:00<?, ?it/s]\nCategory 1 Batches: 100%|██████████| 115/115 [00:00<00:00, 3065.08it/s]\n\nCategory 2 Batches: 100%|██████████| 97/97 [00:00<00:00, 3069.37it/s]\n\nCategory 3 Batches: 100%|██████████| 18/18 [00:00<00:00, 2549.64it/s]\n\nCategory 4 Batches: 100%|██████████| 31/31 [00:00<00:00, 2057.63it/s]\nTraining Categories:  36%|███▋      | 4/11 [00:00<00:00, 34.46it/s]\nCategory 5 Batches: 100%|██████████| 14/14 [00:00<00:00, 2380.90it/s]\n\nCategory 6 Batches: 100%|██████████| 242/242 [00:00<00:00, 3194.25it/s]\n\nCategory 7 Batches: 100%|██████████| 1/1 [00:00<00:00, 674.98it/s]\n\nCategory 8 Batches: 100%|██████████| 5/5 [00:00<00:00, 1962.34it/s]\nTraining Categories:  73%|███████▎  | 8/11 [00:00<00:00, 35.35it/s]\nCategory 9 Batches: 100%|██████████| 21/21 [00:00<00:00, 2302.88it/s]\n\nCategory 10 Batches: 100%|██████████| 1/1 [00:00<00:00, 649.68it/s]\n\nCategory 11 Batches: 100%|██████████| 4/4 [00:00<00:00, 1158.09it/s]\nTraining Categories: 100%|██████████| 11/11 [00:00<00:00, 42.50it/s]\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Predizione\nreg_json = load_json(in_reg_path)\nregressor.predict(reg_json, out_reg_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:28:11.860217Z","iopub.execute_input":"2024-12-30T11:28:11.861187Z","iopub.status.idle":"2024-12-30T11:28:11.973399Z","shell.execute_reply.started":"2024-12-30T11:28:11.861142Z","shell.execute_reply":"2024-12-30T11:28:11.971806Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Predizione\u001b[39;00m\n\u001b[1;32m      2\u001b[0m reg_json \u001b[38;5;241m=\u001b[39m load_json(in_reg_path)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreg_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_reg_path\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[21], line 76\u001b[0m, in \u001b[0;36mBoundingBoxRegressor.predict\u001b[0;34m(self, json_data, output_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, json_data, output_path):\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    Predice i bounding box corretti per ogni categoria dato i dati JSON.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    Salva il risultato in un nuovo file JSON.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     proposals, _, categories, filtered_data, background_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     N \u001b[38;5;241m=\u001b[39m proposals\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Progress bar per le proposte\u001b[39;00m\n","Cell \u001b[0;32mIn[21], line 26\u001b[0m, in \u001b[0;36mBoundingBoxRegressor.parse_json\u001b[0;34m(self, json_data)\u001b[0m\n\u001b[1;32m     23\u001b[0m         categories\u001b[38;5;241m.\u001b[39mappend(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     24\u001b[0m         filtered_data\u001b[38;5;241m.\u001b[39mappend(item)  \u001b[38;5;66;03m# Aggiungi solo dati validi\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(proposals), \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truths\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(categories), filtered_data, background_data\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5323,) + inhomogeneous part."],"ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5323,) + inhomogeneous part.","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"new_reg_json = load_json(out_reg_path)\n\n# Chiama la funzione di test\ntest_model_regressor(\n    net=model,\n    test_loader=RegLoader,\n    criterion=criterion,\n    device=device,\n    path_min_loss=resnet_min_loss_path,\n    pred_json_path=new_reg_path\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}